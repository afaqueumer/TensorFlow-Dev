{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb3046e9",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "766ddc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb59a9ba",
   "metadata": {},
   "source": [
    "### US Health Insurance Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c06ec78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "df = pd.read_csv(\"data/ushealth.csv\")\n",
    "\n",
    "# Create x & y\n",
    "x = df.drop(\"charges\", axis=1)\n",
    "y = df[\"charges\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9565e430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>smoker</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>female</td>\n",
       "      <td>27.900</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>southwest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>male</td>\n",
       "      <td>33.770</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>male</td>\n",
       "      <td>33.000</td>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>male</td>\n",
       "      <td>22.705</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>male</td>\n",
       "      <td>28.880</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1333</th>\n",
       "      <td>50</td>\n",
       "      <td>male</td>\n",
       "      <td>30.970</td>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>18</td>\n",
       "      <td>female</td>\n",
       "      <td>31.920</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northeast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1335</th>\n",
       "      <td>18</td>\n",
       "      <td>female</td>\n",
       "      <td>36.850</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1336</th>\n",
       "      <td>21</td>\n",
       "      <td>female</td>\n",
       "      <td>25.800</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>southwest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337</th>\n",
       "      <td>61</td>\n",
       "      <td>female</td>\n",
       "      <td>29.070</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>northwest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1338 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age     sex     bmi  children smoker     region\n",
       "0      19  female  27.900         0    yes  southwest\n",
       "1      18    male  33.770         1     no  southeast\n",
       "2      28    male  33.000         3     no  southeast\n",
       "3      33    male  22.705         0     no  northwest\n",
       "4      32    male  28.880         0     no  northwest\n",
       "...   ...     ...     ...       ...    ...        ...\n",
       "1333   50    male  30.970         3     no  northwest\n",
       "1334   18  female  31.920         0     no  northeast\n",
       "1335   18  female  36.850         0     no  southeast\n",
       "1336   21  female  25.800         0     no  southwest\n",
       "1337   61  female  29.070         0    yes  northwest\n",
       "\n",
       "[1338 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bdbaa52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       16884.92400\n",
       "1        1725.55230\n",
       "2        4449.46200\n",
       "3       21984.47061\n",
       "4        3866.85520\n",
       "           ...     \n",
       "1333    10600.54830\n",
       "1334     2205.98080\n",
       "1335     1629.83350\n",
       "1336     2007.94500\n",
       "1337    29141.36030\n",
       "Name: charges, Length: 1338, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04213548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build our train and test sets (use random state to ensure same split as before for hyperparams experiment)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Identify the categorical and numeric columns\n",
    "numeric_features = [\"age\", \"bmi\", \"children\"]\n",
    "categorical_features = [\"sex\", \"smoker\", \"region\"]\n",
    "ct = make_column_transformer((MinMaxScaler(), numeric_features),\n",
    "                             (OneHotEncoder(handle_unknown='ignore'), categorical_features))\n",
    "\n",
    "# transform the data using transformers\n",
    "x_train_transformed = ct.fit_transform(x_train)\n",
    "x_test_transformed = ct.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fad85d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ColumnTransformer(transformers=[(&#x27;minmaxscaler&#x27;, MinMaxScaler(),\n",
       "                                 [&#x27;age&#x27;, &#x27;bmi&#x27;, &#x27;children&#x27;]),\n",
       "                                (&#x27;onehotencoder&#x27;,\n",
       "                                 OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                 [&#x27;sex&#x27;, &#x27;smoker&#x27;, &#x27;region&#x27;])])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;minmaxscaler&#x27;, MinMaxScaler(),\n",
       "                                 [&#x27;age&#x27;, &#x27;bmi&#x27;, &#x27;children&#x27;]),\n",
       "                                (&#x27;onehotencoder&#x27;,\n",
       "                                 OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                 [&#x27;sex&#x27;, &#x27;smoker&#x27;, &#x27;region&#x27;])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">minmaxscaler</label><div class=\"sk-toggleable__content\"><pre>[&#x27;age&#x27;, &#x27;bmi&#x27;, &#x27;children&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">onehotencoder</label><div class=\"sk-toggleable__content\"><pre>[&#x27;sex&#x27;, &#x27;smoker&#x27;, &#x27;region&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "ColumnTransformer(transformers=[('minmaxscaler', MinMaxScaler(),\n",
       "                                 ['age', 'bmi', 'children']),\n",
       "                                ('onehotencoder',\n",
       "                                 OneHotEncoder(handle_unknown='ignore'),\n",
       "                                 ['sex', 'smoker', 'region'])])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22ede406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1003, 6), (1003, 11))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_train_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81cb563d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(335, 11)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7571f45d",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83afc254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_transformed.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bfe1c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "32/32 [==============================] - 1s 8ms/step - loss: 13267.6768 - mae: 13267.6768 - val_loss: 13277.1406 - val_mae: 13277.1406\n",
      "Epoch 2/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 13266.4141 - mae: 13266.4141 - val_loss: 13275.1406 - val_mae: 13275.1406\n",
      "Epoch 3/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 13262.9062 - mae: 13262.9062 - val_loss: 13269.3467 - val_mae: 13269.3467\n",
      "Epoch 4/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 13253.6055 - mae: 13253.6055 - val_loss: 13255.2090 - val_mae: 13255.2090\n",
      "Epoch 5/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 13233.3691 - mae: 13233.3691 - val_loss: 13227.1064 - val_mae: 13227.1064\n",
      "Epoch 6/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 13196.5400 - mae: 13196.5400 - val_loss: 13179.3242 - val_mae: 13179.3242\n",
      "Epoch 7/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 13137.5342 - mae: 13137.5342 - val_loss: 13106.8555 - val_mae: 13106.8555\n",
      "Epoch 8/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 13051.6416 - mae: 13051.6416 - val_loss: 13004.9951 - val_mae: 13004.9951\n",
      "Epoch 9/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 12934.3486 - mae: 12934.3486 - val_loss: 12869.6104 - val_mae: 12869.6104\n",
      "Epoch 10/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 12781.4971 - mae: 12781.4971 - val_loss: 12695.9863 - val_mae: 12695.9863\n",
      "Epoch 11/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 12588.6182 - mae: 12588.6182 - val_loss: 12480.5762 - val_mae: 12480.5762\n",
      "Epoch 12/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 12351.8350 - mae: 12351.8350 - val_loss: 12218.9287 - val_mae: 12218.9287\n",
      "Epoch 13/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 12067.5898 - mae: 12067.5898 - val_loss: 11909.0186 - val_mae: 11909.0186\n",
      "Epoch 14/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 11737.9336 - mae: 11737.9336 - val_loss: 11555.2490 - val_mae: 11555.2490\n",
      "Epoch 15/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 11368.2354 - mae: 11368.2354 - val_loss: 11169.6084 - val_mae: 11169.6084\n",
      "Epoch 16/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 10981.4082 - mae: 10981.4082 - val_loss: 10786.0840 - val_mae: 10786.0840\n",
      "Epoch 17/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 10601.9277 - mae: 10601.9277 - val_loss: 10402.4814 - val_mae: 10402.4814\n",
      "Epoch 18/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 10219.4512 - mae: 10219.4512 - val_loss: 10033.3662 - val_mae: 10033.3662\n",
      "Epoch 19/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 9846.5283 - mae: 9846.5283 - val_loss: 9674.0518 - val_mae: 9674.0518\n",
      "Epoch 20/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 9488.3945 - mae: 9488.3945 - val_loss: 9352.9443 - val_mae: 9352.9443\n",
      "Epoch 21/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 9168.2783 - mae: 9168.2783 - val_loss: 9076.4990 - val_mae: 9076.4990\n",
      "Epoch 22/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 8880.1309 - mae: 8880.1309 - val_loss: 8842.5518 - val_mae: 8842.5518\n",
      "Epoch 23/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 8639.4531 - mae: 8639.4531 - val_loss: 8652.3203 - val_mae: 8652.3203\n",
      "Epoch 24/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 8435.2500 - mae: 8435.2500 - val_loss: 8497.3057 - val_mae: 8497.3057\n",
      "Epoch 25/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 8267.1504 - mae: 8267.1504 - val_loss: 8370.1680 - val_mae: 8370.1680\n",
      "Epoch 26/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 8135.1436 - mae: 8135.1436 - val_loss: 8269.1406 - val_mae: 8269.1406\n",
      "Epoch 27/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 8036.7808 - mae: 8036.7808 - val_loss: 8188.5269 - val_mae: 8188.5269\n",
      "Epoch 28/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 7960.0464 - mae: 7960.0464 - val_loss: 8127.7275 - val_mae: 8127.7275\n",
      "Epoch 29/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7901.4146 - mae: 7901.4146 - val_loss: 8075.9375 - val_mae: 8075.9375\n",
      "Epoch 30/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 7854.4541 - mae: 7854.4541 - val_loss: 8026.8291 - val_mae: 8026.8291\n",
      "Epoch 31/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 7811.2129 - mae: 7811.2129 - val_loss: 7988.2583 - val_mae: 7988.2583\n",
      "Epoch 32/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7775.1709 - mae: 7775.1709 - val_loss: 7950.3193 - val_mae: 7950.3193\n",
      "Epoch 33/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7740.3911 - mae: 7740.3911 - val_loss: 7918.2192 - val_mae: 7918.2192\n",
      "Epoch 34/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7707.5210 - mae: 7707.5210 - val_loss: 7884.6895 - val_mae: 7884.6895\n",
      "Epoch 35/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 7674.2378 - mae: 7674.2378 - val_loss: 7852.6016 - val_mae: 7852.6016\n",
      "Epoch 36/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7642.1245 - mae: 7642.1245 - val_loss: 7821.1255 - val_mae: 7821.1255\n",
      "Epoch 37/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 7609.3408 - mae: 7609.3408 - val_loss: 7791.2051 - val_mae: 7791.2051\n",
      "Epoch 38/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 7577.3867 - mae: 7577.3867 - val_loss: 7759.8896 - val_mae: 7759.8896\n",
      "Epoch 39/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 7545.9004 - mae: 7545.9004 - val_loss: 7729.8955 - val_mae: 7729.8955\n",
      "Epoch 40/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 7514.9438 - mae: 7514.9438 - val_loss: 7697.2715 - val_mae: 7697.2715\n",
      "Epoch 41/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7482.4502 - mae: 7482.4502 - val_loss: 7666.0181 - val_mae: 7666.0181\n",
      "Epoch 42/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 7449.6030 - mae: 7449.6030 - val_loss: 7633.9873 - val_mae: 7633.9873\n",
      "Epoch 43/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7417.1699 - mae: 7417.1699 - val_loss: 7601.4097 - val_mae: 7601.4097\n",
      "Epoch 44/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7385.2749 - mae: 7385.2749 - val_loss: 7568.0410 - val_mae: 7568.0410\n",
      "Epoch 45/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7351.3398 - mae: 7351.3398 - val_loss: 7534.1680 - val_mae: 7534.1680\n",
      "Epoch 46/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7317.3184 - mae: 7317.3184 - val_loss: 7500.7261 - val_mae: 7500.7261\n",
      "Epoch 47/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7282.6177 - mae: 7282.6177 - val_loss: 7466.5039 - val_mae: 7466.5039\n",
      "Epoch 48/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7246.8018 - mae: 7246.8018 - val_loss: 7431.2539 - val_mae: 7431.2539\n",
      "Epoch 49/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7212.0581 - mae: 7212.0581 - val_loss: 7396.2490 - val_mae: 7396.2490\n",
      "Epoch 50/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7174.8916 - mae: 7174.8916 - val_loss: 7359.4155 - val_mae: 7359.4155\n",
      "Epoch 51/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7137.3315 - mae: 7137.3315 - val_loss: 7323.3760 - val_mae: 7323.3760\n",
      "Epoch 52/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7099.2783 - mae: 7099.2783 - val_loss: 7285.3364 - val_mae: 7285.3364\n",
      "Epoch 53/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7059.8247 - mae: 7059.8247 - val_loss: 7245.6074 - val_mae: 7245.6074\n",
      "Epoch 54/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7021.5444 - mae: 7021.5444 - val_loss: 7206.6641 - val_mae: 7206.6641\n",
      "Epoch 55/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 6978.4463 - mae: 6978.4463 - val_loss: 7164.3188 - val_mae: 7164.3188\n",
      "Epoch 56/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 6937.0371 - mae: 6937.0371 - val_loss: 7122.2979 - val_mae: 7122.2979\n",
      "Epoch 57/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 2ms/step - loss: 6892.8984 - mae: 6892.8984 - val_loss: 7077.8110 - val_mae: 7077.8110\n",
      "Epoch 58/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 6848.7642 - mae: 6848.7642 - val_loss: 7031.2222 - val_mae: 7031.2222\n",
      "Epoch 59/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 6803.0615 - mae: 6803.0615 - val_loss: 6985.3247 - val_mae: 6985.3247\n",
      "Epoch 60/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 6755.7344 - mae: 6755.7344 - val_loss: 6939.5498 - val_mae: 6939.5498\n",
      "Epoch 61/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 6707.5376 - mae: 6707.5376 - val_loss: 6889.0151 - val_mae: 6889.0151\n",
      "Epoch 62/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 6657.0635 - mae: 6657.0635 - val_loss: 6839.0278 - val_mae: 6839.0278\n",
      "Epoch 63/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 6606.7485 - mae: 6606.7485 - val_loss: 6787.1304 - val_mae: 6787.1304\n",
      "Epoch 64/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 6554.0449 - mae: 6554.0449 - val_loss: 6731.6304 - val_mae: 6731.6304\n",
      "Epoch 65/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 6497.5151 - mae: 6497.5151 - val_loss: 6676.2412 - val_mae: 6676.2412\n",
      "Epoch 66/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 6439.8687 - mae: 6439.8687 - val_loss: 6617.8076 - val_mae: 6617.8076\n",
      "Epoch 67/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 6381.6938 - mae: 6381.6938 - val_loss: 6556.7808 - val_mae: 6556.7808\n",
      "Epoch 68/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 6320.0371 - mae: 6320.0371 - val_loss: 6494.4741 - val_mae: 6494.4741\n",
      "Epoch 69/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 6256.5732 - mae: 6256.5732 - val_loss: 6429.9590 - val_mae: 6429.9590\n",
      "Epoch 70/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 6191.3325 - mae: 6191.3325 - val_loss: 6360.9365 - val_mae: 6360.9365\n",
      "Epoch 71/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 6122.6987 - mae: 6122.6987 - val_loss: 6290.6655 - val_mae: 6290.6655\n",
      "Epoch 72/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 6052.1768 - mae: 6052.1768 - val_loss: 6219.4463 - val_mae: 6219.4463\n",
      "Epoch 73/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 5979.3921 - mae: 5979.3921 - val_loss: 6143.7925 - val_mae: 6143.7925\n",
      "Epoch 74/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 5903.0376 - mae: 5903.0376 - val_loss: 6065.3521 - val_mae: 6065.3521\n",
      "Epoch 75/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 5823.6470 - mae: 5823.6470 - val_loss: 5982.9512 - val_mae: 5982.9512\n",
      "Epoch 76/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 5741.5273 - mae: 5741.5273 - val_loss: 5899.1113 - val_mae: 5899.1113\n",
      "Epoch 77/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 5656.0820 - mae: 5656.0820 - val_loss: 5812.5308 - val_mae: 5812.5308\n",
      "Epoch 78/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 5568.2524 - mae: 5568.2524 - val_loss: 5722.5718 - val_mae: 5722.5718\n",
      "Epoch 79/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 5479.1147 - mae: 5479.1147 - val_loss: 5628.6812 - val_mae: 5628.6812\n",
      "Epoch 80/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 5388.0200 - mae: 5388.0200 - val_loss: 5532.7427 - val_mae: 5532.7427\n",
      "Epoch 81/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 5294.6973 - mae: 5294.6973 - val_loss: 5431.6597 - val_mae: 5431.6597\n",
      "Epoch 82/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 5199.4766 - mae: 5199.4766 - val_loss: 5327.0410 - val_mae: 5327.0410\n",
      "Epoch 83/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 5100.7812 - mae: 5100.7812 - val_loss: 5221.8555 - val_mae: 5221.8555\n",
      "Epoch 84/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 5004.4302 - mae: 5004.4302 - val_loss: 5114.1621 - val_mae: 5114.1621\n",
      "Epoch 85/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 4901.4863 - mae: 4901.4863 - val_loss: 5003.5220 - val_mae: 5003.5220\n",
      "Epoch 86/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 4799.9160 - mae: 4799.9160 - val_loss: 4892.7690 - val_mae: 4892.7690\n",
      "Epoch 87/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 4696.5747 - mae: 4696.5747 - val_loss: 4781.4619 - val_mae: 4781.4619\n",
      "Epoch 88/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 4593.3555 - mae: 4593.3555 - val_loss: 4674.5630 - val_mae: 4674.5630\n",
      "Epoch 89/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 4494.9780 - mae: 4494.9780 - val_loss: 4568.1953 - val_mae: 4568.1953\n",
      "Epoch 90/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 4394.9355 - mae: 4394.9355 - val_loss: 4464.1606 - val_mae: 4464.1606\n",
      "Epoch 91/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 4302.4712 - mae: 4302.4712 - val_loss: 4367.5718 - val_mae: 4367.5718\n",
      "Epoch 92/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 4214.2593 - mae: 4214.2593 - val_loss: 4279.1826 - val_mae: 4279.1826\n",
      "Epoch 93/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 4130.7949 - mae: 4130.7949 - val_loss: 4193.1191 - val_mae: 4193.1191\n",
      "Epoch 94/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 4053.5520 - mae: 4053.5520 - val_loss: 4115.1558 - val_mae: 4115.1558\n",
      "Epoch 95/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3982.6316 - mae: 3982.6316 - val_loss: 4043.0056 - val_mae: 4043.0056\n",
      "Epoch 96/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3920.0906 - mae: 3920.0906 - val_loss: 3979.1194 - val_mae: 3979.1194\n",
      "Epoch 97/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3869.7583 - mae: 3869.7583 - val_loss: 3917.5552 - val_mae: 3917.5552\n",
      "Epoch 98/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3815.3103 - mae: 3815.3103 - val_loss: 3870.8015 - val_mae: 3870.8015\n",
      "Epoch 99/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3772.2529 - mae: 3772.2529 - val_loss: 3826.4973 - val_mae: 3826.4973\n",
      "Epoch 100/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3735.2969 - mae: 3735.2969 - val_loss: 3787.6321 - val_mae: 3787.6321\n",
      "Epoch 101/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3704.4871 - mae: 3704.4871 - val_loss: 3757.8142 - val_mae: 3757.8142\n",
      "Epoch 102/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3680.3359 - mae: 3680.3359 - val_loss: 3735.7075 - val_mae: 3735.7075\n",
      "Epoch 103/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3662.6528 - mae: 3662.6528 - val_loss: 3717.0225 - val_mae: 3717.0225\n",
      "Epoch 104/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3647.8179 - mae: 3647.8179 - val_loss: 3700.3694 - val_mae: 3700.3694\n",
      "Epoch 105/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3637.4263 - mae: 3637.4263 - val_loss: 3687.8018 - val_mae: 3687.8018\n",
      "Epoch 106/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3628.8494 - mae: 3628.8494 - val_loss: 3678.9006 - val_mae: 3678.9006\n",
      "Epoch 107/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3620.0964 - mae: 3620.0964 - val_loss: 3666.8342 - val_mae: 3666.8342\n",
      "Epoch 108/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3614.8889 - mae: 3614.8889 - val_loss: 3659.3037 - val_mae: 3659.3037\n",
      "Epoch 109/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3606.0591 - mae: 3606.0591 - val_loss: 3649.7705 - val_mae: 3649.7705\n",
      "Epoch 110/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3599.1663 - mae: 3599.1663 - val_loss: 3642.7844 - val_mae: 3642.7844\n",
      "Epoch 111/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3592.8411 - mae: 3592.8411 - val_loss: 3634.9377 - val_mae: 3634.9377\n",
      "Epoch 112/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3588.1025 - mae: 3588.1025 - val_loss: 3631.2402 - val_mae: 3631.2402\n",
      "Epoch 113/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 2ms/step - loss: 3585.0542 - mae: 3585.0542 - val_loss: 3619.2415 - val_mae: 3619.2415\n",
      "Epoch 114/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3579.5420 - mae: 3579.5420 - val_loss: 3615.6709 - val_mae: 3615.6709\n",
      "Epoch 115/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3573.9006 - mae: 3573.9006 - val_loss: 3608.4648 - val_mae: 3608.4648\n",
      "Epoch 116/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3568.4758 - mae: 3568.4758 - val_loss: 3601.0056 - val_mae: 3601.0056\n",
      "Epoch 117/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3563.3604 - mae: 3563.3604 - val_loss: 3595.9082 - val_mae: 3595.9082\n",
      "Epoch 118/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3558.5840 - mae: 3558.5840 - val_loss: 3584.9150 - val_mae: 3584.9150\n",
      "Epoch 119/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3552.4656 - mae: 3552.4656 - val_loss: 3580.5359 - val_mae: 3580.5359\n",
      "Epoch 120/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3548.4070 - mae: 3548.4070 - val_loss: 3573.9094 - val_mae: 3573.9094\n",
      "Epoch 121/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3543.5078 - mae: 3543.5078 - val_loss: 3564.6865 - val_mae: 3564.6865\n",
      "Epoch 122/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3535.9741 - mae: 3535.9741 - val_loss: 3559.5977 - val_mae: 3559.5977\n",
      "Epoch 123/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3531.0652 - mae: 3531.0652 - val_loss: 3554.4053 - val_mae: 3554.4053\n",
      "Epoch 124/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3526.2273 - mae: 3526.2273 - val_loss: 3545.0955 - val_mae: 3545.0955\n",
      "Epoch 125/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3521.2009 - mae: 3521.2009 - val_loss: 3537.7852 - val_mae: 3537.7852\n",
      "Epoch 126/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3516.0852 - mae: 3516.0852 - val_loss: 3535.6877 - val_mae: 3535.6877\n",
      "Epoch 127/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3511.3667 - mae: 3511.3667 - val_loss: 3518.9041 - val_mae: 3518.9041\n",
      "Epoch 128/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3504.1460 - mae: 3504.1460 - val_loss: 3513.5310 - val_mae: 3513.5310\n",
      "Epoch 129/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3498.0281 - mae: 3498.0281 - val_loss: 3505.4441 - val_mae: 3505.4441\n",
      "Epoch 130/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3492.9666 - mae: 3492.9666 - val_loss: 3497.7954 - val_mae: 3497.7954\n",
      "Epoch 131/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3487.7974 - mae: 3487.7974 - val_loss: 3492.1082 - val_mae: 3492.1082\n",
      "Epoch 132/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3482.4622 - mae: 3482.4622 - val_loss: 3485.4963 - val_mae: 3485.4963\n",
      "Epoch 133/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3478.1614 - mae: 3478.1614 - val_loss: 3476.1768 - val_mae: 3476.1768\n",
      "Epoch 134/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3473.2712 - mae: 3473.2712 - val_loss: 3468.4612 - val_mae: 3468.4612\n",
      "Epoch 135/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3467.7400 - mae: 3467.7400 - val_loss: 3460.3296 - val_mae: 3460.3296\n",
      "Epoch 136/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3462.3057 - mae: 3462.3057 - val_loss: 3452.5876 - val_mae: 3452.5876\n",
      "Epoch 137/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3456.6770 - mae: 3456.6770 - val_loss: 3442.8735 - val_mae: 3442.8735\n",
      "Epoch 138/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3451.2180 - mae: 3451.2180 - val_loss: 3436.1716 - val_mae: 3436.1716\n",
      "Epoch 139/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3447.2007 - mae: 3447.2007 - val_loss: 3428.2161 - val_mae: 3428.2161\n",
      "Epoch 140/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3441.5645 - mae: 3441.5645 - val_loss: 3418.6104 - val_mae: 3418.6104\n",
      "Epoch 141/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3434.9653 - mae: 3434.9653 - val_loss: 3409.6750 - val_mae: 3409.6750\n",
      "Epoch 142/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3428.9888 - mae: 3428.9888 - val_loss: 3400.2717 - val_mae: 3400.2717\n",
      "Epoch 143/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3424.8994 - mae: 3424.8994 - val_loss: 3391.9033 - val_mae: 3391.9033\n",
      "Epoch 144/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3419.2139 - mae: 3419.2139 - val_loss: 3384.2620 - val_mae: 3384.2620\n",
      "Epoch 145/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3413.0208 - mae: 3413.0208 - val_loss: 3372.8862 - val_mae: 3372.8862\n",
      "Epoch 146/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3407.1345 - mae: 3407.1345 - val_loss: 3364.9458 - val_mae: 3364.9458\n",
      "Epoch 147/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3402.7163 - mae: 3402.7163 - val_loss: 3357.3911 - val_mae: 3357.3911\n",
      "Epoch 148/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3397.3789 - mae: 3397.3789 - val_loss: 3348.1963 - val_mae: 3348.1963\n",
      "Epoch 149/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3391.9121 - mae: 3391.9121 - val_loss: 3340.8879 - val_mae: 3340.8879\n",
      "Epoch 150/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3387.2393 - mae: 3387.2393 - val_loss: 3332.1929 - val_mae: 3332.1929\n",
      "Epoch 151/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3382.2747 - mae: 3382.2747 - val_loss: 3325.3347 - val_mae: 3325.3347\n",
      "Epoch 152/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3377.5986 - mae: 3377.5986 - val_loss: 3316.9402 - val_mae: 3316.9402\n",
      "Epoch 153/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3372.3079 - mae: 3372.3079 - val_loss: 3308.3970 - val_mae: 3308.3970\n",
      "Epoch 154/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3368.4783 - mae: 3368.4783 - val_loss: 3298.5825 - val_mae: 3298.5825\n",
      "Epoch 155/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3363.5908 - mae: 3363.5908 - val_loss: 3292.8687 - val_mae: 3292.8687\n",
      "Epoch 156/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3359.5005 - mae: 3359.5005 - val_loss: 3283.5720 - val_mae: 3283.5720\n",
      "Epoch 157/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3353.1238 - mae: 3353.1238 - val_loss: 3274.8467 - val_mae: 3274.8467\n",
      "Epoch 158/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3349.4089 - mae: 3349.4089 - val_loss: 3267.3247 - val_mae: 3267.3247\n",
      "Epoch 159/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3343.7422 - mae: 3343.7422 - val_loss: 3259.7903 - val_mae: 3259.7903\n",
      "Epoch 160/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3339.4309 - mae: 3339.4309 - val_loss: 3252.7720 - val_mae: 3252.7720\n",
      "Epoch 161/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3334.8679 - mae: 3334.8679 - val_loss: 3244.5635 - val_mae: 3244.5635\n",
      "Epoch 162/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3330.6182 - mae: 3330.6182 - val_loss: 3236.6321 - val_mae: 3236.6321\n",
      "Epoch 163/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3326.9758 - mae: 3326.9758 - val_loss: 3229.2803 - val_mae: 3229.2803\n",
      "Epoch 164/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3321.8506 - mae: 3321.8506 - val_loss: 3222.1321 - val_mae: 3222.1321\n",
      "Epoch 165/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3317.1924 - mae: 3317.1924 - val_loss: 3214.1877 - val_mae: 3214.1877\n",
      "Epoch 166/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3314.2090 - mae: 3314.2090 - val_loss: 3206.5105 - val_mae: 3206.5105\n",
      "Epoch 167/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3309.4292 - mae: 3309.4292 - val_loss: 3200.1111 - val_mae: 3200.1111\n",
      "Epoch 168/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3305.4854 - mae: 3305.4854 - val_loss: 3193.5225 - val_mae: 3193.5225\n",
      "Epoch 169/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 2ms/step - loss: 3302.6021 - mae: 3302.6021 - val_loss: 3190.2351 - val_mae: 3190.2351\n",
      "Epoch 170/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3300.0186 - mae: 3300.0186 - val_loss: 3182.3115 - val_mae: 3182.3115\n",
      "Epoch 171/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3297.5974 - mae: 3297.5974 - val_loss: 3174.9243 - val_mae: 3174.9243\n",
      "Epoch 172/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3291.9785 - mae: 3291.9785 - val_loss: 3169.1443 - val_mae: 3169.1443\n",
      "Epoch 173/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3288.7119 - mae: 3288.7119 - val_loss: 3159.9414 - val_mae: 3159.9414\n",
      "Epoch 174/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3284.8960 - mae: 3284.8960 - val_loss: 3154.0105 - val_mae: 3154.0105\n",
      "Epoch 175/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3281.8186 - mae: 3281.8186 - val_loss: 3147.0813 - val_mae: 3147.0813\n",
      "Epoch 176/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3277.2957 - mae: 3277.2957 - val_loss: 3141.7046 - val_mae: 3141.7046\n",
      "Epoch 177/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3273.8662 - mae: 3273.8662 - val_loss: 3138.9568 - val_mae: 3138.9568\n",
      "Epoch 178/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3271.5754 - mae: 3271.5754 - val_loss: 3128.9753 - val_mae: 3128.9753\n",
      "Epoch 179/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3267.6206 - mae: 3267.6206 - val_loss: 3121.7163 - val_mae: 3121.7163\n",
      "Epoch 180/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3264.1023 - mae: 3264.1023 - val_loss: 3117.2207 - val_mae: 3117.2207\n",
      "Epoch 181/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3261.2109 - mae: 3261.2109 - val_loss: 3111.9956 - val_mae: 3111.9956\n",
      "Epoch 182/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3257.8943 - mae: 3257.8943 - val_loss: 3105.8501 - val_mae: 3105.8501\n",
      "Epoch 183/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3255.6169 - mae: 3255.6169 - val_loss: 3100.5415 - val_mae: 3100.5415\n",
      "Epoch 184/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3253.4780 - mae: 3253.4780 - val_loss: 3095.8589 - val_mae: 3095.8589\n",
      "Epoch 185/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3251.2930 - mae: 3251.2930 - val_loss: 3091.6567 - val_mae: 3091.6567\n",
      "Epoch 186/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3250.2876 - mae: 3250.2876 - val_loss: 3094.8313 - val_mae: 3094.8313\n",
      "Epoch 187/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3248.5261 - mae: 3248.5261 - val_loss: 3083.1721 - val_mae: 3083.1721\n",
      "Epoch 188/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3244.7446 - mae: 3244.7446 - val_loss: 3078.6799 - val_mae: 3078.6799\n",
      "Epoch 189/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3243.0933 - mae: 3243.0933 - val_loss: 3076.6240 - val_mae: 3076.6240\n",
      "Epoch 190/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3241.6184 - mae: 3241.6184 - val_loss: 3073.6245 - val_mae: 3073.6245\n",
      "Epoch 191/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3240.0513 - mae: 3240.0513 - val_loss: 3067.8997 - val_mae: 3067.8997\n",
      "Epoch 192/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3236.8828 - mae: 3236.8828 - val_loss: 3064.6765 - val_mae: 3064.6765\n",
      "Epoch 193/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3235.8884 - mae: 3235.8884 - val_loss: 3063.5244 - val_mae: 3063.5244\n",
      "Epoch 194/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3234.5256 - mae: 3234.5256 - val_loss: 3058.8806 - val_mae: 3058.8806\n",
      "Epoch 195/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3233.1372 - mae: 3233.1372 - val_loss: 3056.3579 - val_mae: 3056.3579\n",
      "Epoch 196/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3230.6355 - mae: 3230.6355 - val_loss: 3052.4497 - val_mae: 3052.4497\n",
      "Epoch 197/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3229.4851 - mae: 3229.4851 - val_loss: 3049.4348 - val_mae: 3049.4348\n",
      "Epoch 198/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3227.7861 - mae: 3227.7861 - val_loss: 3046.4561 - val_mae: 3046.4561\n",
      "Epoch 199/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3226.1038 - mae: 3226.1038 - val_loss: 3042.2812 - val_mae: 3042.2812\n",
      "Epoch 200/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3224.1296 - mae: 3224.1296 - val_loss: 3039.7607 - val_mae: 3039.7607\n",
      "Epoch 201/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3224.6755 - mae: 3224.6755 - val_loss: 3036.4714 - val_mae: 3036.4714\n",
      "Epoch 202/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3222.1538 - mae: 3222.1538 - val_loss: 3033.7627 - val_mae: 3033.7627\n",
      "Epoch 203/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3219.9778 - mae: 3219.9778 - val_loss: 3030.2068 - val_mae: 3030.2068\n",
      "Epoch 204/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3218.5491 - mae: 3218.5491 - val_loss: 3026.1816 - val_mae: 3026.1816\n",
      "Epoch 205/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3217.1423 - mae: 3217.1423 - val_loss: 3024.2939 - val_mae: 3024.2939\n",
      "Epoch 206/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3215.4421 - mae: 3215.4421 - val_loss: 3022.2329 - val_mae: 3022.2329\n",
      "Epoch 207/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3214.6477 - mae: 3214.6477 - val_loss: 3018.4734 - val_mae: 3018.4734\n",
      "Epoch 208/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3213.9670 - mae: 3213.9670 - val_loss: 3017.4192 - val_mae: 3017.4192\n",
      "Epoch 209/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3211.8459 - mae: 3211.8459 - val_loss: 3014.2490 - val_mae: 3014.2490\n",
      "Epoch 210/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3210.0405 - mae: 3210.0405 - val_loss: 3010.4680 - val_mae: 3010.4680\n",
      "Epoch 211/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3209.0967 - mae: 3209.0967 - val_loss: 3008.4888 - val_mae: 3008.4888\n",
      "Epoch 212/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3208.0696 - mae: 3208.0696 - val_loss: 3006.1597 - val_mae: 3006.1597\n",
      "Epoch 213/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3205.8594 - mae: 3205.8594 - val_loss: 3004.0637 - val_mae: 3004.0637\n",
      "Epoch 214/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3205.8569 - mae: 3205.8569 - val_loss: 3005.4875 - val_mae: 3005.4875\n",
      "Epoch 215/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3204.6948 - mae: 3204.6948 - val_loss: 3002.6340 - val_mae: 3002.6340\n",
      "Epoch 216/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3204.1438 - mae: 3204.1438 - val_loss: 2999.1077 - val_mae: 2999.1077\n",
      "Epoch 217/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3200.9402 - mae: 3200.9402 - val_loss: 2995.3987 - val_mae: 2995.3987\n",
      "Epoch 218/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3200.1892 - mae: 3200.1892 - val_loss: 2995.6416 - val_mae: 2995.6416\n",
      "Epoch 219/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3200.1697 - mae: 3200.1697 - val_loss: 2989.7080 - val_mae: 2989.7080\n",
      "Epoch 220/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3198.8408 - mae: 3198.8408 - val_loss: 2986.5989 - val_mae: 2986.5989\n",
      "Epoch 221/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3199.0681 - mae: 3199.0681 - val_loss: 2985.4639 - val_mae: 2985.4639\n",
      "Epoch 222/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3196.0442 - mae: 3196.0442 - val_loss: 2982.0256 - val_mae: 2982.0256\n",
      "Epoch 223/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3194.1648 - mae: 3194.1648 - val_loss: 2979.1218 - val_mae: 2979.1218\n",
      "Epoch 224/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3193.4153 - mae: 3193.4153 - val_loss: 2983.9116 - val_mae: 2983.9116\n",
      "Epoch 225/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 2ms/step - loss: 3191.8704 - mae: 3191.8704 - val_loss: 2974.8215 - val_mae: 2974.8215\n",
      "Epoch 226/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3189.7791 - mae: 3189.7791 - val_loss: 2970.4463 - val_mae: 2970.4463\n",
      "Epoch 227/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3187.9087 - mae: 3187.9087 - val_loss: 2968.3452 - val_mae: 2968.3452\n",
      "Epoch 228/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3188.0808 - mae: 3188.0808 - val_loss: 2968.1018 - val_mae: 2968.1018\n",
      "Epoch 229/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3187.5762 - mae: 3187.5762 - val_loss: 2965.8345 - val_mae: 2965.8345\n",
      "Epoch 230/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3186.1284 - mae: 3186.1284 - val_loss: 2963.6448 - val_mae: 2963.6448\n",
      "Epoch 231/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3185.0146 - mae: 3185.0146 - val_loss: 2961.1240 - val_mae: 2961.1240\n",
      "Epoch 232/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3184.2610 - mae: 3184.2610 - val_loss: 2960.6384 - val_mae: 2960.6384\n",
      "Epoch 233/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3184.0112 - mae: 3184.0112 - val_loss: 2961.9263 - val_mae: 2961.9263\n",
      "Epoch 234/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3184.7480 - mae: 3184.7480 - val_loss: 2958.6294 - val_mae: 2958.6294\n",
      "Epoch 235/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3184.1702 - mae: 3184.1702 - val_loss: 2957.2520 - val_mae: 2957.2520\n",
      "Epoch 236/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3183.5078 - mae: 3183.5078 - val_loss: 2958.3967 - val_mae: 2958.3967\n",
      "Epoch 237/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3183.7319 - mae: 3183.7319 - val_loss: 2957.7998 - val_mae: 2957.7998\n",
      "Epoch 238/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3182.2017 - mae: 3182.2017 - val_loss: 2955.3201 - val_mae: 2955.3201\n",
      "Epoch 239/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3182.0149 - mae: 3182.0149 - val_loss: 2955.8481 - val_mae: 2955.8481\n",
      "Epoch 240/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3181.6475 - mae: 3181.6475 - val_loss: 2956.6411 - val_mae: 2956.6411\n",
      "Epoch 241/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3181.8835 - mae: 3181.8835 - val_loss: 2959.4839 - val_mae: 2959.4839\n",
      "Epoch 242/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3182.3613 - mae: 3182.3613 - val_loss: 2954.9717 - val_mae: 2954.9717\n",
      "Epoch 243/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3181.6106 - mae: 3181.6106 - val_loss: 2955.0354 - val_mae: 2955.0354\n",
      "Epoch 244/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3181.6677 - mae: 3181.6677 - val_loss: 2953.9832 - val_mae: 2953.9832\n",
      "Epoch 245/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3181.7874 - mae: 3181.7874 - val_loss: 2955.3242 - val_mae: 2955.3242\n",
      "Epoch 246/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3180.5527 - mae: 3180.5527 - val_loss: 2954.6641 - val_mae: 2954.6641\n",
      "Epoch 247/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3180.7463 - mae: 3180.7463 - val_loss: 2955.1257 - val_mae: 2955.1257\n",
      "Epoch 248/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3181.6804 - mae: 3181.6804 - val_loss: 2953.4102 - val_mae: 2953.4102\n",
      "Epoch 249/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3181.1472 - mae: 3181.1472 - val_loss: 2952.0430 - val_mae: 2952.0430\n",
      "Epoch 250/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3180.0571 - mae: 3180.0571 - val_loss: 2954.1638 - val_mae: 2954.1638\n",
      "Epoch 251/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3180.0610 - mae: 3180.0610 - val_loss: 2956.5352 - val_mae: 2956.5352\n",
      "Epoch 252/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3179.6980 - mae: 3179.6980 - val_loss: 2953.5330 - val_mae: 2953.5330\n",
      "Epoch 253/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3178.5513 - mae: 3178.5513 - val_loss: 2955.5098 - val_mae: 2955.5098\n",
      "Epoch 254/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3181.8276 - mae: 3181.8276 - val_loss: 2953.3936 - val_mae: 2953.3936\n",
      "Epoch 255/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3178.7358 - mae: 3178.7358 - val_loss: 2952.7861 - val_mae: 2952.7861\n",
      "Epoch 256/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3178.4854 - mae: 3178.4854 - val_loss: 2952.5891 - val_mae: 2952.5891\n",
      "Epoch 257/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3179.1377 - mae: 3179.1377 - val_loss: 2954.5168 - val_mae: 2954.5168\n",
      "Epoch 258/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3179.1406 - mae: 3179.1406 - val_loss: 2953.1648 - val_mae: 2953.1648\n",
      "Epoch 259/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3178.5061 - mae: 3178.5061 - val_loss: 2952.3809 - val_mae: 2952.3809\n",
      "Epoch 260/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3178.1243 - mae: 3178.1243 - val_loss: 2952.9900 - val_mae: 2952.9900\n",
      "Epoch 261/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3177.7913 - mae: 3177.7913 - val_loss: 2951.7756 - val_mae: 2951.7756\n",
      "Epoch 262/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3179.3040 - mae: 3179.3040 - val_loss: 2952.5479 - val_mae: 2952.5479\n",
      "Epoch 263/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3177.9612 - mae: 3177.9612 - val_loss: 2950.5044 - val_mae: 2950.5044\n",
      "Epoch 264/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3179.3821 - mae: 3179.3821 - val_loss: 2951.4626 - val_mae: 2951.4626\n",
      "Epoch 265/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3179.0332 - mae: 3179.0332 - val_loss: 2952.2134 - val_mae: 2952.2134\n",
      "Epoch 266/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3177.4507 - mae: 3177.4507 - val_loss: 2951.0811 - val_mae: 2951.0811\n",
      "Epoch 267/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3177.0369 - mae: 3177.0369 - val_loss: 2951.5701 - val_mae: 2951.5701\n",
      "Epoch 268/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3177.1277 - mae: 3177.1277 - val_loss: 2949.9141 - val_mae: 2949.9141\n",
      "Epoch 269/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3176.4241 - mae: 3176.4241 - val_loss: 2953.9324 - val_mae: 2953.9324\n",
      "Epoch 270/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3177.8584 - mae: 3177.8584 - val_loss: 2950.3687 - val_mae: 2950.3687\n",
      "Epoch 271/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3176.5193 - mae: 3176.5193 - val_loss: 2955.0090 - val_mae: 2955.0090\n",
      "Epoch 272/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3177.5215 - mae: 3177.5215 - val_loss: 2950.7356 - val_mae: 2950.7356\n",
      "Epoch 273/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3176.6450 - mae: 3176.6450 - val_loss: 2949.7339 - val_mae: 2949.7339\n",
      "Epoch 274/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3177.0632 - mae: 3177.0632 - val_loss: 2950.9138 - val_mae: 2950.9138\n",
      "Epoch 275/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3177.5012 - mae: 3177.5012 - val_loss: 2952.2927 - val_mae: 2952.2927\n",
      "Epoch 276/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3176.4570 - mae: 3176.4570 - val_loss: 2954.5989 - val_mae: 2954.5989\n",
      "Epoch 277/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3179.1550 - mae: 3179.1550 - val_loss: 2949.4392 - val_mae: 2949.4392\n",
      "Epoch 278/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3175.5032 - mae: 3175.5032 - val_loss: 2948.8398 - val_mae: 2948.8398\n",
      "Epoch 279/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3176.2539 - mae: 3176.2539 - val_loss: 2949.7268 - val_mae: 2949.7268\n",
      "Epoch 280/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3175.8628 - mae: 3175.8628 - val_loss: 2952.4573 - val_mae: 2952.4573\n",
      "Epoch 281/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 2ms/step - loss: 3176.2788 - mae: 3176.2788 - val_loss: 2952.8816 - val_mae: 2952.8816\n",
      "Epoch 282/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3176.6084 - mae: 3176.6084 - val_loss: 2949.5645 - val_mae: 2949.5645\n",
      "Epoch 283/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3175.1492 - mae: 3175.1492 - val_loss: 2950.1660 - val_mae: 2950.1660\n",
      "Epoch 284/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3175.4426 - mae: 3175.4426 - val_loss: 2952.2710 - val_mae: 2952.2710\n",
      "Epoch 285/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3175.3162 - mae: 3175.3162 - val_loss: 2949.0994 - val_mae: 2949.0994\n",
      "Epoch 286/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3174.6487 - mae: 3174.6487 - val_loss: 2948.6348 - val_mae: 2948.6348\n",
      "Epoch 287/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3176.0166 - mae: 3176.0166 - val_loss: 2949.0164 - val_mae: 2949.0164\n",
      "Epoch 288/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3178.9307 - mae: 3178.9307 - val_loss: 2951.0415 - val_mae: 2951.0415\n",
      "Epoch 289/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3176.3975 - mae: 3176.3975 - val_loss: 2948.2051 - val_mae: 2948.2051\n",
      "Epoch 290/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3174.9863 - mae: 3174.9863 - val_loss: 2949.8669 - val_mae: 2949.8669\n",
      "Epoch 291/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3175.4885 - mae: 3175.4885 - val_loss: 2948.4387 - val_mae: 2948.4387\n",
      "Epoch 292/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3175.2307 - mae: 3175.2307 - val_loss: 2948.7627 - val_mae: 2948.7627\n",
      "Epoch 293/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3176.8555 - mae: 3176.8555 - val_loss: 2948.3708 - val_mae: 2948.3708\n",
      "Epoch 294/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3174.1792 - mae: 3174.1792 - val_loss: 2952.3665 - val_mae: 2952.3665\n",
      "Epoch 295/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3174.8789 - mae: 3174.8789 - val_loss: 2949.2810 - val_mae: 2949.2810\n",
      "Epoch 296/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3174.3030 - mae: 3174.3030 - val_loss: 2948.0254 - val_mae: 2948.0254\n",
      "Epoch 297/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3173.8245 - mae: 3173.8245 - val_loss: 2947.9880 - val_mae: 2947.9880\n",
      "Epoch 298/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3174.0886 - mae: 3174.0886 - val_loss: 2947.6938 - val_mae: 2947.6938\n",
      "Epoch 299/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3174.4143 - mae: 3174.4143 - val_loss: 2947.8184 - val_mae: 2947.8184\n",
      "Epoch 300/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3173.5771 - mae: 3173.5771 - val_loss: 2947.2439 - val_mae: 2947.2439\n",
      "Epoch 301/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3174.1548 - mae: 3174.1548 - val_loss: 2949.0630 - val_mae: 2949.0630\n",
      "Epoch 302/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3174.8105 - mae: 3174.8105 - val_loss: 2947.4521 - val_mae: 2947.4521\n",
      "Epoch 303/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3173.8218 - mae: 3173.8218 - val_loss: 2947.0376 - val_mae: 2947.0376\n",
      "Epoch 304/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3173.7241 - mae: 3173.7241 - val_loss: 2947.3833 - val_mae: 2947.3833\n",
      "Epoch 305/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3173.7605 - mae: 3173.7605 - val_loss: 2947.3904 - val_mae: 2947.3904\n",
      "Epoch 306/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3177.1008 - mae: 3177.1008 - val_loss: 2948.0164 - val_mae: 2948.0164\n",
      "Epoch 307/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3173.4961 - mae: 3173.4961 - val_loss: 2948.5188 - val_mae: 2948.5188\n",
      "Epoch 308/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3175.0603 - mae: 3175.0603 - val_loss: 2947.5662 - val_mae: 2947.5662\n",
      "Epoch 309/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3173.4717 - mae: 3173.4717 - val_loss: 2946.8540 - val_mae: 2946.8540\n",
      "Epoch 310/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3174.1228 - mae: 3174.1228 - val_loss: 2949.2119 - val_mae: 2949.2119\n",
      "Epoch 311/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3173.1165 - mae: 3173.1165 - val_loss: 2947.9539 - val_mae: 2947.9539\n",
      "Epoch 312/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3173.5122 - mae: 3173.5122 - val_loss: 2946.9072 - val_mae: 2946.9072\n",
      "Epoch 313/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3173.2598 - mae: 3173.2598 - val_loss: 2946.2256 - val_mae: 2946.2256\n",
      "Epoch 314/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3173.5740 - mae: 3173.5740 - val_loss: 2945.5725 - val_mae: 2945.5725\n",
      "Epoch 315/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3174.7598 - mae: 3174.7598 - val_loss: 2947.5593 - val_mae: 2947.5593\n",
      "Epoch 316/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3172.6646 - mae: 3172.6646 - val_loss: 2946.5344 - val_mae: 2946.5344\n",
      "Epoch 317/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3172.5742 - mae: 3172.5742 - val_loss: 2947.2346 - val_mae: 2947.2346\n",
      "Epoch 318/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3173.7053 - mae: 3173.7053 - val_loss: 2948.1987 - val_mae: 2948.1987\n",
      "Epoch 319/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3173.3882 - mae: 3173.3882 - val_loss: 2946.7700 - val_mae: 2946.7700\n",
      "Epoch 320/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3172.7476 - mae: 3172.7476 - val_loss: 2945.9680 - val_mae: 2945.9680\n",
      "Epoch 321/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3172.1572 - mae: 3172.1572 - val_loss: 2944.9956 - val_mae: 2944.9956\n",
      "Epoch 322/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3173.8879 - mae: 3173.8879 - val_loss: 2947.7000 - val_mae: 2947.7000\n",
      "Epoch 323/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3173.0435 - mae: 3173.0435 - val_loss: 2946.6978 - val_mae: 2946.6978\n",
      "Epoch 324/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3173.1714 - mae: 3173.1714 - val_loss: 2947.0742 - val_mae: 2947.0742\n",
      "Epoch 325/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3172.0190 - mae: 3172.0190 - val_loss: 2947.1538 - val_mae: 2947.1538\n",
      "Epoch 326/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3173.4270 - mae: 3173.4270 - val_loss: 2948.5762 - val_mae: 2948.5762\n",
      "Epoch 327/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3172.4124 - mae: 3172.4124 - val_loss: 2946.0056 - val_mae: 2946.0056\n",
      "Epoch 328/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3172.3386 - mae: 3172.3386 - val_loss: 2946.7307 - val_mae: 2946.7307\n",
      "Epoch 329/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3171.6748 - mae: 3171.6748 - val_loss: 2945.9944 - val_mae: 2945.9944\n",
      "Epoch 330/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3171.8977 - mae: 3171.8977 - val_loss: 2947.0637 - val_mae: 2947.0637\n",
      "Epoch 331/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3171.7588 - mae: 3171.7588 - val_loss: 2945.6301 - val_mae: 2945.6301\n",
      "Epoch 332/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3172.4634 - mae: 3172.4634 - val_loss: 2945.0908 - val_mae: 2945.0908\n",
      "Epoch 333/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3171.5486 - mae: 3171.5486 - val_loss: 2946.1226 - val_mae: 2946.1226\n",
      "Epoch 334/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3171.5806 - mae: 3171.5806 - val_loss: 2946.0564 - val_mae: 2946.0564\n",
      "Epoch 335/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3171.8809 - mae: 3171.8809 - val_loss: 2947.2629 - val_mae: 2947.2629\n",
      "Epoch 336/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3171.9214 - mae: 3171.9214 - val_loss: 2946.9453 - val_mae: 2946.9453\n",
      "Epoch 337/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 2ms/step - loss: 3171.5674 - mae: 3171.5674 - val_loss: 2945.2759 - val_mae: 2945.2759\n",
      "Epoch 338/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3172.3074 - mae: 3172.3074 - val_loss: 2946.8350 - val_mae: 2946.8350\n",
      "Epoch 339/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3170.8948 - mae: 3170.8948 - val_loss: 2946.8948 - val_mae: 2946.8948\n",
      "Epoch 340/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3171.2886 - mae: 3171.2886 - val_loss: 2947.1287 - val_mae: 2947.1287\n",
      "Epoch 341/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3170.7344 - mae: 3170.7344 - val_loss: 2944.9790 - val_mae: 2944.9790\n",
      "Epoch 342/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3170.8779 - mae: 3170.8779 - val_loss: 2944.7280 - val_mae: 2944.7280\n",
      "Epoch 343/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3171.9038 - mae: 3171.9038 - val_loss: 2945.7798 - val_mae: 2945.7798\n",
      "Epoch 344/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3170.8621 - mae: 3170.8621 - val_loss: 2946.5962 - val_mae: 2946.5962\n",
      "Epoch 345/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3171.1431 - mae: 3171.1431 - val_loss: 2945.6682 - val_mae: 2945.6682\n",
      "Epoch 346/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3170.2915 - mae: 3170.2915 - val_loss: 2945.3818 - val_mae: 2945.3818\n",
      "Epoch 347/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3170.0991 - mae: 3170.0991 - val_loss: 2945.0110 - val_mae: 2945.0110\n",
      "Epoch 348/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3170.4448 - mae: 3170.4448 - val_loss: 2945.2869 - val_mae: 2945.2869\n",
      "Epoch 349/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3170.5977 - mae: 3170.5977 - val_loss: 2945.5725 - val_mae: 2945.5725\n",
      "Epoch 350/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3170.6565 - mae: 3170.6565 - val_loss: 2945.6108 - val_mae: 2945.6108\n",
      "Epoch 351/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3170.7859 - mae: 3170.7859 - val_loss: 2948.3921 - val_mae: 2948.3921\n",
      "Epoch 352/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3171.6917 - mae: 3171.6917 - val_loss: 2943.9443 - val_mae: 2943.9443\n",
      "Epoch 353/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3170.6577 - mae: 3170.6577 - val_loss: 2944.9109 - val_mae: 2944.9109\n",
      "Epoch 354/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3170.4829 - mae: 3170.4829 - val_loss: 2945.5122 - val_mae: 2945.5122\n",
      "Epoch 355/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3170.1057 - mae: 3170.1057 - val_loss: 2944.9294 - val_mae: 2944.9294\n",
      "Epoch 356/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3170.4917 - mae: 3170.4917 - val_loss: 2946.5811 - val_mae: 2946.5811\n",
      "Epoch 357/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3170.5693 - mae: 3170.5693 - val_loss: 2946.3975 - val_mae: 2946.3975\n",
      "Epoch 358/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3172.0388 - mae: 3172.0388 - val_loss: 2944.4766 - val_mae: 2944.4766\n",
      "Epoch 359/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3170.1716 - mae: 3170.1716 - val_loss: 2944.3889 - val_mae: 2944.3889\n",
      "Epoch 360/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3170.7117 - mae: 3170.7117 - val_loss: 2943.1731 - val_mae: 2943.1731\n",
      "Epoch 361/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3170.6970 - mae: 3170.6970 - val_loss: 2945.5063 - val_mae: 2945.5063\n",
      "Epoch 362/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3171.0198 - mae: 3171.0198 - val_loss: 2943.9287 - val_mae: 2943.9287\n",
      "Epoch 363/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3170.5005 - mae: 3170.5005 - val_loss: 2943.4641 - val_mae: 2943.4641\n",
      "Epoch 364/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3169.6240 - mae: 3169.6240 - val_loss: 2944.2449 - val_mae: 2944.2449\n",
      "Epoch 365/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3170.2256 - mae: 3170.2256 - val_loss: 2944.6689 - val_mae: 2944.6689\n",
      "Epoch 366/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3169.8623 - mae: 3169.8623 - val_loss: 2944.1726 - val_mae: 2944.1726\n",
      "Epoch 367/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3170.5117 - mae: 3170.5117 - val_loss: 2945.4016 - val_mae: 2945.4016\n",
      "Epoch 368/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3169.7986 - mae: 3169.7986 - val_loss: 2949.1621 - val_mae: 2949.1621\n",
      "Epoch 369/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3170.1472 - mae: 3170.1472 - val_loss: 2942.9773 - val_mae: 2942.9773\n",
      "Epoch 370/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3169.8955 - mae: 3169.8955 - val_loss: 2943.6125 - val_mae: 2943.6125\n",
      "Epoch 371/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3169.2737 - mae: 3169.2737 - val_loss: 2943.6125 - val_mae: 2943.6125\n",
      "Epoch 372/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3169.0901 - mae: 3169.0901 - val_loss: 2943.9810 - val_mae: 2943.9810\n",
      "Epoch 373/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3168.9446 - mae: 3168.9446 - val_loss: 2944.6821 - val_mae: 2944.6821\n",
      "Epoch 374/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3168.5330 - mae: 3168.5330 - val_loss: 2942.9368 - val_mae: 2942.9368\n",
      "Epoch 375/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3173.5129 - mae: 3173.5129 - val_loss: 2943.0305 - val_mae: 2943.0305\n",
      "Epoch 376/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3168.8459 - mae: 3168.8459 - val_loss: 2943.6538 - val_mae: 2943.6538\n",
      "Epoch 377/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3168.5696 - mae: 3168.5696 - val_loss: 2943.6555 - val_mae: 2943.6555\n",
      "Epoch 378/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3169.2092 - mae: 3169.2092 - val_loss: 2945.1436 - val_mae: 2945.1436\n",
      "Epoch 379/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3168.9763 - mae: 3168.9763 - val_loss: 2943.9463 - val_mae: 2943.9463\n",
      "Epoch 380/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3169.4062 - mae: 3169.4062 - val_loss: 2942.9126 - val_mae: 2942.9126\n",
      "Epoch 381/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3168.9568 - mae: 3168.9568 - val_loss: 2944.3113 - val_mae: 2944.3113\n",
      "Epoch 382/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3168.3040 - mae: 3168.3040 - val_loss: 2942.7258 - val_mae: 2942.7258\n",
      "Epoch 383/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3168.9592 - mae: 3168.9592 - val_loss: 2943.8298 - val_mae: 2943.8298\n",
      "Epoch 384/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3168.6873 - mae: 3168.6873 - val_loss: 2943.5295 - val_mae: 2943.5295\n",
      "Epoch 385/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3168.5930 - mae: 3168.5930 - val_loss: 2943.6252 - val_mae: 2943.6252\n",
      "Epoch 386/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3168.1294 - mae: 3168.1294 - val_loss: 2942.1753 - val_mae: 2942.1753\n",
      "Epoch 387/2000\n",
      "32/32 [==============================] - 0s 926us/step - loss: 3169.0769 - mae: 3169.0769 - val_loss: 2942.8723 - val_mae: 2942.8723\n",
      "Epoch 388/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3168.7373 - mae: 3168.7373 - val_loss: 2945.2371 - val_mae: 2945.2371\n",
      "Epoch 389/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3168.7283 - mae: 3168.7283 - val_loss: 2945.5129 - val_mae: 2945.5129\n",
      "Epoch 390/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3169.6204 - mae: 3169.6204 - val_loss: 2943.6970 - val_mae: 2943.6970\n",
      "Epoch 391/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3168.3625 - mae: 3168.3625 - val_loss: 2942.8745 - val_mae: 2942.8745\n",
      "Epoch 392/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3168.5740 - mae: 3168.5740 - val_loss: 2943.9082 - val_mae: 2943.9082\n",
      "Epoch 393/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 2ms/step - loss: 3169.4958 - mae: 3169.4958 - val_loss: 2944.0078 - val_mae: 2944.0078\n",
      "Epoch 394/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3168.7231 - mae: 3168.7231 - val_loss: 2942.7400 - val_mae: 2942.7400\n",
      "Epoch 395/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3168.2100 - mae: 3168.2100 - val_loss: 2944.0039 - val_mae: 2944.0039\n",
      "Epoch 396/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3168.4507 - mae: 3168.4507 - val_loss: 2942.9451 - val_mae: 2942.9451\n",
      "Epoch 397/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3167.2925 - mae: 3167.2925 - val_loss: 2944.4353 - val_mae: 2944.4353\n",
      "Epoch 398/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3168.6914 - mae: 3168.6914 - val_loss: 2944.5278 - val_mae: 2944.5278\n",
      "Epoch 399/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3167.6624 - mae: 3167.6624 - val_loss: 2943.4143 - val_mae: 2943.4143\n",
      "Epoch 400/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3168.4460 - mae: 3168.4460 - val_loss: 2943.6472 - val_mae: 2943.6472\n",
      "Epoch 401/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3167.9268 - mae: 3167.9268 - val_loss: 2944.7939 - val_mae: 2944.7939\n",
      "Epoch 402/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3168.3662 - mae: 3168.3662 - val_loss: 2941.7051 - val_mae: 2941.7051\n",
      "Epoch 403/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3167.9880 - mae: 3167.9880 - val_loss: 2943.7244 - val_mae: 2943.7244\n",
      "Epoch 404/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3167.3459 - mae: 3167.3459 - val_loss: 2944.0344 - val_mae: 2944.0344\n",
      "Epoch 405/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3167.8679 - mae: 3167.8679 - val_loss: 2942.8862 - val_mae: 2942.8862\n",
      "Epoch 406/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3167.6187 - mae: 3167.6187 - val_loss: 2944.1147 - val_mae: 2944.1147\n",
      "Epoch 407/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3167.0691 - mae: 3167.0691 - val_loss: 2942.4734 - val_mae: 2942.4734\n",
      "Epoch 408/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3168.7576 - mae: 3168.7576 - val_loss: 2942.7891 - val_mae: 2942.7891\n",
      "Epoch 409/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3167.2957 - mae: 3167.2957 - val_loss: 2943.0693 - val_mae: 2943.0693\n",
      "Epoch 410/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3167.5229 - mae: 3167.5229 - val_loss: 2942.3547 - val_mae: 2942.3547\n",
      "Epoch 411/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3167.6978 - mae: 3167.6978 - val_loss: 2943.2527 - val_mae: 2943.2527\n",
      "Epoch 412/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3167.1082 - mae: 3167.1082 - val_loss: 2944.1531 - val_mae: 2944.1531\n",
      "Epoch 413/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3169.6057 - mae: 3169.6057 - val_loss: 2942.2644 - val_mae: 2942.2644\n",
      "Epoch 414/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3167.5234 - mae: 3167.5234 - val_loss: 2944.5195 - val_mae: 2944.5195\n",
      "Epoch 415/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3167.1416 - mae: 3167.1416 - val_loss: 2942.5396 - val_mae: 2942.5396\n",
      "Epoch 416/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3169.1511 - mae: 3169.1511 - val_loss: 2945.8049 - val_mae: 2945.8049\n",
      "Epoch 417/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3168.8149 - mae: 3168.8149 - val_loss: 2942.8208 - val_mae: 2942.8208\n",
      "Epoch 418/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3169.4705 - mae: 3169.4705 - val_loss: 2942.8206 - val_mae: 2942.8206\n",
      "Epoch 419/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3167.6599 - mae: 3167.6599 - val_loss: 2943.6213 - val_mae: 2943.6213\n",
      "Epoch 420/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3166.8860 - mae: 3166.8860 - val_loss: 2943.4812 - val_mae: 2943.4812\n",
      "Epoch 421/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3166.8921 - mae: 3166.8921 - val_loss: 2944.0505 - val_mae: 2944.0505\n",
      "Epoch 422/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3167.3328 - mae: 3167.3328 - val_loss: 2942.7717 - val_mae: 2942.7717\n",
      "Epoch 423/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3167.5779 - mae: 3167.5779 - val_loss: 2941.7266 - val_mae: 2941.7266\n",
      "Epoch 424/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3167.3752 - mae: 3167.3752 - val_loss: 2942.1694 - val_mae: 2942.1694\n",
      "Epoch 425/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3167.4568 - mae: 3167.4568 - val_loss: 2942.3857 - val_mae: 2942.3857\n",
      "Epoch 426/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3166.9934 - mae: 3166.9934 - val_loss: 2942.9290 - val_mae: 2942.9290\n",
      "Epoch 427/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3166.6292 - mae: 3166.6292 - val_loss: 2942.3059 - val_mae: 2942.3059\n",
      "Epoch 428/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3166.4326 - mae: 3166.4326 - val_loss: 2941.8198 - val_mae: 2941.8198\n",
      "Epoch 429/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3166.3640 - mae: 3166.3640 - val_loss: 2942.3735 - val_mae: 2942.3735\n",
      "Epoch 430/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3166.3125 - mae: 3166.3125 - val_loss: 2942.1526 - val_mae: 2942.1526\n",
      "Epoch 431/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3165.8252 - mae: 3165.8252 - val_loss: 2942.2141 - val_mae: 2942.2141\n",
      "Epoch 432/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3167.0198 - mae: 3167.0198 - val_loss: 2941.8337 - val_mae: 2941.8337\n",
      "Epoch 433/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3165.7874 - mae: 3165.7874 - val_loss: 2941.4175 - val_mae: 2941.4175\n",
      "Epoch 434/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3166.2832 - mae: 3166.2832 - val_loss: 2945.5317 - val_mae: 2945.5317\n",
      "Epoch 435/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3167.3259 - mae: 3167.3259 - val_loss: 2944.5388 - val_mae: 2944.5388\n",
      "Epoch 436/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3166.5903 - mae: 3166.5903 - val_loss: 2943.9167 - val_mae: 2943.9167\n",
      "Epoch 437/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3166.3953 - mae: 3166.3953 - val_loss: 2941.4229 - val_mae: 2941.4229\n",
      "Epoch 438/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3167.0325 - mae: 3167.0325 - val_loss: 2942.9680 - val_mae: 2942.9680\n",
      "Epoch 439/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3167.0718 - mae: 3167.0718 - val_loss: 2941.2678 - val_mae: 2941.2678\n",
      "Epoch 440/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3167.1670 - mae: 3167.1670 - val_loss: 2944.8442 - val_mae: 2944.8442\n",
      "Epoch 441/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3166.4890 - mae: 3166.4890 - val_loss: 2942.5918 - val_mae: 2942.5918\n",
      "Epoch 442/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3166.1487 - mae: 3166.1487 - val_loss: 2943.1433 - val_mae: 2943.1433\n",
      "Epoch 443/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3166.4641 - mae: 3166.4641 - val_loss: 2941.0403 - val_mae: 2941.0403\n",
      "Epoch 444/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3166.1506 - mae: 3166.1506 - val_loss: 2940.3345 - val_mae: 2940.3345\n",
      "Epoch 445/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3167.0627 - mae: 3167.0627 - val_loss: 2941.9893 - val_mae: 2941.9893\n",
      "Epoch 446/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3165.3364 - mae: 3165.3364 - val_loss: 2941.9465 - val_mae: 2941.9465\n",
      "Epoch 447/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3166.9556 - mae: 3166.9556 - val_loss: 2942.5010 - val_mae: 2942.5010\n",
      "Epoch 448/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3166.0188 - mae: 3166.0188 - val_loss: 2942.8318 - val_mae: 2942.8318\n",
      "Epoch 449/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 1ms/step - loss: 3166.3992 - mae: 3166.3992 - val_loss: 2941.1609 - val_mae: 2941.1609\n",
      "Epoch 450/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3165.5627 - mae: 3165.5627 - val_loss: 2942.6160 - val_mae: 2942.6160\n",
      "Epoch 451/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3165.2673 - mae: 3165.2673 - val_loss: 2948.1899 - val_mae: 2948.1899\n",
      "Epoch 452/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3168.1953 - mae: 3168.1953 - val_loss: 2940.8364 - val_mae: 2940.8364\n",
      "Epoch 453/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3166.0276 - mae: 3166.0276 - val_loss: 2942.6780 - val_mae: 2942.6780\n",
      "Epoch 454/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3165.6365 - mae: 3165.6365 - val_loss: 2942.9521 - val_mae: 2942.9521\n",
      "Epoch 455/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3165.2410 - mae: 3165.2410 - val_loss: 2941.3823 - val_mae: 2941.3823\n",
      "Epoch 456/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3166.6812 - mae: 3166.6812 - val_loss: 2941.5720 - val_mae: 2941.5720\n",
      "Epoch 457/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3165.8171 - mae: 3165.8171 - val_loss: 2941.9915 - val_mae: 2941.9915\n",
      "Epoch 458/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3167.3662 - mae: 3167.3662 - val_loss: 2941.0535 - val_mae: 2941.0535\n",
      "Epoch 459/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3165.5269 - mae: 3165.5269 - val_loss: 2941.1418 - val_mae: 2941.1418\n",
      "Epoch 460/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3165.4717 - mae: 3165.4717 - val_loss: 2940.4983 - val_mae: 2940.4983\n",
      "Epoch 461/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3164.9573 - mae: 3164.9573 - val_loss: 2941.1016 - val_mae: 2941.1016\n",
      "Epoch 462/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3165.6804 - mae: 3165.6804 - val_loss: 2942.8494 - val_mae: 2942.8494\n",
      "Epoch 463/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3167.3594 - mae: 3167.3594 - val_loss: 2943.3960 - val_mae: 2943.3960\n",
      "Epoch 464/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3166.0796 - mae: 3166.0796 - val_loss: 2943.4631 - val_mae: 2943.4631\n",
      "Epoch 465/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3165.2981 - mae: 3165.2981 - val_loss: 2941.3191 - val_mae: 2941.3191\n",
      "Epoch 466/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3165.8062 - mae: 3165.8062 - val_loss: 2940.9082 - val_mae: 2940.9082\n",
      "Epoch 467/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3166.2427 - mae: 3166.2427 - val_loss: 2940.9995 - val_mae: 2940.9995\n",
      "Epoch 468/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3165.7517 - mae: 3165.7517 - val_loss: 2940.5657 - val_mae: 2940.5657\n",
      "Epoch 469/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3164.7146 - mae: 3164.7146 - val_loss: 2943.0364 - val_mae: 2943.0364\n",
      "Epoch 470/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3166.8040 - mae: 3166.8040 - val_loss: 2943.4038 - val_mae: 2943.4038\n",
      "Epoch 471/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3164.6377 - mae: 3164.6377 - val_loss: 2941.8508 - val_mae: 2941.8508\n",
      "Epoch 472/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3165.3162 - mae: 3165.3162 - val_loss: 2939.5400 - val_mae: 2939.5400\n",
      "Epoch 473/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3165.0208 - mae: 3165.0208 - val_loss: 2941.3997 - val_mae: 2941.3997\n",
      "Epoch 474/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3165.5891 - mae: 3165.5891 - val_loss: 2940.8293 - val_mae: 2940.8293\n",
      "Epoch 475/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3165.3572 - mae: 3165.3572 - val_loss: 2938.9490 - val_mae: 2938.9490\n",
      "Epoch 476/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3166.1345 - mae: 3166.1345 - val_loss: 2940.8494 - val_mae: 2940.8494\n",
      "Epoch 477/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3169.4209 - mae: 3169.4209 - val_loss: 2943.2561 - val_mae: 2943.2561\n",
      "Epoch 478/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3166.5332 - mae: 3166.5332 - val_loss: 2940.3333 - val_mae: 2940.3333\n",
      "Epoch 479/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3165.3423 - mae: 3165.3423 - val_loss: 2941.0095 - val_mae: 2941.0095\n",
      "Epoch 480/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3165.4573 - mae: 3165.4573 - val_loss: 2940.2295 - val_mae: 2940.2295\n",
      "Epoch 481/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3165.1548 - mae: 3165.1548 - val_loss: 2940.7190 - val_mae: 2940.7190\n",
      "Epoch 482/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3165.2832 - mae: 3165.2832 - val_loss: 2940.1643 - val_mae: 2940.1643\n",
      "Epoch 483/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3164.1736 - mae: 3164.1736 - val_loss: 2940.9019 - val_mae: 2940.9019\n",
      "Epoch 484/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3164.6614 - mae: 3164.6614 - val_loss: 2941.5566 - val_mae: 2941.5566\n",
      "Epoch 485/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3164.8071 - mae: 3164.8071 - val_loss: 2940.5559 - val_mae: 2940.5559\n",
      "Epoch 486/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3164.4358 - mae: 3164.4358 - val_loss: 2940.3713 - val_mae: 2940.3713\n",
      "Epoch 487/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3164.4827 - mae: 3164.4827 - val_loss: 2939.4915 - val_mae: 2939.4915\n",
      "Epoch 488/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3165.0481 - mae: 3165.0481 - val_loss: 2941.9912 - val_mae: 2941.9912\n",
      "Epoch 489/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3165.6453 - mae: 3165.6453 - val_loss: 2940.3716 - val_mae: 2940.3716\n",
      "Epoch 490/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3164.2808 - mae: 3164.2808 - val_loss: 2939.9543 - val_mae: 2939.9543\n",
      "Epoch 491/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3164.0725 - mae: 3164.0725 - val_loss: 2939.8071 - val_mae: 2939.8071\n",
      "Epoch 492/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3164.8608 - mae: 3164.8608 - val_loss: 2941.2180 - val_mae: 2941.2180\n",
      "Epoch 493/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3164.6812 - mae: 3164.6812 - val_loss: 2939.2009 - val_mae: 2939.2009\n",
      "Epoch 494/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3165.9033 - mae: 3165.9033 - val_loss: 2942.8188 - val_mae: 2942.8188\n",
      "Epoch 495/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3164.9163 - mae: 3164.9163 - val_loss: 2939.4548 - val_mae: 2939.4548\n",
      "Epoch 496/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3165.1228 - mae: 3165.1228 - val_loss: 2941.6816 - val_mae: 2941.6816\n",
      "Epoch 497/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3165.0859 - mae: 3165.0859 - val_loss: 2940.5151 - val_mae: 2940.5151\n",
      "Epoch 498/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3164.1653 - mae: 3164.1653 - val_loss: 2939.2629 - val_mae: 2939.2629\n",
      "Epoch 499/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3164.4194 - mae: 3164.4194 - val_loss: 2938.7229 - val_mae: 2938.7229\n",
      "Epoch 500/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3165.7200 - mae: 3165.7200 - val_loss: 2944.9978 - val_mae: 2944.9978\n",
      "Epoch 501/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3165.3613 - mae: 3165.3613 - val_loss: 2941.2871 - val_mae: 2941.2871\n",
      "Epoch 502/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3164.2273 - mae: 3164.2273 - val_loss: 2940.3921 - val_mae: 2940.3921\n",
      "Epoch 503/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3164.0322 - mae: 3164.0322 - val_loss: 2940.4661 - val_mae: 2940.4661\n",
      "Epoch 504/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3164.4526 - mae: 3164.4526 - val_loss: 2940.1978 - val_mae: 2940.1978\n",
      "Epoch 505/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 2ms/step - loss: 3164.6426 - mae: 3164.6426 - val_loss: 2940.4800 - val_mae: 2940.4800\n",
      "Epoch 506/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3164.5371 - mae: 3164.5371 - val_loss: 2939.3335 - val_mae: 2939.3335\n",
      "Epoch 507/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3164.5471 - mae: 3164.5471 - val_loss: 2939.8447 - val_mae: 2939.8447\n",
      "Epoch 508/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3164.0481 - mae: 3164.0481 - val_loss: 2938.8425 - val_mae: 2938.8425\n",
      "Epoch 509/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3163.5610 - mae: 3163.5610 - val_loss: 2941.1799 - val_mae: 2941.1799\n",
      "Epoch 510/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3163.6941 - mae: 3163.6941 - val_loss: 2939.3643 - val_mae: 2939.3643\n",
      "Epoch 511/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3163.9800 - mae: 3163.9800 - val_loss: 2939.3826 - val_mae: 2939.3826\n",
      "Epoch 512/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3163.4800 - mae: 3163.4800 - val_loss: 2938.6973 - val_mae: 2938.6973\n",
      "Epoch 513/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3164.9131 - mae: 3164.9131 - val_loss: 2939.7029 - val_mae: 2939.7029\n",
      "Epoch 514/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3165.6306 - mae: 3165.6306 - val_loss: 2943.5115 - val_mae: 2943.5115\n",
      "Epoch 515/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3164.1013 - mae: 3164.1013 - val_loss: 2940.4304 - val_mae: 2940.4304\n",
      "Epoch 516/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3163.4775 - mae: 3163.4775 - val_loss: 2940.3516 - val_mae: 2940.3516\n",
      "Epoch 517/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3165.0830 - mae: 3165.0830 - val_loss: 2941.5854 - val_mae: 2941.5854\n",
      "Epoch 518/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3163.3979 - mae: 3163.3979 - val_loss: 2938.9995 - val_mae: 2938.9995\n",
      "Epoch 519/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3163.7788 - mae: 3163.7788 - val_loss: 2938.1760 - val_mae: 2938.1760\n",
      "Epoch 520/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3163.9026 - mae: 3163.9026 - val_loss: 2939.5012 - val_mae: 2939.5012\n",
      "Epoch 521/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3167.2346 - mae: 3167.2346 - val_loss: 2944.1721 - val_mae: 2944.1721\n",
      "Epoch 522/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3164.8616 - mae: 3164.8616 - val_loss: 2940.1108 - val_mae: 2940.1108\n",
      "Epoch 523/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3162.8948 - mae: 3162.8948 - val_loss: 2940.3157 - val_mae: 2940.3157\n",
      "Epoch 524/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3164.2029 - mae: 3164.2029 - val_loss: 2939.7544 - val_mae: 2939.7544\n",
      "Epoch 525/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3163.1101 - mae: 3163.1101 - val_loss: 2938.9099 - val_mae: 2938.9099\n",
      "Epoch 526/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3164.0400 - mae: 3164.0400 - val_loss: 2940.4084 - val_mae: 2940.4084\n",
      "Epoch 527/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3163.0825 - mae: 3163.0825 - val_loss: 2939.1555 - val_mae: 2939.1555\n",
      "Epoch 528/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3163.0415 - mae: 3163.0415 - val_loss: 2939.7766 - val_mae: 2939.7766\n",
      "Epoch 529/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3163.2502 - mae: 3163.2502 - val_loss: 2940.0608 - val_mae: 2940.0608\n",
      "Epoch 530/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3164.4924 - mae: 3164.4924 - val_loss: 2938.8169 - val_mae: 2938.8169\n",
      "Epoch 531/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3163.2571 - mae: 3163.2571 - val_loss: 2940.9712 - val_mae: 2940.9712\n",
      "Epoch 532/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3163.9111 - mae: 3163.9111 - val_loss: 2940.9329 - val_mae: 2940.9329\n",
      "Epoch 533/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3163.9075 - mae: 3163.9075 - val_loss: 2938.6641 - val_mae: 2938.6641\n",
      "Epoch 534/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3163.1899 - mae: 3163.1899 - val_loss: 2939.5117 - val_mae: 2939.5117\n",
      "Epoch 535/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3162.9009 - mae: 3162.9009 - val_loss: 2937.6252 - val_mae: 2937.6252\n",
      "Epoch 536/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3162.6277 - mae: 3162.6277 - val_loss: 2939.2827 - val_mae: 2939.2827\n",
      "Epoch 537/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3165.1519 - mae: 3165.1519 - val_loss: 2938.9915 - val_mae: 2938.9915\n",
      "Epoch 538/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3162.7439 - mae: 3162.7439 - val_loss: 2939.5234 - val_mae: 2939.5234\n",
      "Epoch 539/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3163.0413 - mae: 3163.0413 - val_loss: 2938.0972 - val_mae: 2938.0972\n",
      "Epoch 540/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3163.8159 - mae: 3163.8159 - val_loss: 2940.2747 - val_mae: 2940.2747\n",
      "Epoch 541/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3162.9163 - mae: 3162.9163 - val_loss: 2939.0898 - val_mae: 2939.0898\n",
      "Epoch 542/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3162.2246 - mae: 3162.2246 - val_loss: 2939.0085 - val_mae: 2939.0085\n",
      "Epoch 543/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3162.6135 - mae: 3162.6135 - val_loss: 2939.8003 - val_mae: 2939.8003\n",
      "Epoch 544/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3162.5190 - mae: 3162.5190 - val_loss: 2939.0627 - val_mae: 2939.0627\n",
      "Epoch 545/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3162.7278 - mae: 3162.7278 - val_loss: 2940.9268 - val_mae: 2940.9268\n",
      "Epoch 546/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3163.6055 - mae: 3163.6055 - val_loss: 2939.3062 - val_mae: 2939.3062\n",
      "Epoch 547/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3163.6082 - mae: 3163.6082 - val_loss: 2938.8142 - val_mae: 2938.8142\n",
      "Epoch 548/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3163.5354 - mae: 3163.5354 - val_loss: 2941.1465 - val_mae: 2941.1465\n",
      "Epoch 549/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3164.0706 - mae: 3164.0706 - val_loss: 2940.2195 - val_mae: 2940.2195\n",
      "Epoch 550/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3162.7048 - mae: 3162.7048 - val_loss: 2939.7673 - val_mae: 2939.7673\n",
      "Epoch 551/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3162.3977 - mae: 3162.3977 - val_loss: 2940.2805 - val_mae: 2940.2805\n",
      "Epoch 552/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3162.5881 - mae: 3162.5881 - val_loss: 2940.2468 - val_mae: 2940.2468\n",
      "Epoch 553/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3162.1169 - mae: 3162.1169 - val_loss: 2939.9507 - val_mae: 2939.9507\n",
      "Epoch 554/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3161.9404 - mae: 3161.9404 - val_loss: 2939.2686 - val_mae: 2939.2686\n",
      "Epoch 555/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3162.8401 - mae: 3162.8401 - val_loss: 2938.9578 - val_mae: 2938.9578\n",
      "Epoch 556/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3162.6880 - mae: 3162.6880 - val_loss: 2939.1914 - val_mae: 2939.1914\n",
      "Epoch 557/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3162.0974 - mae: 3162.0974 - val_loss: 2940.3447 - val_mae: 2940.3447\n",
      "Epoch 558/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3162.4565 - mae: 3162.4565 - val_loss: 2938.8494 - val_mae: 2938.8494\n",
      "Epoch 559/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3161.7898 - mae: 3161.7898 - val_loss: 2939.1614 - val_mae: 2939.1614\n",
      "Epoch 560/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3162.8943 - mae: 3162.8943 - val_loss: 2939.9011 - val_mae: 2939.9011\n",
      "Epoch 561/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 2ms/step - loss: 3162.5586 - mae: 3162.5586 - val_loss: 2940.1934 - val_mae: 2940.1934\n",
      "Epoch 562/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3162.3003 - mae: 3162.3003 - val_loss: 2940.4634 - val_mae: 2940.4634\n",
      "Epoch 563/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3162.0725 - mae: 3162.0725 - val_loss: 2940.8462 - val_mae: 2940.8462\n",
      "Epoch 564/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3163.1721 - mae: 3163.1721 - val_loss: 2941.6372 - val_mae: 2941.6372\n",
      "Epoch 565/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3161.8972 - mae: 3161.8972 - val_loss: 2939.3508 - val_mae: 2939.3508\n",
      "Epoch 566/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3161.7000 - mae: 3161.7000 - val_loss: 2939.2952 - val_mae: 2939.2952\n",
      "Epoch 567/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3162.9529 - mae: 3162.9529 - val_loss: 2939.7393 - val_mae: 2939.7393\n",
      "Epoch 568/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3161.8164 - mae: 3161.8164 - val_loss: 2938.7449 - val_mae: 2938.7449\n",
      "Epoch 569/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3162.0486 - mae: 3162.0486 - val_loss: 2941.9255 - val_mae: 2941.9255\n",
      "Epoch 570/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3163.9250 - mae: 3163.9250 - val_loss: 2939.6033 - val_mae: 2939.6033\n",
      "Epoch 571/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3162.2026 - mae: 3162.2026 - val_loss: 2940.3406 - val_mae: 2940.3406\n",
      "Epoch 572/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3161.7896 - mae: 3161.7896 - val_loss: 2939.4973 - val_mae: 2939.4973\n",
      "Epoch 573/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3161.8184 - mae: 3161.8184 - val_loss: 2938.4702 - val_mae: 2938.4702\n",
      "Epoch 574/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3162.2261 - mae: 3162.2261 - val_loss: 2938.3921 - val_mae: 2938.3921\n",
      "Epoch 575/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3162.4875 - mae: 3162.4875 - val_loss: 2938.2510 - val_mae: 2938.2510\n",
      "Epoch 576/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3162.2356 - mae: 3162.2356 - val_loss: 2939.0671 - val_mae: 2939.0671\n",
      "Epoch 577/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3162.7383 - mae: 3162.7383 - val_loss: 2937.5706 - val_mae: 2937.5706\n",
      "Epoch 578/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3161.3052 - mae: 3161.3052 - val_loss: 2937.5532 - val_mae: 2937.5532\n",
      "Epoch 579/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3161.2954 - mae: 3161.2954 - val_loss: 2941.5103 - val_mae: 2941.5103\n",
      "Epoch 580/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3161.7925 - mae: 3161.7925 - val_loss: 2939.6228 - val_mae: 2939.6228\n",
      "Epoch 581/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3161.6213 - mae: 3161.6213 - val_loss: 2939.5176 - val_mae: 2939.5176\n",
      "Epoch 582/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3161.9302 - mae: 3161.9302 - val_loss: 2940.8398 - val_mae: 2940.8398\n",
      "Epoch 583/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3162.3560 - mae: 3162.3560 - val_loss: 2940.9836 - val_mae: 2940.9836\n",
      "Epoch 584/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3162.7390 - mae: 3162.7390 - val_loss: 2940.2307 - val_mae: 2940.2307\n",
      "Epoch 585/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3161.6814 - mae: 3161.6814 - val_loss: 2938.3499 - val_mae: 2938.3499\n",
      "Epoch 586/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3161.0823 - mae: 3161.0823 - val_loss: 2939.1897 - val_mae: 2939.1897\n",
      "Epoch 587/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3161.7302 - mae: 3161.7302 - val_loss: 2938.7458 - val_mae: 2938.7458\n",
      "Epoch 588/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3161.0474 - mae: 3161.0474 - val_loss: 2938.9192 - val_mae: 2938.9192\n",
      "Epoch 589/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3160.5103 - mae: 3160.5103 - val_loss: 2939.6597 - val_mae: 2939.6597\n",
      "Epoch 590/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3161.0884 - mae: 3161.0884 - val_loss: 2940.0098 - val_mae: 2940.0098\n",
      "Epoch 591/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3163.0408 - mae: 3163.0408 - val_loss: 2939.0503 - val_mae: 2939.0503\n",
      "Epoch 592/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3160.7429 - mae: 3160.7429 - val_loss: 2941.0100 - val_mae: 2941.0100\n",
      "Epoch 593/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3160.5278 - mae: 3160.5278 - val_loss: 2939.9109 - val_mae: 2939.9109\n",
      "Epoch 594/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3160.5271 - mae: 3160.5271 - val_loss: 2940.5500 - val_mae: 2940.5500\n",
      "Epoch 595/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3160.8306 - mae: 3160.8306 - val_loss: 2940.1934 - val_mae: 2940.1934\n",
      "Epoch 596/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3161.4028 - mae: 3161.4028 - val_loss: 2939.7854 - val_mae: 2939.7854\n",
      "Epoch 597/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3161.2751 - mae: 3161.2751 - val_loss: 2939.9172 - val_mae: 2939.9172\n",
      "Epoch 598/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3161.1575 - mae: 3161.1575 - val_loss: 2939.7581 - val_mae: 2939.7581\n",
      "Epoch 599/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3160.8740 - mae: 3160.8740 - val_loss: 2940.5327 - val_mae: 2940.5327\n",
      "Epoch 600/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3160.4370 - mae: 3160.4370 - val_loss: 2940.8274 - val_mae: 2940.8274\n",
      "Epoch 601/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3160.0337 - mae: 3160.0337 - val_loss: 2940.7188 - val_mae: 2940.7188\n",
      "Epoch 602/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3160.6487 - mae: 3160.6487 - val_loss: 2942.0588 - val_mae: 2942.0588\n",
      "Epoch 603/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3160.4885 - mae: 3160.4885 - val_loss: 2938.7971 - val_mae: 2938.7971\n",
      "Epoch 604/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3160.2642 - mae: 3160.2642 - val_loss: 2939.9109 - val_mae: 2939.9109\n",
      "Epoch 605/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3160.8586 - mae: 3160.8586 - val_loss: 2939.7268 - val_mae: 2939.7268\n",
      "Epoch 606/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3160.5476 - mae: 3160.5476 - val_loss: 2939.6677 - val_mae: 2939.6677\n",
      "Epoch 607/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3159.6943 - mae: 3159.6943 - val_loss: 2941.1001 - val_mae: 2941.1001\n",
      "Epoch 608/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3160.8481 - mae: 3160.8481 - val_loss: 2940.9663 - val_mae: 2940.9663\n",
      "Epoch 609/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3160.2456 - mae: 3160.2456 - val_loss: 2940.8953 - val_mae: 2940.8953\n",
      "Epoch 610/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3162.2324 - mae: 3162.2324 - val_loss: 2942.3071 - val_mae: 2942.3071\n",
      "Epoch 611/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3160.9519 - mae: 3160.9519 - val_loss: 2939.4590 - val_mae: 2939.4590\n",
      "Epoch 612/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3161.1572 - mae: 3161.1572 - val_loss: 2942.2271 - val_mae: 2942.2271\n",
      "Epoch 613/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3165.1216 - mae: 3165.1216 - val_loss: 2940.5479 - val_mae: 2940.5479\n",
      "Epoch 614/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3161.0952 - mae: 3161.0952 - val_loss: 2938.2336 - val_mae: 2938.2336\n",
      "Epoch 615/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3160.0642 - mae: 3160.0642 - val_loss: 2940.2576 - val_mae: 2940.2576\n",
      "Epoch 616/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3159.8792 - mae: 3159.8792 - val_loss: 2941.1526 - val_mae: 2941.1526\n",
      "Epoch 617/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 2ms/step - loss: 3159.4548 - mae: 3159.4548 - val_loss: 2940.5745 - val_mae: 2940.5745\n",
      "Epoch 618/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3159.0601 - mae: 3159.0601 - val_loss: 2939.9973 - val_mae: 2939.9973\n",
      "Epoch 619/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3159.7041 - mae: 3159.7041 - val_loss: 2942.9980 - val_mae: 2942.9980\n",
      "Epoch 620/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3159.5696 - mae: 3159.5696 - val_loss: 2939.5906 - val_mae: 2939.5906\n",
      "Epoch 621/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3159.3948 - mae: 3159.3948 - val_loss: 2940.3491 - val_mae: 2940.3491\n",
      "Epoch 622/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3159.7300 - mae: 3159.7300 - val_loss: 2941.2112 - val_mae: 2941.2112\n",
      "Epoch 623/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3158.8708 - mae: 3158.8708 - val_loss: 2939.5085 - val_mae: 2939.5085\n",
      "Epoch 624/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3158.9187 - mae: 3158.9187 - val_loss: 2939.4861 - val_mae: 2939.4861\n",
      "Epoch 625/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3159.9370 - mae: 3159.9370 - val_loss: 2940.0398 - val_mae: 2940.0398\n",
      "Epoch 626/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3160.5872 - mae: 3160.5872 - val_loss: 2940.7490 - val_mae: 2940.7490\n",
      "Epoch 627/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3160.2148 - mae: 3160.2148 - val_loss: 2939.5737 - val_mae: 2939.5737\n",
      "Epoch 628/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3159.7517 - mae: 3159.7517 - val_loss: 2941.3594 - val_mae: 2941.3594\n",
      "Epoch 629/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3158.3643 - mae: 3158.3643 - val_loss: 2940.2148 - val_mae: 2940.2148\n",
      "Epoch 630/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3159.9927 - mae: 3159.9927 - val_loss: 2942.6917 - val_mae: 2942.6917\n",
      "Epoch 631/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3159.7812 - mae: 3159.7812 - val_loss: 2940.2053 - val_mae: 2940.2053\n",
      "Epoch 632/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3158.3589 - mae: 3158.3589 - val_loss: 2941.7542 - val_mae: 2941.7542\n",
      "Epoch 633/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3160.9976 - mae: 3160.9976 - val_loss: 2940.4446 - val_mae: 2940.4446\n",
      "Epoch 634/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3158.5356 - mae: 3158.5356 - val_loss: 2941.2671 - val_mae: 2941.2671\n",
      "Epoch 635/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3158.7122 - mae: 3158.7122 - val_loss: 2941.5291 - val_mae: 2941.5291\n",
      "Epoch 636/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3158.5320 - mae: 3158.5320 - val_loss: 2940.3557 - val_mae: 2940.3557\n",
      "Epoch 637/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3157.6868 - mae: 3157.6868 - val_loss: 2941.9492 - val_mae: 2941.9492\n",
      "Epoch 638/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3158.4604 - mae: 3158.4604 - val_loss: 2940.0310 - val_mae: 2940.0310\n",
      "Epoch 639/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3160.7334 - mae: 3160.7334 - val_loss: 2941.3906 - val_mae: 2941.3906\n",
      "Epoch 640/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3157.9463 - mae: 3157.9463 - val_loss: 2942.7542 - val_mae: 2942.7542\n",
      "Epoch 641/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3158.2537 - mae: 3158.2537 - val_loss: 2940.6213 - val_mae: 2940.6213\n",
      "Epoch 642/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3158.5728 - mae: 3158.5728 - val_loss: 2940.8643 - val_mae: 2940.8643\n",
      "Epoch 643/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3157.2588 - mae: 3157.2588 - val_loss: 2941.3572 - val_mae: 2941.3572\n",
      "Epoch 644/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3157.3555 - mae: 3157.3555 - val_loss: 2941.2893 - val_mae: 2941.2893\n",
      "Epoch 645/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3156.8821 - mae: 3156.8821 - val_loss: 2941.3579 - val_mae: 2941.3579\n",
      "Epoch 646/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3157.4255 - mae: 3157.4255 - val_loss: 2939.6843 - val_mae: 2939.6843\n",
      "Epoch 647/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3158.2087 - mae: 3158.2087 - val_loss: 2939.7639 - val_mae: 2939.7639\n",
      "Epoch 648/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3158.8953 - mae: 3158.8953 - val_loss: 2938.6467 - val_mae: 2938.6467\n",
      "Epoch 649/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3157.1938 - mae: 3157.1938 - val_loss: 2939.4163 - val_mae: 2939.4163\n",
      "Epoch 650/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3158.6228 - mae: 3158.6228 - val_loss: 2941.3582 - val_mae: 2941.3582\n",
      "Epoch 651/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3157.8081 - mae: 3157.8081 - val_loss: 2938.4009 - val_mae: 2938.4009\n",
      "Epoch 652/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3156.6987 - mae: 3156.6987 - val_loss: 2938.8889 - val_mae: 2938.8889\n",
      "Epoch 653/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3157.4951 - mae: 3157.4951 - val_loss: 2940.0771 - val_mae: 2940.0771\n",
      "Epoch 654/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3157.2502 - mae: 3157.2502 - val_loss: 2937.4766 - val_mae: 2937.4766\n",
      "Epoch 655/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3158.2231 - mae: 3158.2231 - val_loss: 2939.8296 - val_mae: 2939.8296\n",
      "Epoch 656/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3157.0640 - mae: 3157.0640 - val_loss: 2938.4790 - val_mae: 2938.4790\n",
      "Epoch 657/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3157.4111 - mae: 3157.4111 - val_loss: 2939.5657 - val_mae: 2939.5657\n",
      "Epoch 658/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3156.0911 - mae: 3156.0911 - val_loss: 2939.3196 - val_mae: 2939.3196\n",
      "Epoch 659/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3156.1353 - mae: 3156.1353 - val_loss: 2941.2078 - val_mae: 2941.2078\n",
      "Epoch 660/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3156.1572 - mae: 3156.1572 - val_loss: 2940.8472 - val_mae: 2940.8472\n",
      "Epoch 661/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3155.6409 - mae: 3155.6409 - val_loss: 2939.0657 - val_mae: 2939.0657\n",
      "Epoch 662/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3155.4854 - mae: 3155.4854 - val_loss: 2938.5574 - val_mae: 2938.5574\n",
      "Epoch 663/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3155.6499 - mae: 3155.6499 - val_loss: 2939.5852 - val_mae: 2939.5852\n",
      "Epoch 664/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3156.0557 - mae: 3156.0557 - val_loss: 2938.9441 - val_mae: 2938.9441\n",
      "Epoch 665/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3155.8848 - mae: 3155.8848 - val_loss: 2938.3525 - val_mae: 2938.3525\n",
      "Epoch 666/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3155.5083 - mae: 3155.5083 - val_loss: 2937.7041 - val_mae: 2937.7041\n",
      "Epoch 667/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3155.7773 - mae: 3155.7773 - val_loss: 2938.1443 - val_mae: 2938.1443\n",
      "Epoch 668/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3157.0181 - mae: 3157.0181 - val_loss: 2938.5210 - val_mae: 2938.5210\n",
      "Epoch 669/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3155.7317 - mae: 3155.7317 - val_loss: 2940.8650 - val_mae: 2940.8650\n",
      "Epoch 670/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3155.0044 - mae: 3155.0044 - val_loss: 2938.2915 - val_mae: 2938.2915\n",
      "Epoch 671/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3154.3623 - mae: 3154.3623 - val_loss: 2938.9656 - val_mae: 2938.9656\n",
      "Epoch 672/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3155.4543 - mae: 3155.4543 - val_loss: 2939.8452 - val_mae: 2939.8452\n",
      "Epoch 673/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 2ms/step - loss: 3155.5872 - mae: 3155.5872 - val_loss: 2938.3318 - val_mae: 2938.3318\n",
      "Epoch 674/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3154.3020 - mae: 3154.3020 - val_loss: 2939.2844 - val_mae: 2939.2844\n",
      "Epoch 675/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3153.8967 - mae: 3153.8967 - val_loss: 2940.4521 - val_mae: 2940.4521\n",
      "Epoch 676/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3155.0046 - mae: 3155.0046 - val_loss: 2937.9150 - val_mae: 2937.9150\n",
      "Epoch 677/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3153.8108 - mae: 3153.8108 - val_loss: 2938.5938 - val_mae: 2938.5938\n",
      "Epoch 678/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3153.8909 - mae: 3153.8909 - val_loss: 2938.3367 - val_mae: 2938.3367\n",
      "Epoch 679/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3153.4199 - mae: 3153.4199 - val_loss: 2938.4363 - val_mae: 2938.4363\n",
      "Epoch 680/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3153.1875 - mae: 3153.1875 - val_loss: 2939.6453 - val_mae: 2939.6453\n",
      "Epoch 681/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3153.8784 - mae: 3153.8784 - val_loss: 2939.9172 - val_mae: 2939.9172\n",
      "Epoch 682/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3153.4304 - mae: 3153.4304 - val_loss: 2939.3057 - val_mae: 2939.3057\n",
      "Epoch 683/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3153.2922 - mae: 3153.2922 - val_loss: 2939.8818 - val_mae: 2939.8818\n",
      "Epoch 684/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3153.6133 - mae: 3153.6133 - val_loss: 2938.8330 - val_mae: 2938.8330\n",
      "Epoch 685/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3152.4553 - mae: 3152.4553 - val_loss: 2939.8245 - val_mae: 2939.8245\n",
      "Epoch 686/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3152.8855 - mae: 3152.8855 - val_loss: 2938.8813 - val_mae: 2938.8813\n",
      "Epoch 687/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3152.4985 - mae: 3152.4985 - val_loss: 2940.8401 - val_mae: 2940.8401\n",
      "Epoch 688/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3153.2312 - mae: 3153.2312 - val_loss: 2940.0632 - val_mae: 2940.0632\n",
      "Epoch 689/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3152.2625 - mae: 3152.2625 - val_loss: 2939.6047 - val_mae: 2939.6047\n",
      "Epoch 690/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3152.6211 - mae: 3152.6211 - val_loss: 2940.1294 - val_mae: 2940.1294\n",
      "Epoch 691/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3152.9934 - mae: 3152.9934 - val_loss: 2938.9871 - val_mae: 2938.9871\n",
      "Epoch 692/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3153.6658 - mae: 3153.6658 - val_loss: 2940.1150 - val_mae: 2940.1150\n",
      "Epoch 693/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3152.2437 - mae: 3152.2437 - val_loss: 2939.7795 - val_mae: 2939.7795\n",
      "Epoch 694/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3151.7424 - mae: 3151.7424 - val_loss: 2938.3701 - val_mae: 2938.3701\n",
      "Epoch 695/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3152.0906 - mae: 3152.0906 - val_loss: 2938.7197 - val_mae: 2938.7197\n",
      "Epoch 696/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3153.0566 - mae: 3153.0566 - val_loss: 2939.7598 - val_mae: 2939.7598\n",
      "Epoch 697/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3151.6047 - mae: 3151.6047 - val_loss: 2940.2231 - val_mae: 2940.2231\n",
      "Epoch 698/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3151.9255 - mae: 3151.9255 - val_loss: 2937.0276 - val_mae: 2937.0276\n",
      "Epoch 699/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3151.3486 - mae: 3151.3486 - val_loss: 2938.3516 - val_mae: 2938.3516\n",
      "Epoch 700/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3151.8840 - mae: 3151.8840 - val_loss: 2937.4575 - val_mae: 2937.4575\n",
      "Epoch 701/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3151.1167 - mae: 3151.1167 - val_loss: 2938.6941 - val_mae: 2938.6941\n",
      "Epoch 702/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3151.0525 - mae: 3151.0525 - val_loss: 2938.9377 - val_mae: 2938.9377\n",
      "Epoch 703/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3150.9619 - mae: 3150.9619 - val_loss: 2941.6943 - val_mae: 2941.6943\n",
      "Epoch 704/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3152.3254 - mae: 3152.3254 - val_loss: 2940.1829 - val_mae: 2940.1829\n",
      "Epoch 705/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3151.1885 - mae: 3151.1885 - val_loss: 2937.7739 - val_mae: 2937.7739\n",
      "Epoch 706/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3150.0061 - mae: 3150.0061 - val_loss: 2936.3984 - val_mae: 2936.3984\n",
      "Epoch 707/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3150.3416 - mae: 3150.3416 - val_loss: 2937.5457 - val_mae: 2937.5457\n",
      "Epoch 708/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3150.3916 - mae: 3150.3916 - val_loss: 2937.3250 - val_mae: 2937.3250\n",
      "Epoch 709/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3150.3210 - mae: 3150.3210 - val_loss: 2936.5369 - val_mae: 2936.5369\n",
      "Epoch 710/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3152.2048 - mae: 3152.2048 - val_loss: 2940.4502 - val_mae: 2940.4502\n",
      "Epoch 711/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3151.2654 - mae: 3151.2654 - val_loss: 2936.1733 - val_mae: 2936.1733\n",
      "Epoch 712/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3150.6772 - mae: 3150.6772 - val_loss: 2936.2188 - val_mae: 2936.2188\n",
      "Epoch 713/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3149.9568 - mae: 3149.9568 - val_loss: 2937.4873 - val_mae: 2937.4873\n",
      "Epoch 714/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3149.7512 - mae: 3149.7512 - val_loss: 2937.4019 - val_mae: 2937.4019\n",
      "Epoch 715/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3153.8708 - mae: 3153.8708 - val_loss: 2936.6133 - val_mae: 2936.6133\n",
      "Epoch 716/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3149.3860 - mae: 3149.3860 - val_loss: 2936.8235 - val_mae: 2936.8235\n",
      "Epoch 717/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3148.6367 - mae: 3148.6367 - val_loss: 2936.5093 - val_mae: 2936.5093\n",
      "Epoch 718/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3148.6440 - mae: 3148.6440 - val_loss: 2939.9490 - val_mae: 2939.9490\n",
      "Epoch 719/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3149.6089 - mae: 3149.6089 - val_loss: 2936.9409 - val_mae: 2936.9409\n",
      "Epoch 720/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3148.4844 - mae: 3148.4844 - val_loss: 2936.6055 - val_mae: 2936.6055\n",
      "Epoch 721/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3148.5820 - mae: 3148.5820 - val_loss: 2936.2952 - val_mae: 2936.2952\n",
      "Epoch 722/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3149.3545 - mae: 3149.3545 - val_loss: 2938.7173 - val_mae: 2938.7173\n",
      "Epoch 723/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3148.8572 - mae: 3148.8572 - val_loss: 2935.8105 - val_mae: 2935.8105\n",
      "Epoch 724/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3147.1672 - mae: 3147.1672 - val_loss: 2936.0962 - val_mae: 2936.0962\n",
      "Epoch 725/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3149.4219 - mae: 3149.4219 - val_loss: 2937.4690 - val_mae: 2937.4690\n",
      "Epoch 726/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3147.3433 - mae: 3147.3433 - val_loss: 2934.9497 - val_mae: 2934.9497\n",
      "Epoch 727/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3147.4373 - mae: 3147.4373 - val_loss: 2934.4871 - val_mae: 2934.4871\n",
      "Epoch 728/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3146.8694 - mae: 3146.8694 - val_loss: 2935.7380 - val_mae: 2935.7380\n",
      "Epoch 729/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 1ms/step - loss: 3146.3127 - mae: 3146.3127 - val_loss: 2935.3799 - val_mae: 2935.3799\n",
      "Epoch 730/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3146.6516 - mae: 3146.6516 - val_loss: 2935.7686 - val_mae: 2935.7686\n",
      "Epoch 731/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3146.3223 - mae: 3146.3223 - val_loss: 2935.2412 - val_mae: 2935.2412\n",
      "Epoch 732/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3146.2053 - mae: 3146.2053 - val_loss: 2934.3628 - val_mae: 2934.3628\n",
      "Epoch 733/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3146.4282 - mae: 3146.4282 - val_loss: 2934.7781 - val_mae: 2934.7781\n",
      "Epoch 734/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3146.2556 - mae: 3146.2556 - val_loss: 2934.8528 - val_mae: 2934.8528\n",
      "Epoch 735/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3145.1382 - mae: 3145.1382 - val_loss: 2937.2036 - val_mae: 2937.2036\n",
      "Epoch 736/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3147.0798 - mae: 3147.0798 - val_loss: 2933.6392 - val_mae: 2933.6392\n",
      "Epoch 737/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3145.5073 - mae: 3145.5073 - val_loss: 2935.0642 - val_mae: 2935.0642\n",
      "Epoch 738/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3144.7510 - mae: 3144.7510 - val_loss: 2934.6699 - val_mae: 2934.6699\n",
      "Epoch 739/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3144.8965 - mae: 3144.8965 - val_loss: 2937.7839 - val_mae: 2937.7839\n",
      "Epoch 740/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3144.5549 - mae: 3144.5549 - val_loss: 2936.1072 - val_mae: 2936.1072\n",
      "Epoch 741/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3143.9141 - mae: 3143.9141 - val_loss: 2936.7109 - val_mae: 2936.7109\n",
      "Epoch 742/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3143.4036 - mae: 3143.4036 - val_loss: 2936.2366 - val_mae: 2936.2366\n",
      "Epoch 743/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3142.8354 - mae: 3142.8354 - val_loss: 2934.5925 - val_mae: 2934.5925\n",
      "Epoch 744/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3143.3953 - mae: 3143.3953 - val_loss: 2933.8508 - val_mae: 2933.8508\n",
      "Epoch 745/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3143.1089 - mae: 3143.1089 - val_loss: 2933.5867 - val_mae: 2933.5867\n",
      "Epoch 746/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3143.8740 - mae: 3143.8740 - val_loss: 2934.8875 - val_mae: 2934.8875\n",
      "Epoch 747/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3144.5181 - mae: 3144.5181 - val_loss: 2935.6160 - val_mae: 2935.6160\n",
      "Epoch 748/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3143.3643 - mae: 3143.3643 - val_loss: 2934.5591 - val_mae: 2934.5591\n",
      "Epoch 749/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3141.4961 - mae: 3141.4961 - val_loss: 2932.3943 - val_mae: 2932.3943\n",
      "Epoch 750/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3141.6230 - mae: 3141.6230 - val_loss: 2933.4299 - val_mae: 2933.4299\n",
      "Epoch 751/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3141.3757 - mae: 3141.3757 - val_loss: 2933.1282 - val_mae: 2933.1282\n",
      "Epoch 752/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3141.3943 - mae: 3141.3943 - val_loss: 2933.0188 - val_mae: 2933.0188\n",
      "Epoch 753/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3140.5789 - mae: 3140.5789 - val_loss: 2934.0022 - val_mae: 2934.0022\n",
      "Epoch 754/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3140.2463 - mae: 3140.2463 - val_loss: 2931.6389 - val_mae: 2931.6389\n",
      "Epoch 755/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3140.4399 - mae: 3140.4399 - val_loss: 2932.4963 - val_mae: 2932.4963\n",
      "Epoch 756/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3139.7234 - mae: 3139.7234 - val_loss: 2934.1633 - val_mae: 2934.1633\n",
      "Epoch 757/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3140.5498 - mae: 3140.5498 - val_loss: 2932.1560 - val_mae: 2932.1560\n",
      "Epoch 758/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3140.8145 - mae: 3140.8145 - val_loss: 2932.4045 - val_mae: 2932.4045\n",
      "Epoch 759/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3139.2693 - mae: 3139.2693 - val_loss: 2931.5537 - val_mae: 2931.5537\n",
      "Epoch 760/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3138.8975 - mae: 3138.8975 - val_loss: 2933.4905 - val_mae: 2933.4905\n",
      "Epoch 761/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3139.3389 - mae: 3139.3389 - val_loss: 2931.4534 - val_mae: 2931.4534\n",
      "Epoch 762/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3138.7561 - mae: 3138.7561 - val_loss: 2932.2905 - val_mae: 2932.2905\n",
      "Epoch 763/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3138.5618 - mae: 3138.5618 - val_loss: 2931.8774 - val_mae: 2931.8774\n",
      "Epoch 764/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3139.5129 - mae: 3139.5129 - val_loss: 2931.7642 - val_mae: 2931.7642\n",
      "Epoch 765/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3138.8330 - mae: 3138.8330 - val_loss: 2934.6318 - val_mae: 2934.6318\n",
      "Epoch 766/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3138.6917 - mae: 3138.6917 - val_loss: 2931.4382 - val_mae: 2931.4382\n",
      "Epoch 767/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3136.8218 - mae: 3136.8218 - val_loss: 2933.4688 - val_mae: 2933.4688\n",
      "Epoch 768/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3138.5237 - mae: 3138.5237 - val_loss: 2938.7434 - val_mae: 2938.7434\n",
      "Epoch 769/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3137.4561 - mae: 3137.4561 - val_loss: 2931.6772 - val_mae: 2931.6772\n",
      "Epoch 770/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3135.9277 - mae: 3135.9277 - val_loss: 2932.9270 - val_mae: 2932.9270\n",
      "Epoch 771/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3135.9348 - mae: 3135.9348 - val_loss: 2930.7646 - val_mae: 2930.7646\n",
      "Epoch 772/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3135.9119 - mae: 3135.9119 - val_loss: 2929.4331 - val_mae: 2929.4331\n",
      "Epoch 773/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3136.1768 - mae: 3136.1768 - val_loss: 2932.8596 - val_mae: 2932.8596\n",
      "Epoch 774/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3136.4890 - mae: 3136.4890 - val_loss: 2931.0229 - val_mae: 2931.0229\n",
      "Epoch 775/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3135.7949 - mae: 3135.7949 - val_loss: 2928.9631 - val_mae: 2928.9631\n",
      "Epoch 776/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3137.1194 - mae: 3137.1194 - val_loss: 2933.1289 - val_mae: 2933.1289\n",
      "Epoch 777/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3133.7280 - mae: 3133.7280 - val_loss: 2929.1150 - val_mae: 2929.1150\n",
      "Epoch 778/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3134.5376 - mae: 3134.5376 - val_loss: 2929.2273 - val_mae: 2929.2273\n",
      "Epoch 779/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3134.0208 - mae: 3134.0208 - val_loss: 2930.6328 - val_mae: 2930.6328\n",
      "Epoch 780/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3134.1731 - mae: 3134.1731 - val_loss: 2931.3591 - val_mae: 2931.3591\n",
      "Epoch 781/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3134.7314 - mae: 3134.7314 - val_loss: 2929.4873 - val_mae: 2929.4873\n",
      "Epoch 782/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3133.6123 - mae: 3133.6123 - val_loss: 2930.1743 - val_mae: 2930.1743\n",
      "Epoch 783/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3133.3054 - mae: 3133.3054 - val_loss: 2929.6821 - val_mae: 2929.6821\n",
      "Epoch 784/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3132.7566 - mae: 3132.7566 - val_loss: 2931.2112 - val_mae: 2931.2112\n",
      "Epoch 785/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 2ms/step - loss: 3132.3293 - mae: 3132.3293 - val_loss: 2929.2339 - val_mae: 2929.2339\n",
      "Epoch 786/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3131.7747 - mae: 3131.7747 - val_loss: 2928.2427 - val_mae: 2928.2427\n",
      "Epoch 787/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3132.6853 - mae: 3132.6853 - val_loss: 2928.1365 - val_mae: 2928.1365\n",
      "Epoch 788/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3131.8945 - mae: 3131.8945 - val_loss: 2930.5979 - val_mae: 2930.5979\n",
      "Epoch 789/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3131.4758 - mae: 3131.4758 - val_loss: 2926.4634 - val_mae: 2926.4634\n",
      "Epoch 790/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3131.3005 - mae: 3131.3005 - val_loss: 2928.2773 - val_mae: 2928.2773\n",
      "Epoch 791/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3131.5786 - mae: 3131.5786 - val_loss: 2927.7209 - val_mae: 2927.7209\n",
      "Epoch 792/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3130.6174 - mae: 3130.6174 - val_loss: 2927.9766 - val_mae: 2927.9766\n",
      "Epoch 793/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3129.6658 - mae: 3129.6658 - val_loss: 2925.9924 - val_mae: 2925.9924\n",
      "Epoch 794/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3129.7783 - mae: 3129.7783 - val_loss: 2926.7532 - val_mae: 2926.7532\n",
      "Epoch 795/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3130.4048 - mae: 3130.4048 - val_loss: 2925.9624 - val_mae: 2925.9624\n",
      "Epoch 796/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3129.3284 - mae: 3129.3284 - val_loss: 2925.6331 - val_mae: 2925.6331\n",
      "Epoch 797/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3128.7803 - mae: 3128.7803 - val_loss: 2926.2078 - val_mae: 2926.2078\n",
      "Epoch 798/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3128.7007 - mae: 3128.7007 - val_loss: 2926.5422 - val_mae: 2926.5422\n",
      "Epoch 799/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3128.4648 - mae: 3128.4648 - val_loss: 2927.8655 - val_mae: 2927.8655\n",
      "Epoch 800/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3128.1453 - mae: 3128.1453 - val_loss: 2924.7063 - val_mae: 2924.7063\n",
      "Epoch 801/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3127.8860 - mae: 3127.8860 - val_loss: 2925.0461 - val_mae: 2925.0461\n",
      "Epoch 802/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3127.1768 - mae: 3127.1768 - val_loss: 2924.0669 - val_mae: 2924.0669\n",
      "Epoch 803/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3126.6538 - mae: 3126.6538 - val_loss: 2923.8164 - val_mae: 2923.8164\n",
      "Epoch 804/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3126.5957 - mae: 3126.5957 - val_loss: 2925.6750 - val_mae: 2925.6750\n",
      "Epoch 805/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3126.3997 - mae: 3126.3997 - val_loss: 2923.3093 - val_mae: 2923.3093\n",
      "Epoch 806/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3126.7063 - mae: 3126.7063 - val_loss: 2926.1130 - val_mae: 2926.1130\n",
      "Epoch 807/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3127.5225 - mae: 3127.5225 - val_loss: 2924.6880 - val_mae: 2924.6880\n",
      "Epoch 808/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3124.9658 - mae: 3124.9658 - val_loss: 2923.5803 - val_mae: 2923.5803\n",
      "Epoch 809/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3125.6589 - mae: 3125.6589 - val_loss: 2923.3923 - val_mae: 2923.3923\n",
      "Epoch 810/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3125.3494 - mae: 3125.3494 - val_loss: 2921.3208 - val_mae: 2921.3208\n",
      "Epoch 811/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3124.4089 - mae: 3124.4089 - val_loss: 2920.6799 - val_mae: 2920.6799\n",
      "Epoch 812/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3125.7827 - mae: 3125.7827 - val_loss: 2923.5381 - val_mae: 2923.5381\n",
      "Epoch 813/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3124.3584 - mae: 3124.3584 - val_loss: 2920.3074 - val_mae: 2920.3074\n",
      "Epoch 814/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3124.0503 - mae: 3124.0503 - val_loss: 2920.1399 - val_mae: 2920.1399\n",
      "Epoch 815/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3123.2290 - mae: 3123.2290 - val_loss: 2919.5454 - val_mae: 2919.5454\n",
      "Epoch 816/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3123.6199 - mae: 3123.6199 - val_loss: 2919.9077 - val_mae: 2919.9077\n",
      "Epoch 817/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3122.6194 - mae: 3122.6194 - val_loss: 2919.9514 - val_mae: 2919.9514\n",
      "Epoch 818/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3123.6765 - mae: 3123.6765 - val_loss: 2921.1794 - val_mae: 2921.1794\n",
      "Epoch 819/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3122.1238 - mae: 3122.1238 - val_loss: 2921.0581 - val_mae: 2921.0581\n",
      "Epoch 820/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3121.8025 - mae: 3121.8025 - val_loss: 2921.1145 - val_mae: 2921.1145\n",
      "Epoch 821/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3121.2422 - mae: 3121.2422 - val_loss: 2919.5913 - val_mae: 2919.5913\n",
      "Epoch 822/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3120.9221 - mae: 3120.9221 - val_loss: 2919.0139 - val_mae: 2919.0139\n",
      "Epoch 823/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3120.2532 - mae: 3120.2532 - val_loss: 2918.8411 - val_mae: 2918.8411\n",
      "Epoch 824/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3120.1907 - mae: 3120.1907 - val_loss: 2919.4141 - val_mae: 2919.4141\n",
      "Epoch 825/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3120.8059 - mae: 3120.8059 - val_loss: 2924.1003 - val_mae: 2924.1003\n",
      "Epoch 826/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3120.5793 - mae: 3120.5793 - val_loss: 2919.6387 - val_mae: 2919.6387\n",
      "Epoch 827/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3119.1733 - mae: 3119.1733 - val_loss: 2921.0256 - val_mae: 2921.0256\n",
      "Epoch 828/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3120.4922 - mae: 3120.4922 - val_loss: 2917.4333 - val_mae: 2917.4333\n",
      "Epoch 829/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3118.7456 - mae: 3118.7456 - val_loss: 2919.2068 - val_mae: 2919.2068\n",
      "Epoch 830/2000\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 3119.1543 - mae: 3119.1543 - val_loss: 2918.3381 - val_mae: 2918.3381\n",
      "Epoch 831/2000\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 3121.4900 - mae: 3121.4900 - val_loss: 2920.2998 - val_mae: 2920.2998\n",
      "Epoch 832/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3118.3918 - mae: 3118.3918 - val_loss: 2920.4683 - val_mae: 2920.4683\n",
      "Epoch 833/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3117.4785 - mae: 3117.4785 - val_loss: 2918.5886 - val_mae: 2918.5886\n",
      "Epoch 834/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3116.9783 - mae: 3116.9783 - val_loss: 2917.0039 - val_mae: 2917.0039\n",
      "Epoch 835/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3117.6121 - mae: 3117.6121 - val_loss: 2919.9817 - val_mae: 2919.9817\n",
      "Epoch 836/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3117.7610 - mae: 3117.7610 - val_loss: 2918.0051 - val_mae: 2918.0051\n",
      "Epoch 837/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3116.0488 - mae: 3116.0488 - val_loss: 2917.8674 - val_mae: 2917.8674\n",
      "Epoch 838/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3115.4568 - mae: 3115.4568 - val_loss: 2916.7905 - val_mae: 2916.7905\n",
      "Epoch 839/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3116.0222 - mae: 3116.0222 - val_loss: 2917.0178 - val_mae: 2917.0178\n",
      "Epoch 840/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3115.0466 - mae: 3115.0466 - val_loss: 2917.1887 - val_mae: 2917.1887\n",
      "Epoch 841/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 2ms/step - loss: 3115.3516 - mae: 3115.3516 - val_loss: 2919.1401 - val_mae: 2919.1401\n",
      "Epoch 842/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3114.3738 - mae: 3114.3738 - val_loss: 2917.6699 - val_mae: 2917.6699\n",
      "Epoch 843/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3114.8040 - mae: 3114.8040 - val_loss: 2917.4392 - val_mae: 2917.4392\n",
      "Epoch 844/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3113.9321 - mae: 3113.9321 - val_loss: 2916.0615 - val_mae: 2916.0615\n",
      "Epoch 845/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3113.9368 - mae: 3113.9368 - val_loss: 2914.8416 - val_mae: 2914.8416\n",
      "Epoch 846/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3114.2502 - mae: 3114.2502 - val_loss: 2919.7720 - val_mae: 2919.7720\n",
      "Epoch 847/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3115.5251 - mae: 3115.5251 - val_loss: 2914.9355 - val_mae: 2914.9355\n",
      "Epoch 848/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3113.2959 - mae: 3113.2959 - val_loss: 2914.5972 - val_mae: 2914.5972\n",
      "Epoch 849/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3112.3206 - mae: 3112.3206 - val_loss: 2917.6982 - val_mae: 2917.6982\n",
      "Epoch 850/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3113.1047 - mae: 3113.1047 - val_loss: 2917.0537 - val_mae: 2917.0537\n",
      "Epoch 851/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3112.5024 - mae: 3112.5024 - val_loss: 2916.3035 - val_mae: 2916.3035\n",
      "Epoch 852/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3110.9961 - mae: 3110.9961 - val_loss: 2915.4165 - val_mae: 2915.4165\n",
      "Epoch 853/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3111.3406 - mae: 3111.3406 - val_loss: 2915.6306 - val_mae: 2915.6306\n",
      "Epoch 854/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3110.5085 - mae: 3110.5085 - val_loss: 2914.1628 - val_mae: 2914.1628\n",
      "Epoch 855/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3109.7546 - mae: 3109.7546 - val_loss: 2914.9526 - val_mae: 2914.9526\n",
      "Epoch 856/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3109.5647 - mae: 3109.5647 - val_loss: 2915.2449 - val_mae: 2915.2449\n",
      "Epoch 857/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3109.2510 - mae: 3109.2510 - val_loss: 2914.6929 - val_mae: 2914.6929\n",
      "Epoch 858/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3108.7087 - mae: 3108.7087 - val_loss: 2913.7908 - val_mae: 2913.7908\n",
      "Epoch 859/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3108.0305 - mae: 3108.0305 - val_loss: 2914.0791 - val_mae: 2914.0791\n",
      "Epoch 860/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3109.3486 - mae: 3109.3486 - val_loss: 2913.1045 - val_mae: 2913.1045\n",
      "Epoch 861/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3107.7461 - mae: 3107.7461 - val_loss: 2911.7715 - val_mae: 2911.7715\n",
      "Epoch 862/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3106.7764 - mae: 3106.7764 - val_loss: 2911.1057 - val_mae: 2911.1057\n",
      "Epoch 863/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3107.2869 - mae: 3107.2869 - val_loss: 2911.1143 - val_mae: 2911.1143\n",
      "Epoch 864/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3106.5232 - mae: 3106.5232 - val_loss: 2911.5627 - val_mae: 2911.5627\n",
      "Epoch 865/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3106.3108 - mae: 3106.3108 - val_loss: 2911.8936 - val_mae: 2911.8936\n",
      "Epoch 866/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3107.4353 - mae: 3107.4353 - val_loss: 2910.7124 - val_mae: 2910.7124\n",
      "Epoch 867/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3106.1060 - mae: 3106.1060 - val_loss: 2910.2646 - val_mae: 2910.2646\n",
      "Epoch 868/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3105.1191 - mae: 3105.1191 - val_loss: 2910.0059 - val_mae: 2910.0059\n",
      "Epoch 869/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3104.5002 - mae: 3104.5002 - val_loss: 2910.8738 - val_mae: 2910.8738\n",
      "Epoch 870/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3106.1567 - mae: 3106.1567 - val_loss: 2909.0828 - val_mae: 2909.0828\n",
      "Epoch 871/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3103.9109 - mae: 3103.9109 - val_loss: 2908.7241 - val_mae: 2908.7241\n",
      "Epoch 872/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3103.5188 - mae: 3103.5188 - val_loss: 2909.1238 - val_mae: 2909.1238\n",
      "Epoch 873/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3103.8967 - mae: 3103.8967 - val_loss: 2908.2297 - val_mae: 2908.2297\n",
      "Epoch 874/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3102.9473 - mae: 3102.9473 - val_loss: 2908.2537 - val_mae: 2908.2537\n",
      "Epoch 875/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3103.6240 - mae: 3103.6240 - val_loss: 2908.4541 - val_mae: 2908.4541\n",
      "Epoch 876/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3102.7922 - mae: 3102.7922 - val_loss: 2910.7493 - val_mae: 2910.7493\n",
      "Epoch 877/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3105.2456 - mae: 3105.2456 - val_loss: 2906.2224 - val_mae: 2906.2224\n",
      "Epoch 878/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3103.0891 - mae: 3103.0891 - val_loss: 2909.4258 - val_mae: 2909.4258\n",
      "Epoch 879/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3100.7502 - mae: 3100.7502 - val_loss: 2907.2810 - val_mae: 2907.2810\n",
      "Epoch 880/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3102.5562 - mae: 3102.5562 - val_loss: 2910.4739 - val_mae: 2910.4739\n",
      "Epoch 881/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3100.6099 - mae: 3100.6099 - val_loss: 2906.2747 - val_mae: 2906.2747\n",
      "Epoch 882/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3100.2207 - mae: 3100.2207 - val_loss: 2905.6218 - val_mae: 2905.6218\n",
      "Epoch 883/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3099.2532 - mae: 3099.2532 - val_loss: 2904.6753 - val_mae: 2904.6753\n",
      "Epoch 884/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3099.8728 - mae: 3099.8728 - val_loss: 2907.1223 - val_mae: 2907.1223\n",
      "Epoch 885/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3099.2510 - mae: 3099.2510 - val_loss: 2904.8167 - val_mae: 2904.8167\n",
      "Epoch 886/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3098.2935 - mae: 3098.2935 - val_loss: 2906.1177 - val_mae: 2906.1177\n",
      "Epoch 887/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3101.1309 - mae: 3101.1309 - val_loss: 2905.4409 - val_mae: 2905.4409\n",
      "Epoch 888/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3097.8982 - mae: 3097.8982 - val_loss: 2904.2805 - val_mae: 2904.2805\n",
      "Epoch 889/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3098.4204 - mae: 3098.4204 - val_loss: 2905.2693 - val_mae: 2905.2693\n",
      "Epoch 890/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3098.6660 - mae: 3098.6660 - val_loss: 2904.0723 - val_mae: 2904.0723\n",
      "Epoch 891/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3096.5564 - mae: 3096.5564 - val_loss: 2903.4617 - val_mae: 2903.4617\n",
      "Epoch 892/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3095.4255 - mae: 3095.4255 - val_loss: 2904.1597 - val_mae: 2904.1597\n",
      "Epoch 893/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3095.1731 - mae: 3095.1731 - val_loss: 2904.3062 - val_mae: 2904.3062\n",
      "Epoch 894/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3095.3547 - mae: 3095.3547 - val_loss: 2905.5815 - val_mae: 2905.5815\n",
      "Epoch 895/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3095.5667 - mae: 3095.5667 - val_loss: 2900.4363 - val_mae: 2900.4363\n",
      "Epoch 896/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3094.3220 - mae: 3094.3220 - val_loss: 2902.8960 - val_mae: 2902.8960\n",
      "Epoch 897/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 1ms/step - loss: 3094.4924 - mae: 3094.4924 - val_loss: 2901.3916 - val_mae: 2901.3916\n",
      "Epoch 898/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3093.2275 - mae: 3093.2275 - val_loss: 2900.3330 - val_mae: 2900.3330\n",
      "Epoch 899/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3092.7258 - mae: 3092.7258 - val_loss: 2901.3672 - val_mae: 2901.3672\n",
      "Epoch 900/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3092.3086 - mae: 3092.3086 - val_loss: 2900.3657 - val_mae: 2900.3657\n",
      "Epoch 901/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3092.3984 - mae: 3092.3984 - val_loss: 2904.7869 - val_mae: 2904.7869\n",
      "Epoch 902/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3091.8152 - mae: 3091.8152 - val_loss: 2900.2180 - val_mae: 2900.2180\n",
      "Epoch 903/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3090.4348 - mae: 3090.4348 - val_loss: 2899.4727 - val_mae: 2899.4727\n",
      "Epoch 904/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3090.7295 - mae: 3090.7295 - val_loss: 2900.0518 - val_mae: 2900.0518\n",
      "Epoch 905/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3090.1926 - mae: 3090.1926 - val_loss: 2900.0845 - val_mae: 2900.0845\n",
      "Epoch 906/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3089.0430 - mae: 3089.0430 - val_loss: 2899.3818 - val_mae: 2899.3818\n",
      "Epoch 907/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3090.0967 - mae: 3090.0967 - val_loss: 2898.1226 - val_mae: 2898.1226\n",
      "Epoch 908/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3089.0176 - mae: 3089.0176 - val_loss: 2898.2769 - val_mae: 2898.2769\n",
      "Epoch 909/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3087.8374 - mae: 3087.8374 - val_loss: 2896.9841 - val_mae: 2896.9841\n",
      "Epoch 910/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3088.1072 - mae: 3088.1072 - val_loss: 2897.0842 - val_mae: 2897.0842\n",
      "Epoch 911/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3086.6262 - mae: 3086.6262 - val_loss: 2896.9050 - val_mae: 2896.9050\n",
      "Epoch 912/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3089.2583 - mae: 3089.2583 - val_loss: 2894.1819 - val_mae: 2894.1819\n",
      "Epoch 913/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3087.2397 - mae: 3087.2397 - val_loss: 2894.4287 - val_mae: 2894.4287\n",
      "Epoch 914/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3085.3811 - mae: 3085.3811 - val_loss: 2896.5359 - val_mae: 2896.5359\n",
      "Epoch 915/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3085.7603 - mae: 3085.7603 - val_loss: 2896.5037 - val_mae: 2896.5037\n",
      "Epoch 916/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3084.0564 - mae: 3084.0564 - val_loss: 2893.4172 - val_mae: 2893.4172\n",
      "Epoch 917/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3084.5469 - mae: 3084.5469 - val_loss: 2893.0991 - val_mae: 2893.0991\n",
      "Epoch 918/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3084.3967 - mae: 3084.3967 - val_loss: 2893.4331 - val_mae: 2893.4331\n",
      "Epoch 919/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3083.0952 - mae: 3083.0952 - val_loss: 2894.0249 - val_mae: 2894.0249\n",
      "Epoch 920/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3082.2256 - mae: 3082.2256 - val_loss: 2893.6594 - val_mae: 2893.6594\n",
      "Epoch 921/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3082.0251 - mae: 3082.0251 - val_loss: 2891.6223 - val_mae: 2891.6223\n",
      "Epoch 922/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3081.5117 - mae: 3081.5117 - val_loss: 2892.4404 - val_mae: 2892.4404\n",
      "Epoch 923/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3081.5156 - mae: 3081.5156 - val_loss: 2890.0601 - val_mae: 2890.0601\n",
      "Epoch 924/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3080.8318 - mae: 3080.8318 - val_loss: 2890.7388 - val_mae: 2890.7388\n",
      "Epoch 925/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3079.2139 - mae: 3079.2139 - val_loss: 2893.2649 - val_mae: 2893.2649\n",
      "Epoch 926/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3078.7471 - mae: 3078.7471 - val_loss: 2890.5266 - val_mae: 2890.5266\n",
      "Epoch 927/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3078.9998 - mae: 3078.9998 - val_loss: 2888.3350 - val_mae: 2888.3350\n",
      "Epoch 928/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3077.9065 - mae: 3077.9065 - val_loss: 2887.6602 - val_mae: 2887.6602\n",
      "Epoch 929/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3077.3865 - mae: 3077.3865 - val_loss: 2889.4541 - val_mae: 2889.4541\n",
      "Epoch 930/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3076.0623 - mae: 3076.0623 - val_loss: 2887.0281 - val_mae: 2887.0281\n",
      "Epoch 931/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3074.9514 - mae: 3074.9514 - val_loss: 2886.2654 - val_mae: 2886.2654\n",
      "Epoch 932/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3074.6057 - mae: 3074.6057 - val_loss: 2884.7305 - val_mae: 2884.7305\n",
      "Epoch 933/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3073.4563 - mae: 3073.4563 - val_loss: 2884.4126 - val_mae: 2884.4126\n",
      "Epoch 934/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3073.1169 - mae: 3073.1169 - val_loss: 2883.5271 - val_mae: 2883.5271\n",
      "Epoch 935/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3072.2397 - mae: 3072.2397 - val_loss: 2882.7634 - val_mae: 2882.7634\n",
      "Epoch 936/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3072.0447 - mae: 3072.0447 - val_loss: 2882.2271 - val_mae: 2882.2271\n",
      "Epoch 937/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3070.7598 - mae: 3070.7598 - val_loss: 2882.4167 - val_mae: 2882.4167\n",
      "Epoch 938/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3070.2620 - mae: 3070.2620 - val_loss: 2882.7581 - val_mae: 2882.7581\n",
      "Epoch 939/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3070.3245 - mae: 3070.3245 - val_loss: 2881.8655 - val_mae: 2881.8655\n",
      "Epoch 940/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3070.3831 - mae: 3070.3831 - val_loss: 2884.7937 - val_mae: 2884.7937\n",
      "Epoch 941/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3069.2256 - mae: 3069.2256 - val_loss: 2882.4568 - val_mae: 2882.4568\n",
      "Epoch 942/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3067.0461 - mae: 3067.0461 - val_loss: 2881.6453 - val_mae: 2881.6453\n",
      "Epoch 943/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3066.9116 - mae: 3066.9116 - val_loss: 2880.3083 - val_mae: 2880.3083\n",
      "Epoch 944/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3065.1458 - mae: 3065.1458 - val_loss: 2880.2415 - val_mae: 2880.2415\n",
      "Epoch 945/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3065.1223 - mae: 3065.1223 - val_loss: 2880.5088 - val_mae: 2880.5088\n",
      "Epoch 946/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3064.0896 - mae: 3064.0896 - val_loss: 2877.7441 - val_mae: 2877.7441\n",
      "Epoch 947/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3061.9578 - mae: 3061.9578 - val_loss: 2877.6917 - val_mae: 2877.6917\n",
      "Epoch 948/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3061.6301 - mae: 3061.6301 - val_loss: 2876.6160 - val_mae: 2876.6160\n",
      "Epoch 949/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3062.1799 - mae: 3062.1799 - val_loss: 2873.6870 - val_mae: 2873.6870\n",
      "Epoch 950/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3061.2129 - mae: 3061.2129 - val_loss: 2874.6506 - val_mae: 2874.6506\n",
      "Epoch 951/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3059.7590 - mae: 3059.7590 - val_loss: 2874.7339 - val_mae: 2874.7339\n",
      "Epoch 952/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3057.9048 - mae: 3057.9048 - val_loss: 2873.0840 - val_mae: 2873.0840\n",
      "Epoch 953/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 1ms/step - loss: 3057.1030 - mae: 3057.1030 - val_loss: 2870.9988 - val_mae: 2870.9988\n",
      "Epoch 954/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3056.8169 - mae: 3056.8169 - val_loss: 2871.1426 - val_mae: 2871.1426\n",
      "Epoch 955/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3054.6345 - mae: 3054.6345 - val_loss: 2871.2505 - val_mae: 2871.2505\n",
      "Epoch 956/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3054.2429 - mae: 3054.2429 - val_loss: 2869.9260 - val_mae: 2869.9260\n",
      "Epoch 957/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3052.6904 - mae: 3052.6904 - val_loss: 2870.1680 - val_mae: 2870.1680\n",
      "Epoch 958/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3051.5938 - mae: 3051.5938 - val_loss: 2869.6802 - val_mae: 2869.6802\n",
      "Epoch 959/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3050.3091 - mae: 3050.3091 - val_loss: 2867.9109 - val_mae: 2867.9109\n",
      "Epoch 960/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3048.2668 - mae: 3048.2668 - val_loss: 2867.3840 - val_mae: 2867.3840\n",
      "Epoch 961/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3048.8276 - mae: 3048.8276 - val_loss: 2866.1326 - val_mae: 2866.1326\n",
      "Epoch 962/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3046.2837 - mae: 3046.2837 - val_loss: 2864.0037 - val_mae: 2864.0037\n",
      "Epoch 963/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3044.3596 - mae: 3044.3596 - val_loss: 2863.8501 - val_mae: 2863.8501\n",
      "Epoch 964/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3043.7532 - mae: 3043.7532 - val_loss: 2863.3074 - val_mae: 2863.3074\n",
      "Epoch 965/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3043.5171 - mae: 3043.5171 - val_loss: 2863.1262 - val_mae: 2863.1262\n",
      "Epoch 966/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3040.7629 - mae: 3040.7629 - val_loss: 2861.4915 - val_mae: 2861.4915\n",
      "Epoch 967/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3038.8733 - mae: 3038.8733 - val_loss: 2861.2349 - val_mae: 2861.2349\n",
      "Epoch 968/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3037.4956 - mae: 3037.4956 - val_loss: 2860.5024 - val_mae: 2860.5024\n",
      "Epoch 969/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3036.1187 - mae: 3036.1187 - val_loss: 2860.7280 - val_mae: 2860.7280\n",
      "Epoch 970/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3034.9209 - mae: 3034.9209 - val_loss: 2859.0007 - val_mae: 2859.0007\n",
      "Epoch 971/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3033.1438 - mae: 3033.1438 - val_loss: 2859.4607 - val_mae: 2859.4607\n",
      "Epoch 972/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3031.8669 - mae: 3031.8669 - val_loss: 2857.2297 - val_mae: 2857.2297\n",
      "Epoch 973/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3031.7209 - mae: 3031.7209 - val_loss: 2857.2446 - val_mae: 2857.2446\n",
      "Epoch 974/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3032.2107 - mae: 3032.2107 - val_loss: 2853.9844 - val_mae: 2853.9844\n",
      "Epoch 975/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3027.8535 - mae: 3027.8535 - val_loss: 2853.6584 - val_mae: 2853.6584\n",
      "Epoch 976/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3026.0364 - mae: 3026.0364 - val_loss: 2853.7893 - val_mae: 2853.7893\n",
      "Epoch 977/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3025.0491 - mae: 3025.0491 - val_loss: 2852.2512 - val_mae: 2852.2512\n",
      "Epoch 978/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3023.5820 - mae: 3023.5820 - val_loss: 2851.9827 - val_mae: 2851.9827\n",
      "Epoch 979/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3021.5725 - mae: 3021.5725 - val_loss: 2850.1062 - val_mae: 2850.1062\n",
      "Epoch 980/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3020.0969 - mae: 3020.0969 - val_loss: 2848.7109 - val_mae: 2848.7109\n",
      "Epoch 981/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3018.9648 - mae: 3018.9648 - val_loss: 2846.6099 - val_mae: 2846.6099\n",
      "Epoch 982/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3018.4260 - mae: 3018.4260 - val_loss: 2846.4519 - val_mae: 2846.4519\n",
      "Epoch 983/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3017.0208 - mae: 3017.0208 - val_loss: 2848.4341 - val_mae: 2848.4341\n",
      "Epoch 984/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3015.5537 - mae: 3015.5537 - val_loss: 2845.9910 - val_mae: 2845.9910\n",
      "Epoch 985/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3013.9893 - mae: 3013.9893 - val_loss: 2846.3315 - val_mae: 2846.3315\n",
      "Epoch 986/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3012.0598 - mae: 3012.0598 - val_loss: 2844.1672 - val_mae: 2844.1672\n",
      "Epoch 987/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3010.6951 - mae: 3010.6951 - val_loss: 2843.0303 - val_mae: 2843.0303\n",
      "Epoch 988/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3009.2764 - mae: 3009.2764 - val_loss: 2843.0186 - val_mae: 2843.0186\n",
      "Epoch 989/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3010.3672 - mae: 3010.3672 - val_loss: 2839.8381 - val_mae: 2839.8381\n",
      "Epoch 990/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3006.9985 - mae: 3006.9985 - val_loss: 2838.2651 - val_mae: 2838.2651\n",
      "Epoch 991/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3006.1147 - mae: 3006.1147 - val_loss: 2838.6130 - val_mae: 2838.6130\n",
      "Epoch 992/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3004.9331 - mae: 3004.9331 - val_loss: 2837.7969 - val_mae: 2837.7969\n",
      "Epoch 993/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3003.5103 - mae: 3003.5103 - val_loss: 2835.6597 - val_mae: 2835.6597\n",
      "Epoch 994/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3003.9768 - mae: 3003.9768 - val_loss: 2834.6514 - val_mae: 2834.6514\n",
      "Epoch 995/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3001.8950 - mae: 3001.8950 - val_loss: 2838.1340 - val_mae: 2838.1340\n",
      "Epoch 996/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2999.4907 - mae: 2999.4907 - val_loss: 2833.2222 - val_mae: 2833.2222\n",
      "Epoch 997/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2998.7104 - mae: 2998.7104 - val_loss: 2832.8459 - val_mae: 2832.8459\n",
      "Epoch 998/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2997.0566 - mae: 2997.0566 - val_loss: 2831.6279 - val_mae: 2831.6279\n",
      "Epoch 999/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2995.6482 - mae: 2995.6482 - val_loss: 2831.3467 - val_mae: 2831.3467\n",
      "Epoch 1000/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2994.9932 - mae: 2994.9932 - val_loss: 2829.1902 - val_mae: 2829.1902\n",
      "Epoch 1001/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2993.0776 - mae: 2993.0776 - val_loss: 2828.5674 - val_mae: 2828.5674\n",
      "Epoch 1002/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2991.7903 - mae: 2991.7903 - val_loss: 2830.4739 - val_mae: 2830.4739\n",
      "Epoch 1003/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2991.2859 - mae: 2991.2859 - val_loss: 2829.2837 - val_mae: 2829.2837\n",
      "Epoch 1004/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2989.6929 - mae: 2989.6929 - val_loss: 2825.8726 - val_mae: 2825.8726\n",
      "Epoch 1005/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2988.7932 - mae: 2988.7932 - val_loss: 2824.8367 - val_mae: 2824.8367\n",
      "Epoch 1006/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2986.7266 - mae: 2986.7266 - val_loss: 2824.3208 - val_mae: 2824.3208\n",
      "Epoch 1007/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2986.2275 - mae: 2986.2275 - val_loss: 2824.9358 - val_mae: 2824.9358\n",
      "Epoch 1008/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2985.3633 - mae: 2985.3633 - val_loss: 2822.9216 - val_mae: 2822.9216\n",
      "Epoch 1009/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 2ms/step - loss: 2983.6350 - mae: 2983.6350 - val_loss: 2822.2795 - val_mae: 2822.2795\n",
      "Epoch 1010/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2983.2302 - mae: 2983.2302 - val_loss: 2821.1379 - val_mae: 2821.1379\n",
      "Epoch 1011/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2981.8667 - mae: 2981.8667 - val_loss: 2821.0881 - val_mae: 2821.0881\n",
      "Epoch 1012/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2981.1147 - mae: 2981.1147 - val_loss: 2822.2280 - val_mae: 2822.2280\n",
      "Epoch 1013/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2979.4207 - mae: 2979.4207 - val_loss: 2818.4031 - val_mae: 2818.4031\n",
      "Epoch 1014/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2979.4080 - mae: 2979.4080 - val_loss: 2820.0564 - val_mae: 2820.0564\n",
      "Epoch 1015/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2978.8264 - mae: 2978.8264 - val_loss: 2815.7673 - val_mae: 2815.7673\n",
      "Epoch 1016/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2975.8909 - mae: 2975.8909 - val_loss: 2816.9814 - val_mae: 2816.9814\n",
      "Epoch 1017/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2974.4546 - mae: 2974.4546 - val_loss: 2816.9514 - val_mae: 2816.9514\n",
      "Epoch 1018/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2973.3330 - mae: 2973.3330 - val_loss: 2815.9741 - val_mae: 2815.9741\n",
      "Epoch 1019/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2972.5962 - mae: 2972.5962 - val_loss: 2814.1987 - val_mae: 2814.1987\n",
      "Epoch 1020/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2970.3213 - mae: 2970.3213 - val_loss: 2813.4941 - val_mae: 2813.4941\n",
      "Epoch 1021/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2969.2107 - mae: 2969.2107 - val_loss: 2812.1096 - val_mae: 2812.1096\n",
      "Epoch 1022/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2969.4817 - mae: 2969.4817 - val_loss: 2811.4683 - val_mae: 2811.4683\n",
      "Epoch 1023/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2967.5503 - mae: 2967.5503 - val_loss: 2810.8801 - val_mae: 2810.8801\n",
      "Epoch 1024/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2965.9463 - mae: 2965.9463 - val_loss: 2810.5835 - val_mae: 2810.5835\n",
      "Epoch 1025/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2963.8333 - mae: 2963.8333 - val_loss: 2809.5061 - val_mae: 2809.5061\n",
      "Epoch 1026/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2963.2595 - mae: 2963.2595 - val_loss: 2809.2412 - val_mae: 2809.2412\n",
      "Epoch 1027/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2961.5032 - mae: 2961.5032 - val_loss: 2806.0957 - val_mae: 2806.0957\n",
      "Epoch 1028/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2960.6382 - mae: 2960.6382 - val_loss: 2804.8887 - val_mae: 2804.8887\n",
      "Epoch 1029/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2959.1704 - mae: 2959.1704 - val_loss: 2805.2461 - val_mae: 2805.2461\n",
      "Epoch 1030/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2957.4080 - mae: 2957.4080 - val_loss: 2805.3879 - val_mae: 2805.3879\n",
      "Epoch 1031/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2956.5911 - mae: 2956.5911 - val_loss: 2804.7358 - val_mae: 2804.7358\n",
      "Epoch 1032/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2956.9517 - mae: 2956.9517 - val_loss: 2802.4104 - val_mae: 2802.4104\n",
      "Epoch 1033/2000\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 2954.5186 - mae: 2954.5186 - val_loss: 2803.6096 - val_mae: 2803.6096\n",
      "Epoch 1034/2000\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 2952.9495 - mae: 2952.9495 - val_loss: 2801.7710 - val_mae: 2801.7710\n",
      "Epoch 1035/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2951.2461 - mae: 2951.2461 - val_loss: 2799.9185 - val_mae: 2799.9185\n",
      "Epoch 1036/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2950.0881 - mae: 2950.0881 - val_loss: 2799.7200 - val_mae: 2799.7200\n",
      "Epoch 1037/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2948.4392 - mae: 2948.4392 - val_loss: 2798.8516 - val_mae: 2798.8516\n",
      "Epoch 1038/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2948.0959 - mae: 2948.0959 - val_loss: 2797.3472 - val_mae: 2797.3472\n",
      "Epoch 1039/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2946.0591 - mae: 2946.0591 - val_loss: 2796.6323 - val_mae: 2796.6323\n",
      "Epoch 1040/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2944.5488 - mae: 2944.5488 - val_loss: 2795.5820 - val_mae: 2795.5820\n",
      "Epoch 1041/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2943.4661 - mae: 2943.4661 - val_loss: 2794.3540 - val_mae: 2794.3540\n",
      "Epoch 1042/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2942.1018 - mae: 2942.1018 - val_loss: 2792.7251 - val_mae: 2792.7251\n",
      "Epoch 1043/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2940.9353 - mae: 2940.9353 - val_loss: 2791.7231 - val_mae: 2791.7231\n",
      "Epoch 1044/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2940.1272 - mae: 2940.1272 - val_loss: 2791.8120 - val_mae: 2791.8120\n",
      "Epoch 1045/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2938.5132 - mae: 2938.5132 - val_loss: 2791.0076 - val_mae: 2791.0076\n",
      "Epoch 1046/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2937.6328 - mae: 2937.6328 - val_loss: 2787.6816 - val_mae: 2787.6816\n",
      "Epoch 1047/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2935.9749 - mae: 2935.9749 - val_loss: 2788.6270 - val_mae: 2788.6270\n",
      "Epoch 1048/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2934.7402 - mae: 2934.7402 - val_loss: 2786.7429 - val_mae: 2786.7429\n",
      "Epoch 1049/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2934.1980 - mae: 2934.1980 - val_loss: 2786.7090 - val_mae: 2786.7090\n",
      "Epoch 1050/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2932.6306 - mae: 2932.6306 - val_loss: 2785.8145 - val_mae: 2785.8145\n",
      "Epoch 1051/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2931.3823 - mae: 2931.3823 - val_loss: 2783.9727 - val_mae: 2783.9727\n",
      "Epoch 1052/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2929.3423 - mae: 2929.3423 - val_loss: 2784.0989 - val_mae: 2784.0989\n",
      "Epoch 1053/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2928.6404 - mae: 2928.6404 - val_loss: 2782.3679 - val_mae: 2782.3679\n",
      "Epoch 1054/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2926.9404 - mae: 2926.9404 - val_loss: 2781.5266 - val_mae: 2781.5266\n",
      "Epoch 1055/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2926.3613 - mae: 2926.3613 - val_loss: 2780.5701 - val_mae: 2780.5701\n",
      "Epoch 1056/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2925.0557 - mae: 2925.0557 - val_loss: 2778.2666 - val_mae: 2778.2666\n",
      "Epoch 1057/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2923.3547 - mae: 2923.3547 - val_loss: 2777.1086 - val_mae: 2777.1086\n",
      "Epoch 1058/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2922.1592 - mae: 2922.1592 - val_loss: 2775.8411 - val_mae: 2775.8411\n",
      "Epoch 1059/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2924.0891 - mae: 2924.0891 - val_loss: 2776.0208 - val_mae: 2776.0208\n",
      "Epoch 1060/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2921.1101 - mae: 2921.1101 - val_loss: 2774.1499 - val_mae: 2774.1499\n",
      "Epoch 1061/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2920.7588 - mae: 2920.7588 - val_loss: 2774.9268 - val_mae: 2774.9268\n",
      "Epoch 1062/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2918.1760 - mae: 2918.1760 - val_loss: 2770.5500 - val_mae: 2770.5500\n",
      "Epoch 1063/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2915.7134 - mae: 2915.7134 - val_loss: 2770.4771 - val_mae: 2770.4771\n",
      "Epoch 1064/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2914.6890 - mae: 2914.6890 - val_loss: 2768.7173 - val_mae: 2768.7173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1065/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2913.4026 - mae: 2913.4026 - val_loss: 2767.8281 - val_mae: 2767.8281\n",
      "Epoch 1066/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2911.6648 - mae: 2911.6648 - val_loss: 2767.3301 - val_mae: 2767.3301\n",
      "Epoch 1067/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2910.7561 - mae: 2910.7561 - val_loss: 2767.9829 - val_mae: 2767.9829\n",
      "Epoch 1068/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2911.1628 - mae: 2911.1628 - val_loss: 2764.1428 - val_mae: 2764.1428\n",
      "Epoch 1069/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2908.8752 - mae: 2908.8752 - val_loss: 2763.8240 - val_mae: 2763.8240\n",
      "Epoch 1070/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2907.4482 - mae: 2907.4482 - val_loss: 2762.8547 - val_mae: 2762.8547\n",
      "Epoch 1071/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2907.2856 - mae: 2907.2856 - val_loss: 2763.8284 - val_mae: 2763.8284\n",
      "Epoch 1072/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2904.7786 - mae: 2904.7786 - val_loss: 2761.6934 - val_mae: 2761.6934\n",
      "Epoch 1073/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2905.5010 - mae: 2905.5010 - val_loss: 2761.3738 - val_mae: 2761.3738\n",
      "Epoch 1074/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2903.5227 - mae: 2903.5227 - val_loss: 2758.6460 - val_mae: 2758.6460\n",
      "Epoch 1075/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2901.7676 - mae: 2901.7676 - val_loss: 2758.9370 - val_mae: 2758.9370\n",
      "Epoch 1076/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2900.9160 - mae: 2900.9160 - val_loss: 2758.9180 - val_mae: 2758.9180\n",
      "Epoch 1077/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2900.0024 - mae: 2900.0024 - val_loss: 2755.4834 - val_mae: 2755.4834\n",
      "Epoch 1078/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2898.1543 - mae: 2898.1543 - val_loss: 2755.8777 - val_mae: 2755.8777\n",
      "Epoch 1079/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2896.4287 - mae: 2896.4287 - val_loss: 2755.3928 - val_mae: 2755.3928\n",
      "Epoch 1080/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2895.3127 - mae: 2895.3127 - val_loss: 2753.1892 - val_mae: 2753.1892\n",
      "Epoch 1081/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2894.5884 - mae: 2894.5884 - val_loss: 2751.9443 - val_mae: 2751.9443\n",
      "Epoch 1082/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2893.3008 - mae: 2893.3008 - val_loss: 2751.1963 - val_mae: 2751.1963\n",
      "Epoch 1083/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2891.9263 - mae: 2891.9263 - val_loss: 2749.6863 - val_mae: 2749.6863\n",
      "Epoch 1084/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2891.6580 - mae: 2891.6580 - val_loss: 2751.3665 - val_mae: 2751.3665\n",
      "Epoch 1085/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2890.4834 - mae: 2890.4834 - val_loss: 2749.4131 - val_mae: 2749.4131\n",
      "Epoch 1086/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2889.4087 - mae: 2889.4087 - val_loss: 2749.0620 - val_mae: 2749.0620\n",
      "Epoch 1087/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2886.8169 - mae: 2886.8169 - val_loss: 2748.2354 - val_mae: 2748.2354\n",
      "Epoch 1088/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2886.8137 - mae: 2886.8137 - val_loss: 2749.3977 - val_mae: 2749.3977\n",
      "Epoch 1089/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2886.3269 - mae: 2886.3269 - val_loss: 2748.9165 - val_mae: 2748.9165\n",
      "Epoch 1090/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2883.9771 - mae: 2883.9771 - val_loss: 2745.7979 - val_mae: 2745.7979\n",
      "Epoch 1091/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2882.4487 - mae: 2882.4487 - val_loss: 2744.2351 - val_mae: 2744.2351\n",
      "Epoch 1092/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2880.9631 - mae: 2880.9631 - val_loss: 2744.5217 - val_mae: 2744.5217\n",
      "Epoch 1093/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2880.7263 - mae: 2880.7263 - val_loss: 2743.6902 - val_mae: 2743.6902\n",
      "Epoch 1094/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2878.6125 - mae: 2878.6125 - val_loss: 2743.0488 - val_mae: 2743.0488\n",
      "Epoch 1095/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2877.1245 - mae: 2877.1245 - val_loss: 2742.2915 - val_mae: 2742.2915\n",
      "Epoch 1096/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2875.5911 - mae: 2875.5911 - val_loss: 2741.3271 - val_mae: 2741.3271\n",
      "Epoch 1097/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2874.4734 - mae: 2874.4734 - val_loss: 2740.7632 - val_mae: 2740.7632\n",
      "Epoch 1098/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2873.1497 - mae: 2873.1497 - val_loss: 2740.0242 - val_mae: 2740.0242\n",
      "Epoch 1099/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2871.7722 - mae: 2871.7722 - val_loss: 2739.6633 - val_mae: 2739.6633\n",
      "Epoch 1100/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2871.4402 - mae: 2871.4402 - val_loss: 2737.9624 - val_mae: 2737.9624\n",
      "Epoch 1101/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2869.6816 - mae: 2869.6816 - val_loss: 2737.0413 - val_mae: 2737.0413\n",
      "Epoch 1102/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2868.6199 - mae: 2868.6199 - val_loss: 2738.3218 - val_mae: 2738.3218\n",
      "Epoch 1103/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2868.1277 - mae: 2868.1277 - val_loss: 2733.3601 - val_mae: 2733.3601\n",
      "Epoch 1104/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2865.5576 - mae: 2865.5576 - val_loss: 2737.1948 - val_mae: 2737.1948\n",
      "Epoch 1105/2000\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 2865.4407 - mae: 2865.4407 - val_loss: 2733.1721 - val_mae: 2733.1721\n",
      "Epoch 1106/2000\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 2863.8088 - mae: 2863.8088 - val_loss: 2733.7100 - val_mae: 2733.7100\n",
      "Epoch 1107/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2863.0708 - mae: 2863.0708 - val_loss: 2736.6985 - val_mae: 2736.6985\n",
      "Epoch 1108/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2861.3428 - mae: 2861.3428 - val_loss: 2732.0977 - val_mae: 2732.0977\n",
      "Epoch 1109/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2858.9587 - mae: 2858.9587 - val_loss: 2731.6079 - val_mae: 2731.6079\n",
      "Epoch 1110/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2860.0161 - mae: 2860.0161 - val_loss: 2730.6650 - val_mae: 2730.6650\n",
      "Epoch 1111/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2857.2163 - mae: 2857.2163 - val_loss: 2729.1189 - val_mae: 2729.1189\n",
      "Epoch 1112/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2855.8594 - mae: 2855.8594 - val_loss: 2726.4280 - val_mae: 2726.4280\n",
      "Epoch 1113/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2855.4136 - mae: 2855.4136 - val_loss: 2727.7427 - val_mae: 2727.7427\n",
      "Epoch 1114/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2853.1960 - mae: 2853.1960 - val_loss: 2726.7173 - val_mae: 2726.7173\n",
      "Epoch 1115/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2852.3542 - mae: 2852.3542 - val_loss: 2725.3374 - val_mae: 2725.3374\n",
      "Epoch 1116/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2851.7505 - mae: 2851.7505 - val_loss: 2725.2319 - val_mae: 2725.2319\n",
      "Epoch 1117/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2849.8730 - mae: 2849.8730 - val_loss: 2723.7339 - val_mae: 2723.7339\n",
      "Epoch 1118/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2849.0469 - mae: 2849.0469 - val_loss: 2722.6187 - val_mae: 2722.6187\n",
      "Epoch 1119/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2846.2026 - mae: 2846.2026 - val_loss: 2722.1492 - val_mae: 2722.1492\n",
      "Epoch 1120/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2845.3740 - mae: 2845.3740 - val_loss: 2721.6843 - val_mae: 2721.6843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1121/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2843.6541 - mae: 2843.6541 - val_loss: 2719.9832 - val_mae: 2719.9832\n",
      "Epoch 1122/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2842.7844 - mae: 2842.7844 - val_loss: 2720.1226 - val_mae: 2720.1226\n",
      "Epoch 1123/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2841.2708 - mae: 2841.2708 - val_loss: 2718.7917 - val_mae: 2718.7917\n",
      "Epoch 1124/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2840.5378 - mae: 2840.5378 - val_loss: 2718.8328 - val_mae: 2718.8328\n",
      "Epoch 1125/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2839.2876 - mae: 2839.2876 - val_loss: 2716.1672 - val_mae: 2716.1672\n",
      "Epoch 1126/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2837.6404 - mae: 2837.6404 - val_loss: 2716.0930 - val_mae: 2716.0930\n",
      "Epoch 1127/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2835.8547 - mae: 2835.8547 - val_loss: 2716.5637 - val_mae: 2716.5637\n",
      "Epoch 1128/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2836.3328 - mae: 2836.3328 - val_loss: 2717.2095 - val_mae: 2717.2095\n",
      "Epoch 1129/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2836.1729 - mae: 2836.1729 - val_loss: 2715.5330 - val_mae: 2715.5330\n",
      "Epoch 1130/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2834.4729 - mae: 2834.4729 - val_loss: 2712.0112 - val_mae: 2712.0112\n",
      "Epoch 1131/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2831.4036 - mae: 2831.4036 - val_loss: 2709.6245 - val_mae: 2709.6245\n",
      "Epoch 1132/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2830.2092 - mae: 2830.2092 - val_loss: 2710.0964 - val_mae: 2710.0964\n",
      "Epoch 1133/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2828.4722 - mae: 2828.4722 - val_loss: 2710.1807 - val_mae: 2710.1807\n",
      "Epoch 1134/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2827.5447 - mae: 2827.5447 - val_loss: 2711.7454 - val_mae: 2711.7454\n",
      "Epoch 1135/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2826.7285 - mae: 2826.7285 - val_loss: 2708.6462 - val_mae: 2708.6462\n",
      "Epoch 1136/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2824.2385 - mae: 2824.2385 - val_loss: 2706.8484 - val_mae: 2706.8484\n",
      "Epoch 1137/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2823.0425 - mae: 2823.0425 - val_loss: 2706.1284 - val_mae: 2706.1284\n",
      "Epoch 1138/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2822.6223 - mae: 2822.6223 - val_loss: 2710.8457 - val_mae: 2710.8457\n",
      "Epoch 1139/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2822.2356 - mae: 2822.2356 - val_loss: 2703.3560 - val_mae: 2703.3560\n",
      "Epoch 1140/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2818.6196 - mae: 2818.6196 - val_loss: 2705.1880 - val_mae: 2705.1880\n",
      "Epoch 1141/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2818.3079 - mae: 2818.3079 - val_loss: 2702.4846 - val_mae: 2702.4846\n",
      "Epoch 1142/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2816.7749 - mae: 2816.7749 - val_loss: 2699.9109 - val_mae: 2699.9109\n",
      "Epoch 1143/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2815.4763 - mae: 2815.4763 - val_loss: 2699.4844 - val_mae: 2699.4844\n",
      "Epoch 1144/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2813.8049 - mae: 2813.8049 - val_loss: 2699.7607 - val_mae: 2699.7607\n",
      "Epoch 1145/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2812.6450 - mae: 2812.6450 - val_loss: 2699.0745 - val_mae: 2699.0745\n",
      "Epoch 1146/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2810.4836 - mae: 2810.4836 - val_loss: 2699.0320 - val_mae: 2699.0320\n",
      "Epoch 1147/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2810.1487 - mae: 2810.1487 - val_loss: 2695.9812 - val_mae: 2695.9812\n",
      "Epoch 1148/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2807.7170 - mae: 2807.7170 - val_loss: 2697.2378 - val_mae: 2697.2378\n",
      "Epoch 1149/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2807.5122 - mae: 2807.5122 - val_loss: 2694.8518 - val_mae: 2694.8518\n",
      "Epoch 1150/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2805.1538 - mae: 2805.1538 - val_loss: 2694.9504 - val_mae: 2694.9504\n",
      "Epoch 1151/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2803.6877 - mae: 2803.6877 - val_loss: 2693.3000 - val_mae: 2693.3000\n",
      "Epoch 1152/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2802.5923 - mae: 2802.5923 - val_loss: 2692.0479 - val_mae: 2692.0479\n",
      "Epoch 1153/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2801.6929 - mae: 2801.6929 - val_loss: 2695.6799 - val_mae: 2695.6799\n",
      "Epoch 1154/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2799.1885 - mae: 2799.1885 - val_loss: 2689.3989 - val_mae: 2689.3989\n",
      "Epoch 1155/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2797.8535 - mae: 2797.8535 - val_loss: 2689.1458 - val_mae: 2689.1458\n",
      "Epoch 1156/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2797.9124 - mae: 2797.9124 - val_loss: 2688.4702 - val_mae: 2688.4702\n",
      "Epoch 1157/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2796.8523 - mae: 2796.8523 - val_loss: 2688.4036 - val_mae: 2688.4036\n",
      "Epoch 1158/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2794.0339 - mae: 2794.0339 - val_loss: 2687.2527 - val_mae: 2687.2527\n",
      "Epoch 1159/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2792.4829 - mae: 2792.4829 - val_loss: 2684.5413 - val_mae: 2684.5413\n",
      "Epoch 1160/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2791.2764 - mae: 2791.2764 - val_loss: 2685.3071 - val_mae: 2685.3071\n",
      "Epoch 1161/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2789.8901 - mae: 2789.8901 - val_loss: 2683.1104 - val_mae: 2683.1104\n",
      "Epoch 1162/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2788.0066 - mae: 2788.0066 - val_loss: 2684.1768 - val_mae: 2684.1768\n",
      "Epoch 1163/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2787.4773 - mae: 2787.4773 - val_loss: 2680.9956 - val_mae: 2680.9956\n",
      "Epoch 1164/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2786.1636 - mae: 2786.1636 - val_loss: 2680.1626 - val_mae: 2680.1626\n",
      "Epoch 1165/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2783.6362 - mae: 2783.6362 - val_loss: 2678.0881 - val_mae: 2678.0881\n",
      "Epoch 1166/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2782.2297 - mae: 2782.2297 - val_loss: 2678.4839 - val_mae: 2678.4839\n",
      "Epoch 1167/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2781.7852 - mae: 2781.7852 - val_loss: 2676.2073 - val_mae: 2676.2073\n",
      "Epoch 1168/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2779.7395 - mae: 2779.7395 - val_loss: 2676.1353 - val_mae: 2676.1353\n",
      "Epoch 1169/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2778.1743 - mae: 2778.1743 - val_loss: 2675.0332 - val_mae: 2675.0332\n",
      "Epoch 1170/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2776.9619 - mae: 2776.9619 - val_loss: 2675.5535 - val_mae: 2675.5535\n",
      "Epoch 1171/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2775.2864 - mae: 2775.2864 - val_loss: 2673.0989 - val_mae: 2673.0989\n",
      "Epoch 1172/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2773.3604 - mae: 2773.3604 - val_loss: 2670.9707 - val_mae: 2670.9707\n",
      "Epoch 1173/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2771.9883 - mae: 2771.9883 - val_loss: 2672.2893 - val_mae: 2672.2893\n",
      "Epoch 1174/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2770.2815 - mae: 2770.2815 - val_loss: 2669.9529 - val_mae: 2669.9529\n",
      "Epoch 1175/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2769.0134 - mae: 2769.0134 - val_loss: 2667.8037 - val_mae: 2667.8037\n",
      "Epoch 1176/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2767.7627 - mae: 2767.7627 - val_loss: 2669.0620 - val_mae: 2669.0620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1177/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2767.1887 - mae: 2767.1887 - val_loss: 2666.2168 - val_mae: 2666.2168\n",
      "Epoch 1178/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2767.8447 - mae: 2767.8447 - val_loss: 2664.3589 - val_mae: 2664.3589\n",
      "Epoch 1179/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2764.5044 - mae: 2764.5044 - val_loss: 2663.5466 - val_mae: 2663.5466\n",
      "Epoch 1180/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2761.9978 - mae: 2761.9978 - val_loss: 2665.1448 - val_mae: 2665.1448\n",
      "Epoch 1181/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2761.6865 - mae: 2761.6865 - val_loss: 2662.4175 - val_mae: 2662.4175\n",
      "Epoch 1182/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2759.5015 - mae: 2759.5015 - val_loss: 2662.3083 - val_mae: 2662.3083\n",
      "Epoch 1183/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2758.2400 - mae: 2758.2400 - val_loss: 2660.1001 - val_mae: 2660.1001\n",
      "Epoch 1184/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2757.5427 - mae: 2757.5427 - val_loss: 2658.9197 - val_mae: 2658.9197\n",
      "Epoch 1185/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2755.2261 - mae: 2755.2261 - val_loss: 2657.5593 - val_mae: 2657.5593\n",
      "Epoch 1186/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2753.4915 - mae: 2753.4915 - val_loss: 2658.1047 - val_mae: 2658.1047\n",
      "Epoch 1187/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2752.5583 - mae: 2752.5583 - val_loss: 2657.3521 - val_mae: 2657.3521\n",
      "Epoch 1188/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2750.6321 - mae: 2750.6321 - val_loss: 2655.9675 - val_mae: 2655.9675\n",
      "Epoch 1189/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2748.9045 - mae: 2748.9045 - val_loss: 2655.3809 - val_mae: 2655.3809\n",
      "Epoch 1190/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2748.0356 - mae: 2748.0356 - val_loss: 2652.1072 - val_mae: 2652.1072\n",
      "Epoch 1191/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2746.5046 - mae: 2746.5046 - val_loss: 2652.9890 - val_mae: 2652.9890\n",
      "Epoch 1192/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2744.0754 - mae: 2744.0754 - val_loss: 2650.0242 - val_mae: 2650.0242\n",
      "Epoch 1193/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2742.7004 - mae: 2742.7004 - val_loss: 2649.8066 - val_mae: 2649.8066\n",
      "Epoch 1194/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2741.3635 - mae: 2741.3635 - val_loss: 2649.9258 - val_mae: 2649.9258\n",
      "Epoch 1195/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2741.1531 - mae: 2741.1531 - val_loss: 2648.3240 - val_mae: 2648.3240\n",
      "Epoch 1196/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2740.1208 - mae: 2740.1208 - val_loss: 2655.3228 - val_mae: 2655.3228\n",
      "Epoch 1197/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2739.3779 - mae: 2739.3779 - val_loss: 2645.5847 - val_mae: 2645.5847\n",
      "Epoch 1198/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2735.3799 - mae: 2735.3799 - val_loss: 2645.4678 - val_mae: 2645.4678\n",
      "Epoch 1199/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2734.1782 - mae: 2734.1782 - val_loss: 2644.0522 - val_mae: 2644.0522\n",
      "Epoch 1200/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2732.5488 - mae: 2732.5488 - val_loss: 2642.5244 - val_mae: 2642.5244\n",
      "Epoch 1201/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2730.4670 - mae: 2730.4670 - val_loss: 2641.6934 - val_mae: 2641.6934\n",
      "Epoch 1202/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2728.8479 - mae: 2728.8479 - val_loss: 2640.6379 - val_mae: 2640.6379\n",
      "Epoch 1203/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2727.5344 - mae: 2727.5344 - val_loss: 2639.8484 - val_mae: 2639.8484\n",
      "Epoch 1204/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2726.3669 - mae: 2726.3669 - val_loss: 2639.4023 - val_mae: 2639.4023\n",
      "Epoch 1205/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2724.9202 - mae: 2724.9202 - val_loss: 2638.7939 - val_mae: 2638.7939\n",
      "Epoch 1206/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2723.1909 - mae: 2723.1909 - val_loss: 2635.0186 - val_mae: 2635.0186\n",
      "Epoch 1207/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2721.8660 - mae: 2721.8660 - val_loss: 2634.5627 - val_mae: 2634.5627\n",
      "Epoch 1208/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2720.1570 - mae: 2720.1570 - val_loss: 2638.0681 - val_mae: 2638.0681\n",
      "Epoch 1209/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2720.6211 - mae: 2720.6211 - val_loss: 2634.5671 - val_mae: 2634.5671\n",
      "Epoch 1210/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2718.5271 - mae: 2718.5271 - val_loss: 2630.7222 - val_mae: 2630.7222\n",
      "Epoch 1211/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2716.9026 - mae: 2716.9026 - val_loss: 2629.3176 - val_mae: 2629.3176\n",
      "Epoch 1212/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2714.8315 - mae: 2714.8315 - val_loss: 2627.0532 - val_mae: 2627.0532\n",
      "Epoch 1213/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2712.0144 - mae: 2712.0144 - val_loss: 2625.7766 - val_mae: 2625.7766\n",
      "Epoch 1214/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2710.9714 - mae: 2710.9714 - val_loss: 2627.3806 - val_mae: 2627.3806\n",
      "Epoch 1215/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2709.9338 - mae: 2709.9338 - val_loss: 2625.7583 - val_mae: 2625.7583\n",
      "Epoch 1216/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2707.1511 - mae: 2707.1511 - val_loss: 2624.1736 - val_mae: 2624.1736\n",
      "Epoch 1217/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2705.3667 - mae: 2705.3667 - val_loss: 2622.4712 - val_mae: 2622.4712\n",
      "Epoch 1218/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2703.7258 - mae: 2703.7258 - val_loss: 2621.3716 - val_mae: 2621.3716\n",
      "Epoch 1219/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2701.9573 - mae: 2701.9573 - val_loss: 2621.0981 - val_mae: 2621.0981\n",
      "Epoch 1220/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2701.1038 - mae: 2701.1038 - val_loss: 2618.8518 - val_mae: 2618.8518\n",
      "Epoch 1221/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2698.9661 - mae: 2698.9661 - val_loss: 2618.0679 - val_mae: 2618.0679\n",
      "Epoch 1222/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2697.1729 - mae: 2697.1729 - val_loss: 2616.2539 - val_mae: 2616.2539\n",
      "Epoch 1223/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2696.3457 - mae: 2696.3457 - val_loss: 2614.5342 - val_mae: 2614.5342\n",
      "Epoch 1224/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2695.0613 - mae: 2695.0613 - val_loss: 2612.8682 - val_mae: 2612.8682\n",
      "Epoch 1225/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2692.6235 - mae: 2692.6235 - val_loss: 2612.0386 - val_mae: 2612.0386\n",
      "Epoch 1226/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2690.7244 - mae: 2690.7244 - val_loss: 2611.5701 - val_mae: 2611.5701\n",
      "Epoch 1227/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2689.3638 - mae: 2689.3638 - val_loss: 2610.5098 - val_mae: 2610.5098\n",
      "Epoch 1228/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2689.6531 - mae: 2689.6531 - val_loss: 2608.7749 - val_mae: 2608.7749\n",
      "Epoch 1229/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2686.7190 - mae: 2686.7190 - val_loss: 2607.3079 - val_mae: 2607.3079\n",
      "Epoch 1230/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2684.4719 - mae: 2684.4719 - val_loss: 2605.4592 - val_mae: 2605.4592\n",
      "Epoch 1231/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2683.4087 - mae: 2683.4087 - val_loss: 2603.4539 - val_mae: 2603.4539\n",
      "Epoch 1232/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2681.9856 - mae: 2681.9856 - val_loss: 2603.9077 - val_mae: 2603.9077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1233/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2681.2200 - mae: 2681.2200 - val_loss: 2603.0076 - val_mae: 2603.0076\n",
      "Epoch 1234/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2679.3562 - mae: 2679.3562 - val_loss: 2600.1665 - val_mae: 2600.1665\n",
      "Epoch 1235/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2676.0947 - mae: 2676.0947 - val_loss: 2598.8003 - val_mae: 2598.8003\n",
      "Epoch 1236/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2674.9348 - mae: 2674.9348 - val_loss: 2598.2468 - val_mae: 2598.2468\n",
      "Epoch 1237/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2675.8391 - mae: 2675.8391 - val_loss: 2596.0776 - val_mae: 2596.0776\n",
      "Epoch 1238/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2671.6428 - mae: 2671.6428 - val_loss: 2594.2268 - val_mae: 2594.2268\n",
      "Epoch 1239/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2670.4133 - mae: 2670.4133 - val_loss: 2594.1682 - val_mae: 2594.1682\n",
      "Epoch 1240/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2668.9048 - mae: 2668.9048 - val_loss: 2593.6294 - val_mae: 2593.6294\n",
      "Epoch 1241/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2667.2507 - mae: 2667.2505 - val_loss: 2591.7402 - val_mae: 2591.7402\n",
      "Epoch 1242/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2666.8918 - mae: 2666.8918 - val_loss: 2590.7214 - val_mae: 2590.7214\n",
      "Epoch 1243/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2664.1396 - mae: 2664.1396 - val_loss: 2589.0042 - val_mae: 2589.0042\n",
      "Epoch 1244/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2662.2749 - mae: 2662.2749 - val_loss: 2587.4409 - val_mae: 2587.4409\n",
      "Epoch 1245/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2660.5295 - mae: 2660.5295 - val_loss: 2586.4878 - val_mae: 2586.4878\n",
      "Epoch 1246/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2659.0513 - mae: 2659.0513 - val_loss: 2584.7512 - val_mae: 2584.7512\n",
      "Epoch 1247/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2656.9517 - mae: 2656.9517 - val_loss: 2585.7405 - val_mae: 2585.7405\n",
      "Epoch 1248/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2656.5830 - mae: 2656.5830 - val_loss: 2582.6499 - val_mae: 2582.6499\n",
      "Epoch 1249/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2654.2412 - mae: 2654.2412 - val_loss: 2581.9619 - val_mae: 2581.9619\n",
      "Epoch 1250/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2653.5354 - mae: 2653.5354 - val_loss: 2579.3086 - val_mae: 2579.3086\n",
      "Epoch 1251/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2650.7344 - mae: 2650.7344 - val_loss: 2578.4678 - val_mae: 2578.4678\n",
      "Epoch 1252/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2649.1802 - mae: 2649.1802 - val_loss: 2578.1057 - val_mae: 2578.1057\n",
      "Epoch 1253/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2648.3389 - mae: 2648.3389 - val_loss: 2576.4451 - val_mae: 2576.4451\n",
      "Epoch 1254/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2647.9004 - mae: 2647.9004 - val_loss: 2573.8389 - val_mae: 2573.8389\n",
      "Epoch 1255/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2644.8894 - mae: 2644.8894 - val_loss: 2573.5002 - val_mae: 2573.5002\n",
      "Epoch 1256/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2643.2891 - mae: 2643.2891 - val_loss: 2571.6016 - val_mae: 2571.6016\n",
      "Epoch 1257/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2641.4502 - mae: 2641.4502 - val_loss: 2570.2632 - val_mae: 2570.2632\n",
      "Epoch 1258/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2638.6826 - mae: 2638.6826 - val_loss: 2568.7175 - val_mae: 2568.7175\n",
      "Epoch 1259/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2637.8958 - mae: 2637.8958 - val_loss: 2566.7559 - val_mae: 2566.7559\n",
      "Epoch 1260/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2636.6912 - mae: 2636.6912 - val_loss: 2565.1411 - val_mae: 2565.1411\n",
      "Epoch 1261/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2633.6968 - mae: 2633.6968 - val_loss: 2563.1602 - val_mae: 2563.1602\n",
      "Epoch 1262/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2634.0173 - mae: 2634.0173 - val_loss: 2562.2502 - val_mae: 2562.2502\n",
      "Epoch 1263/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2630.1448 - mae: 2630.1448 - val_loss: 2560.5894 - val_mae: 2560.5894\n",
      "Epoch 1264/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2628.6223 - mae: 2628.6223 - val_loss: 2560.4214 - val_mae: 2560.4214\n",
      "Epoch 1265/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2627.4556 - mae: 2627.4556 - val_loss: 2560.0859 - val_mae: 2560.0859\n",
      "Epoch 1266/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2625.2771 - mae: 2625.2771 - val_loss: 2556.7969 - val_mae: 2556.7969\n",
      "Epoch 1267/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2623.8147 - mae: 2623.8147 - val_loss: 2555.6697 - val_mae: 2555.6697\n",
      "Epoch 1268/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2621.9709 - mae: 2621.9709 - val_loss: 2554.2891 - val_mae: 2554.2891\n",
      "Epoch 1269/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2620.6165 - mae: 2620.6165 - val_loss: 2552.8718 - val_mae: 2552.8718\n",
      "Epoch 1270/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2619.8259 - mae: 2619.8259 - val_loss: 2551.3564 - val_mae: 2551.3564\n",
      "Epoch 1271/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2616.9087 - mae: 2616.9087 - val_loss: 2549.6094 - val_mae: 2549.6094\n",
      "Epoch 1272/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2615.0637 - mae: 2615.0637 - val_loss: 2548.3049 - val_mae: 2548.3049\n",
      "Epoch 1273/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2613.4631 - mae: 2613.4631 - val_loss: 2547.6555 - val_mae: 2547.6555\n",
      "Epoch 1274/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2611.7690 - mae: 2611.7690 - val_loss: 2545.6040 - val_mae: 2545.6040\n",
      "Epoch 1275/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2609.4409 - mae: 2609.4409 - val_loss: 2544.4741 - val_mae: 2544.4741\n",
      "Epoch 1276/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2607.8550 - mae: 2607.8550 - val_loss: 2542.9766 - val_mae: 2542.9766\n",
      "Epoch 1277/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2606.0557 - mae: 2606.0557 - val_loss: 2541.2505 - val_mae: 2541.2505\n",
      "Epoch 1278/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2604.4417 - mae: 2604.4417 - val_loss: 2541.3389 - val_mae: 2541.3389\n",
      "Epoch 1279/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2602.5747 - mae: 2602.5747 - val_loss: 2539.0903 - val_mae: 2539.0903\n",
      "Epoch 1280/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2601.5422 - mae: 2601.5422 - val_loss: 2539.9348 - val_mae: 2539.9348\n",
      "Epoch 1281/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2599.5037 - mae: 2599.5037 - val_loss: 2536.3643 - val_mae: 2536.3643\n",
      "Epoch 1282/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2598.4834 - mae: 2598.4834 - val_loss: 2535.7932 - val_mae: 2535.7932\n",
      "Epoch 1283/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2597.1694 - mae: 2597.1694 - val_loss: 2534.6934 - val_mae: 2534.6934\n",
      "Epoch 1284/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2595.7476 - mae: 2595.7476 - val_loss: 2531.9807 - val_mae: 2531.9807\n",
      "Epoch 1285/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2592.1030 - mae: 2592.1030 - val_loss: 2530.2219 - val_mae: 2530.2219\n",
      "Epoch 1286/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2591.5283 - mae: 2591.5283 - val_loss: 2529.0754 - val_mae: 2529.0754\n",
      "Epoch 1287/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2588.5854 - mae: 2588.5854 - val_loss: 2529.2354 - val_mae: 2529.2354\n",
      "Epoch 1288/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2587.4636 - mae: 2587.4636 - val_loss: 2526.6262 - val_mae: 2526.6262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1289/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2585.5818 - mae: 2585.5818 - val_loss: 2524.4470 - val_mae: 2524.4470\n",
      "Epoch 1290/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2584.5947 - mae: 2584.5947 - val_loss: 2524.3865 - val_mae: 2524.3865\n",
      "Epoch 1291/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2582.0884 - mae: 2582.0884 - val_loss: 2522.4055 - val_mae: 2522.4055\n",
      "Epoch 1292/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2581.4153 - mae: 2581.4153 - val_loss: 2521.1040 - val_mae: 2521.1040\n",
      "Epoch 1293/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2578.9126 - mae: 2578.9126 - val_loss: 2518.8784 - val_mae: 2518.8784\n",
      "Epoch 1294/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2578.6951 - mae: 2578.6951 - val_loss: 2519.3936 - val_mae: 2519.3936\n",
      "Epoch 1295/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2575.0071 - mae: 2575.0071 - val_loss: 2516.0181 - val_mae: 2516.0181\n",
      "Epoch 1296/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2573.8660 - mae: 2573.8660 - val_loss: 2515.8420 - val_mae: 2515.8420\n",
      "Epoch 1297/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2571.6741 - mae: 2571.6741 - val_loss: 2514.3330 - val_mae: 2514.3330\n",
      "Epoch 1298/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2568.8799 - mae: 2568.8799 - val_loss: 2512.4866 - val_mae: 2512.4866\n",
      "Epoch 1299/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2569.3840 - mae: 2569.3840 - val_loss: 2513.3652 - val_mae: 2513.3652\n",
      "Epoch 1300/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2567.8352 - mae: 2567.8352 - val_loss: 2509.9075 - val_mae: 2509.9075\n",
      "Epoch 1301/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2564.6465 - mae: 2564.6465 - val_loss: 2507.7786 - val_mae: 2507.7786\n",
      "Epoch 1302/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2562.0967 - mae: 2562.0967 - val_loss: 2506.8611 - val_mae: 2506.8611\n",
      "Epoch 1303/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2562.0051 - mae: 2562.0051 - val_loss: 2505.4414 - val_mae: 2505.4414\n",
      "Epoch 1304/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2560.5752 - mae: 2560.5752 - val_loss: 2505.0535 - val_mae: 2505.0535\n",
      "Epoch 1305/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2558.2859 - mae: 2558.2859 - val_loss: 2506.5593 - val_mae: 2506.5593\n",
      "Epoch 1306/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2556.3181 - mae: 2556.3181 - val_loss: 2501.3132 - val_mae: 2501.3132\n",
      "Epoch 1307/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2554.5305 - mae: 2554.5305 - val_loss: 2498.8535 - val_mae: 2498.8535\n",
      "Epoch 1308/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2553.2891 - mae: 2553.2891 - val_loss: 2499.0930 - val_mae: 2499.0930\n",
      "Epoch 1309/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2551.6064 - mae: 2551.6064 - val_loss: 2495.8413 - val_mae: 2495.8413\n",
      "Epoch 1310/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2548.4202 - mae: 2548.4202 - val_loss: 2495.0522 - val_mae: 2495.0522\n",
      "Epoch 1311/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2546.6477 - mae: 2546.6477 - val_loss: 2495.0701 - val_mae: 2495.0701\n",
      "Epoch 1312/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2544.9211 - mae: 2544.9211 - val_loss: 2492.4500 - val_mae: 2492.4500\n",
      "Epoch 1313/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2544.5574 - mae: 2544.5574 - val_loss: 2490.7283 - val_mae: 2490.7283\n",
      "Epoch 1314/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2541.5837 - mae: 2541.5837 - val_loss: 2488.5989 - val_mae: 2488.5989\n",
      "Epoch 1315/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2539.0256 - mae: 2539.0256 - val_loss: 2487.6064 - val_mae: 2487.6064\n",
      "Epoch 1316/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2539.2034 - mae: 2539.2034 - val_loss: 2486.1375 - val_mae: 2486.1375\n",
      "Epoch 1317/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2537.4749 - mae: 2537.4749 - val_loss: 2484.6406 - val_mae: 2484.6406\n",
      "Epoch 1318/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2535.1204 - mae: 2535.1204 - val_loss: 2482.7974 - val_mae: 2482.7974\n",
      "Epoch 1319/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2532.6216 - mae: 2532.6216 - val_loss: 2482.1658 - val_mae: 2482.1658\n",
      "Epoch 1320/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2531.5608 - mae: 2531.5608 - val_loss: 2480.4348 - val_mae: 2480.4348\n",
      "Epoch 1321/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2529.9277 - mae: 2529.9277 - val_loss: 2478.9536 - val_mae: 2478.9536\n",
      "Epoch 1322/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2528.7847 - mae: 2528.7847 - val_loss: 2477.9863 - val_mae: 2477.9863\n",
      "Epoch 1323/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2525.8438 - mae: 2525.8438 - val_loss: 2475.8240 - val_mae: 2475.8240\n",
      "Epoch 1324/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2523.7380 - mae: 2523.7380 - val_loss: 2475.3628 - val_mae: 2475.3628\n",
      "Epoch 1325/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2522.2957 - mae: 2522.2957 - val_loss: 2474.6484 - val_mae: 2474.6484\n",
      "Epoch 1326/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2520.3320 - mae: 2520.3320 - val_loss: 2471.7703 - val_mae: 2471.7703\n",
      "Epoch 1327/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2518.9663 - mae: 2518.9663 - val_loss: 2472.7578 - val_mae: 2472.7578\n",
      "Epoch 1328/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2518.8140 - mae: 2518.8140 - val_loss: 2469.4929 - val_mae: 2469.4929\n",
      "Epoch 1329/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2515.0479 - mae: 2515.0479 - val_loss: 2467.7432 - val_mae: 2467.7432\n",
      "Epoch 1330/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2514.0496 - mae: 2514.0496 - val_loss: 2466.0686 - val_mae: 2466.0686\n",
      "Epoch 1331/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2512.8650 - mae: 2512.8650 - val_loss: 2465.9758 - val_mae: 2465.9758\n",
      "Epoch 1332/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2513.2593 - mae: 2513.2593 - val_loss: 2463.7463 - val_mae: 2463.7463\n",
      "Epoch 1333/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2509.1682 - mae: 2509.1682 - val_loss: 2462.0549 - val_mae: 2462.0549\n",
      "Epoch 1334/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2505.9849 - mae: 2505.9849 - val_loss: 2460.7109 - val_mae: 2460.7109\n",
      "Epoch 1335/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2506.2197 - mae: 2506.2197 - val_loss: 2460.9443 - val_mae: 2460.9443\n",
      "Epoch 1336/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2501.9216 - mae: 2501.9216 - val_loss: 2457.4419 - val_mae: 2457.4419\n",
      "Epoch 1337/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2500.5085 - mae: 2500.5085 - val_loss: 2455.8511 - val_mae: 2455.8511\n",
      "Epoch 1338/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2500.1228 - mae: 2500.1228 - val_loss: 2454.4324 - val_mae: 2454.4324\n",
      "Epoch 1339/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2496.9238 - mae: 2496.9238 - val_loss: 2453.3350 - val_mae: 2453.3350\n",
      "Epoch 1340/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2494.9590 - mae: 2494.9590 - val_loss: 2450.5776 - val_mae: 2450.5776\n",
      "Epoch 1341/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2494.5806 - mae: 2494.5806 - val_loss: 2449.6177 - val_mae: 2449.6177\n",
      "Epoch 1342/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2492.4609 - mae: 2492.4609 - val_loss: 2448.3098 - val_mae: 2448.3098\n",
      "Epoch 1343/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2490.4785 - mae: 2490.4785 - val_loss: 2447.9353 - val_mae: 2447.9353\n",
      "Epoch 1344/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2489.7971 - mae: 2489.7971 - val_loss: 2445.0027 - val_mae: 2445.0027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1345/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2487.7273 - mae: 2487.7273 - val_loss: 2443.2371 - val_mae: 2443.2371\n",
      "Epoch 1346/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2484.6938 - mae: 2484.6938 - val_loss: 2441.6550 - val_mae: 2441.6550\n",
      "Epoch 1347/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2482.2385 - mae: 2482.2385 - val_loss: 2440.2361 - val_mae: 2440.2361\n",
      "Epoch 1348/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2481.7542 - mae: 2481.7542 - val_loss: 2437.9927 - val_mae: 2437.9927\n",
      "Epoch 1349/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2479.8774 - mae: 2479.8774 - val_loss: 2437.2852 - val_mae: 2437.2852\n",
      "Epoch 1350/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2477.3518 - mae: 2477.3518 - val_loss: 2435.4263 - val_mae: 2435.4263\n",
      "Epoch 1351/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2475.7058 - mae: 2475.7058 - val_loss: 2433.5059 - val_mae: 2433.5059\n",
      "Epoch 1352/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2473.3887 - mae: 2473.3887 - val_loss: 2434.3699 - val_mae: 2434.3699\n",
      "Epoch 1353/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2475.3521 - mae: 2475.3521 - val_loss: 2434.2725 - val_mae: 2434.2725\n",
      "Epoch 1354/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2472.9910 - mae: 2472.9910 - val_loss: 2429.5159 - val_mae: 2429.5159\n",
      "Epoch 1355/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2469.5901 - mae: 2469.5901 - val_loss: 2429.0803 - val_mae: 2429.0803\n",
      "Epoch 1356/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2467.5601 - mae: 2467.5601 - val_loss: 2425.9912 - val_mae: 2425.9912\n",
      "Epoch 1357/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2464.9194 - mae: 2464.9194 - val_loss: 2424.8564 - val_mae: 2424.8564\n",
      "Epoch 1358/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2464.1538 - mae: 2464.1538 - val_loss: 2423.4355 - val_mae: 2423.4355\n",
      "Epoch 1359/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2462.1089 - mae: 2462.1089 - val_loss: 2421.4324 - val_mae: 2421.4324\n",
      "Epoch 1360/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2459.7700 - mae: 2459.7700 - val_loss: 2420.0454 - val_mae: 2420.0454\n",
      "Epoch 1361/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2457.5967 - mae: 2457.5967 - val_loss: 2418.1958 - val_mae: 2418.1958\n",
      "Epoch 1362/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2455.9846 - mae: 2455.9846 - val_loss: 2416.5505 - val_mae: 2416.5505\n",
      "Epoch 1363/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2454.4094 - mae: 2454.4094 - val_loss: 2415.5249 - val_mae: 2415.5249\n",
      "Epoch 1364/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2453.0864 - mae: 2453.0864 - val_loss: 2414.4387 - val_mae: 2414.4387\n",
      "Epoch 1365/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2452.3738 - mae: 2452.3738 - val_loss: 2411.8069 - val_mae: 2411.8069\n",
      "Epoch 1366/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2449.7690 - mae: 2449.7690 - val_loss: 2410.9746 - val_mae: 2410.9746\n",
      "Epoch 1367/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2448.4507 - mae: 2448.4507 - val_loss: 2408.7903 - val_mae: 2408.7903\n",
      "Epoch 1368/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2445.7666 - mae: 2445.7666 - val_loss: 2407.0457 - val_mae: 2407.0457\n",
      "Epoch 1369/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2444.0608 - mae: 2444.0608 - val_loss: 2406.5320 - val_mae: 2406.5320\n",
      "Epoch 1370/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2443.1267 - mae: 2443.1267 - val_loss: 2404.0652 - val_mae: 2404.0652\n",
      "Epoch 1371/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2441.8486 - mae: 2441.8486 - val_loss: 2402.5964 - val_mae: 2402.5964\n",
      "Epoch 1372/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2439.1931 - mae: 2439.1931 - val_loss: 2400.9802 - val_mae: 2400.9802\n",
      "Epoch 1373/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2438.6328 - mae: 2438.6328 - val_loss: 2399.5210 - val_mae: 2399.5210\n",
      "Epoch 1374/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2435.0935 - mae: 2435.0935 - val_loss: 2397.4084 - val_mae: 2397.4084\n",
      "Epoch 1375/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2433.5264 - mae: 2433.5264 - val_loss: 2395.7397 - val_mae: 2395.7397\n",
      "Epoch 1376/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2431.2986 - mae: 2431.2986 - val_loss: 2393.9553 - val_mae: 2393.9553\n",
      "Epoch 1377/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2430.9016 - mae: 2430.9016 - val_loss: 2392.0439 - val_mae: 2392.0439\n",
      "Epoch 1378/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2428.0576 - mae: 2428.0576 - val_loss: 2390.5051 - val_mae: 2390.5051\n",
      "Epoch 1379/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2426.4260 - mae: 2426.4260 - val_loss: 2390.0046 - val_mae: 2390.0046\n",
      "Epoch 1380/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2425.8518 - mae: 2425.8518 - val_loss: 2387.3970 - val_mae: 2387.3970\n",
      "Epoch 1381/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2424.0811 - mae: 2424.0811 - val_loss: 2386.7791 - val_mae: 2386.7791\n",
      "Epoch 1382/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2421.5430 - mae: 2421.5430 - val_loss: 2384.6462 - val_mae: 2384.6462\n",
      "Epoch 1383/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2421.8203 - mae: 2421.8203 - val_loss: 2383.2825 - val_mae: 2383.2825\n",
      "Epoch 1384/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2418.6304 - mae: 2418.6304 - val_loss: 2382.6497 - val_mae: 2382.6497\n",
      "Epoch 1385/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2417.1375 - mae: 2417.1375 - val_loss: 2382.0190 - val_mae: 2382.0190\n",
      "Epoch 1386/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2415.8521 - mae: 2415.8521 - val_loss: 2377.9275 - val_mae: 2377.9275\n",
      "Epoch 1387/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2413.1895 - mae: 2413.1895 - val_loss: 2376.6633 - val_mae: 2376.6633\n",
      "Epoch 1388/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2411.3831 - mae: 2411.3831 - val_loss: 2375.1682 - val_mae: 2375.1682\n",
      "Epoch 1389/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2410.0420 - mae: 2410.0420 - val_loss: 2375.0706 - val_mae: 2375.0706\n",
      "Epoch 1390/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2408.8772 - mae: 2408.8772 - val_loss: 2371.3176 - val_mae: 2371.3176\n",
      "Epoch 1391/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2406.7910 - mae: 2406.7910 - val_loss: 2370.6714 - val_mae: 2370.6714\n",
      "Epoch 1392/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2404.5762 - mae: 2404.5762 - val_loss: 2368.2297 - val_mae: 2368.2297\n",
      "Epoch 1393/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2404.3435 - mae: 2404.3435 - val_loss: 2367.3193 - val_mae: 2367.3193\n",
      "Epoch 1394/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2401.4817 - mae: 2401.4817 - val_loss: 2365.0984 - val_mae: 2365.0984\n",
      "Epoch 1395/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2400.5063 - mae: 2400.5063 - val_loss: 2364.1670 - val_mae: 2364.1670\n",
      "Epoch 1396/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2398.7522 - mae: 2398.7522 - val_loss: 2362.3564 - val_mae: 2362.3564\n",
      "Epoch 1397/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2397.2002 - mae: 2397.2002 - val_loss: 2361.7925 - val_mae: 2361.7925\n",
      "Epoch 1398/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2396.3843 - mae: 2396.3843 - val_loss: 2359.1006 - val_mae: 2359.1006\n",
      "Epoch 1399/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2397.2258 - mae: 2397.2258 - val_loss: 2357.4663 - val_mae: 2357.4663\n",
      "Epoch 1400/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2392.6882 - mae: 2392.6882 - val_loss: 2355.9207 - val_mae: 2355.9207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1401/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2391.5981 - mae: 2391.5981 - val_loss: 2354.3833 - val_mae: 2354.3833\n",
      "Epoch 1402/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2389.6230 - mae: 2389.6230 - val_loss: 2353.2729 - val_mae: 2353.2729\n",
      "Epoch 1403/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2388.7317 - mae: 2388.7317 - val_loss: 2352.0945 - val_mae: 2352.0945\n",
      "Epoch 1404/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2388.4656 - mae: 2388.4656 - val_loss: 2349.7744 - val_mae: 2349.7744\n",
      "Epoch 1405/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2386.0139 - mae: 2386.0139 - val_loss: 2348.6033 - val_mae: 2348.6033\n",
      "Epoch 1406/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2385.8069 - mae: 2385.8069 - val_loss: 2346.9226 - val_mae: 2346.9226\n",
      "Epoch 1407/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2383.2166 - mae: 2383.2166 - val_loss: 2345.1807 - val_mae: 2345.1807\n",
      "Epoch 1408/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2382.7063 - mae: 2382.7063 - val_loss: 2344.0398 - val_mae: 2344.0398\n",
      "Epoch 1409/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2380.4802 - mae: 2380.4802 - val_loss: 2341.9790 - val_mae: 2341.9790\n",
      "Epoch 1410/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2378.7688 - mae: 2378.7688 - val_loss: 2340.3596 - val_mae: 2340.3596\n",
      "Epoch 1411/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2376.8044 - mae: 2376.8044 - val_loss: 2342.3777 - val_mae: 2342.3777\n",
      "Epoch 1412/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2377.6472 - mae: 2377.6472 - val_loss: 2337.2888 - val_mae: 2337.2888\n",
      "Epoch 1413/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2375.1714 - mae: 2375.1714 - val_loss: 2336.2380 - val_mae: 2336.2380\n",
      "Epoch 1414/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2373.4592 - mae: 2373.4592 - val_loss: 2334.9385 - val_mae: 2334.9385\n",
      "Epoch 1415/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2371.6946 - mae: 2371.6946 - val_loss: 2332.7827 - val_mae: 2332.7827\n",
      "Epoch 1416/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2370.0684 - mae: 2370.0684 - val_loss: 2332.7788 - val_mae: 2332.7788\n",
      "Epoch 1417/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2370.0713 - mae: 2370.0710 - val_loss: 2330.6975 - val_mae: 2330.6975\n",
      "Epoch 1418/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2367.9961 - mae: 2367.9961 - val_loss: 2330.8403 - val_mae: 2330.8403\n",
      "Epoch 1419/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2366.4463 - mae: 2366.4463 - val_loss: 2328.1138 - val_mae: 2328.1138\n",
      "Epoch 1420/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2365.2449 - mae: 2365.2449 - val_loss: 2325.2939 - val_mae: 2325.2939\n",
      "Epoch 1421/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2363.0220 - mae: 2363.0220 - val_loss: 2324.8975 - val_mae: 2324.8975\n",
      "Epoch 1422/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2361.7991 - mae: 2361.7991 - val_loss: 2323.2559 - val_mae: 2323.2559\n",
      "Epoch 1423/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2360.2219 - mae: 2360.2219 - val_loss: 2322.6997 - val_mae: 2322.6997\n",
      "Epoch 1424/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2359.0022 - mae: 2359.0022 - val_loss: 2320.3552 - val_mae: 2320.3552\n",
      "Epoch 1425/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2357.4944 - mae: 2357.4944 - val_loss: 2323.3521 - val_mae: 2323.3521\n",
      "Epoch 1426/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2356.8442 - mae: 2356.8442 - val_loss: 2317.6523 - val_mae: 2317.6523\n",
      "Epoch 1427/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2354.0984 - mae: 2354.0984 - val_loss: 2316.4324 - val_mae: 2316.4324\n",
      "Epoch 1428/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2352.2366 - mae: 2352.2366 - val_loss: 2314.6392 - val_mae: 2314.6392\n",
      "Epoch 1429/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2350.6660 - mae: 2350.6660 - val_loss: 2314.9822 - val_mae: 2314.9822\n",
      "Epoch 1430/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2349.3762 - mae: 2349.3762 - val_loss: 2311.1670 - val_mae: 2311.1670\n",
      "Epoch 1431/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2348.2578 - mae: 2348.2578 - val_loss: 2311.0378 - val_mae: 2311.0378\n",
      "Epoch 1432/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2347.0225 - mae: 2347.0225 - val_loss: 2309.3254 - val_mae: 2309.3254\n",
      "Epoch 1433/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2346.3589 - mae: 2346.3589 - val_loss: 2307.4348 - val_mae: 2307.4348\n",
      "Epoch 1434/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2344.6699 - mae: 2344.6699 - val_loss: 2306.7261 - val_mae: 2306.7261\n",
      "Epoch 1435/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2342.9155 - mae: 2342.9155 - val_loss: 2304.1772 - val_mae: 2304.1772\n",
      "Epoch 1436/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2340.9268 - mae: 2340.9268 - val_loss: 2304.0732 - val_mae: 2304.0732\n",
      "Epoch 1437/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2341.7039 - mae: 2341.7039 - val_loss: 2302.1584 - val_mae: 2302.1584\n",
      "Epoch 1438/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2341.0320 - mae: 2341.0320 - val_loss: 2300.3635 - val_mae: 2300.3635\n",
      "Epoch 1439/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2336.8770 - mae: 2336.8770 - val_loss: 2299.1943 - val_mae: 2299.1943\n",
      "Epoch 1440/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2335.6199 - mae: 2335.6199 - val_loss: 2297.6375 - val_mae: 2297.6375\n",
      "Epoch 1441/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2333.9685 - mae: 2333.9685 - val_loss: 2295.9858 - val_mae: 2295.9858\n",
      "Epoch 1442/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2332.6646 - mae: 2332.6646 - val_loss: 2295.8225 - val_mae: 2295.8225\n",
      "Epoch 1443/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2331.7451 - mae: 2331.7451 - val_loss: 2294.5264 - val_mae: 2294.5264\n",
      "Epoch 1444/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2329.6824 - mae: 2329.6824 - val_loss: 2293.4805 - val_mae: 2293.4805\n",
      "Epoch 1445/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2328.0930 - mae: 2328.0930 - val_loss: 2292.1387 - val_mae: 2292.1387\n",
      "Epoch 1446/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2326.4363 - mae: 2326.4363 - val_loss: 2290.9375 - val_mae: 2290.9375\n",
      "Epoch 1447/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2325.1050 - mae: 2325.1050 - val_loss: 2289.5635 - val_mae: 2289.5635\n",
      "Epoch 1448/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2324.5979 - mae: 2324.5979 - val_loss: 2288.1975 - val_mae: 2288.1975\n",
      "Epoch 1449/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2323.1951 - mae: 2323.1951 - val_loss: 2287.1548 - val_mae: 2287.1548\n",
      "Epoch 1450/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2321.6338 - mae: 2321.6338 - val_loss: 2285.6902 - val_mae: 2285.6902\n",
      "Epoch 1451/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2319.7512 - mae: 2319.7512 - val_loss: 2288.1042 - val_mae: 2288.1042\n",
      "Epoch 1452/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2320.6362 - mae: 2320.6362 - val_loss: 2284.5879 - val_mae: 2284.5879\n",
      "Epoch 1453/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2316.7166 - mae: 2316.7166 - val_loss: 2282.2136 - val_mae: 2282.2136\n",
      "Epoch 1454/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2315.8120 - mae: 2315.8120 - val_loss: 2280.1975 - val_mae: 2280.1975\n",
      "Epoch 1455/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2314.0168 - mae: 2314.0168 - val_loss: 2279.1665 - val_mae: 2279.1665\n",
      "Epoch 1456/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2313.3691 - mae: 2313.3691 - val_loss: 2278.1792 - val_mae: 2278.1792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1457/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2312.1257 - mae: 2312.1257 - val_loss: 2276.6855 - val_mae: 2276.6855\n",
      "Epoch 1458/2000\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 2310.3381 - mae: 2310.3381 - val_loss: 2276.2588 - val_mae: 2276.2588\n",
      "Epoch 1459/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2309.8337 - mae: 2309.8337 - val_loss: 2274.2231 - val_mae: 2274.2231\n",
      "Epoch 1460/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2309.5244 - mae: 2309.5244 - val_loss: 2273.1521 - val_mae: 2273.1521\n",
      "Epoch 1461/2000\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 2309.2300 - mae: 2309.2300 - val_loss: 2272.2961 - val_mae: 2272.2961\n",
      "Epoch 1462/2000\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 2306.6663 - mae: 2306.6663 - val_loss: 2270.6680 - val_mae: 2270.6680\n",
      "Epoch 1463/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2304.6438 - mae: 2304.6438 - val_loss: 2268.8645 - val_mae: 2268.8645\n",
      "Epoch 1464/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2304.4509 - mae: 2304.4509 - val_loss: 2267.4128 - val_mae: 2267.4128\n",
      "Epoch 1465/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2302.6670 - mae: 2302.6670 - val_loss: 2267.9126 - val_mae: 2267.9126\n",
      "Epoch 1466/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2301.2168 - mae: 2301.2168 - val_loss: 2265.6099 - val_mae: 2265.6099\n",
      "Epoch 1467/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2300.7566 - mae: 2300.7566 - val_loss: 2264.0684 - val_mae: 2264.0684\n",
      "Epoch 1468/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2300.3149 - mae: 2300.3149 - val_loss: 2263.4138 - val_mae: 2263.4138\n",
      "Epoch 1469/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2297.6958 - mae: 2297.6958 - val_loss: 2261.7720 - val_mae: 2261.7720\n",
      "Epoch 1470/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2296.8616 - mae: 2296.8616 - val_loss: 2260.7710 - val_mae: 2260.7710\n",
      "Epoch 1471/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2296.1274 - mae: 2296.1274 - val_loss: 2258.8215 - val_mae: 2258.8215\n",
      "Epoch 1472/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2294.9133 - mae: 2294.9133 - val_loss: 2257.4978 - val_mae: 2257.4978\n",
      "Epoch 1473/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2294.1458 - mae: 2294.1458 - val_loss: 2256.8503 - val_mae: 2256.8503\n",
      "Epoch 1474/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2292.5352 - mae: 2292.5352 - val_loss: 2256.5288 - val_mae: 2256.5288\n",
      "Epoch 1475/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2291.4185 - mae: 2291.4185 - val_loss: 2253.9888 - val_mae: 2253.9888\n",
      "Epoch 1476/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2290.5757 - mae: 2290.5757 - val_loss: 2254.1902 - val_mae: 2254.1902\n",
      "Epoch 1477/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2291.0222 - mae: 2291.0222 - val_loss: 2250.8674 - val_mae: 2250.8674\n",
      "Epoch 1478/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2288.1492 - mae: 2288.1492 - val_loss: 2250.4731 - val_mae: 2250.4731\n",
      "Epoch 1479/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2287.7883 - mae: 2287.7883 - val_loss: 2250.8748 - val_mae: 2250.8748\n",
      "Epoch 1480/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2286.5718 - mae: 2286.5718 - val_loss: 2247.6816 - val_mae: 2247.6816\n",
      "Epoch 1481/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2285.5532 - mae: 2285.5532 - val_loss: 2247.3171 - val_mae: 2247.3171\n",
      "Epoch 1482/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2284.5850 - mae: 2284.5850 - val_loss: 2246.4822 - val_mae: 2246.4822\n",
      "Epoch 1483/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2283.9761 - mae: 2283.9761 - val_loss: 2244.1089 - val_mae: 2244.1089\n",
      "Epoch 1484/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2280.9065 - mae: 2280.9065 - val_loss: 2242.2444 - val_mae: 2242.2444\n",
      "Epoch 1485/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2280.3604 - mae: 2280.3604 - val_loss: 2241.8701 - val_mae: 2241.8701\n",
      "Epoch 1486/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2278.9563 - mae: 2278.9563 - val_loss: 2240.7683 - val_mae: 2240.7683\n",
      "Epoch 1487/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2278.1526 - mae: 2278.1526 - val_loss: 2239.5652 - val_mae: 2239.5652\n",
      "Epoch 1488/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2277.4841 - mae: 2277.4841 - val_loss: 2239.3364 - val_mae: 2239.3364\n",
      "Epoch 1489/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2275.5745 - mae: 2275.5745 - val_loss: 2236.6724 - val_mae: 2236.6724\n",
      "Epoch 1490/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2274.0989 - mae: 2274.0989 - val_loss: 2236.5383 - val_mae: 2236.5383\n",
      "Epoch 1491/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2272.7932 - mae: 2272.7932 - val_loss: 2234.3098 - val_mae: 2234.3098\n",
      "Epoch 1492/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2271.9922 - mae: 2271.9922 - val_loss: 2233.2451 - val_mae: 2233.2451\n",
      "Epoch 1493/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2270.6023 - mae: 2270.6023 - val_loss: 2231.1982 - val_mae: 2231.1982\n",
      "Epoch 1494/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2270.2356 - mae: 2270.2356 - val_loss: 2230.5942 - val_mae: 2230.5942\n",
      "Epoch 1495/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2268.8408 - mae: 2268.8408 - val_loss: 2229.3035 - val_mae: 2229.3035\n",
      "Epoch 1496/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2267.8621 - mae: 2267.8621 - val_loss: 2228.3088 - val_mae: 2228.3088\n",
      "Epoch 1497/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2266.4038 - mae: 2266.4038 - val_loss: 2226.5068 - val_mae: 2226.5068\n",
      "Epoch 1498/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2265.5984 - mae: 2265.5984 - val_loss: 2225.4067 - val_mae: 2225.4067\n",
      "Epoch 1499/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2264.6895 - mae: 2264.6895 - val_loss: 2225.4956 - val_mae: 2225.4956\n",
      "Epoch 1500/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2265.1716 - mae: 2265.1716 - val_loss: 2222.7029 - val_mae: 2222.7029\n",
      "Epoch 1501/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2262.8823 - mae: 2262.8823 - val_loss: 2221.6978 - val_mae: 2221.6978\n",
      "Epoch 1502/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2262.5603 - mae: 2262.5603 - val_loss: 2220.2004 - val_mae: 2220.2004\n",
      "Epoch 1503/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2262.2466 - mae: 2262.2466 - val_loss: 2219.8533 - val_mae: 2219.8533\n",
      "Epoch 1504/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2260.4600 - mae: 2260.4600 - val_loss: 2219.1340 - val_mae: 2219.1340\n",
      "Epoch 1505/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2260.1189 - mae: 2260.1189 - val_loss: 2217.5369 - val_mae: 2217.5369\n",
      "Epoch 1506/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2257.9656 - mae: 2257.9656 - val_loss: 2218.8547 - val_mae: 2218.8547\n",
      "Epoch 1507/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2259.8171 - mae: 2259.8171 - val_loss: 2215.0994 - val_mae: 2215.0994\n",
      "Epoch 1508/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2256.1064 - mae: 2256.1064 - val_loss: 2213.7061 - val_mae: 2213.7061\n",
      "Epoch 1509/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2254.5369 - mae: 2254.5369 - val_loss: 2212.5281 - val_mae: 2212.5281\n",
      "Epoch 1510/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2254.6040 - mae: 2254.6040 - val_loss: 2212.9050 - val_mae: 2212.9050\n",
      "Epoch 1511/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2254.1082 - mae: 2254.1082 - val_loss: 2212.3979 - val_mae: 2212.3979\n",
      "Epoch 1512/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2252.6870 - mae: 2252.6870 - val_loss: 2209.7454 - val_mae: 2209.7454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1513/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2251.5276 - mae: 2251.5276 - val_loss: 2208.2380 - val_mae: 2208.2380\n",
      "Epoch 1514/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2250.7722 - mae: 2250.7722 - val_loss: 2205.8147 - val_mae: 2205.8147\n",
      "Epoch 1515/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2248.9136 - mae: 2248.9136 - val_loss: 2205.1619 - val_mae: 2205.1619\n",
      "Epoch 1516/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2248.8660 - mae: 2248.8660 - val_loss: 2205.0969 - val_mae: 2205.0969\n",
      "Epoch 1517/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2248.6040 - mae: 2248.6040 - val_loss: 2203.2087 - val_mae: 2203.2087\n",
      "Epoch 1518/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2249.5374 - mae: 2249.5374 - val_loss: 2206.7979 - val_mae: 2206.7979\n",
      "Epoch 1519/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2247.8660 - mae: 2247.8660 - val_loss: 2201.4663 - val_mae: 2201.4663\n",
      "Epoch 1520/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2245.5330 - mae: 2245.5330 - val_loss: 2200.4182 - val_mae: 2200.4182\n",
      "Epoch 1521/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2244.1279 - mae: 2244.1279 - val_loss: 2199.5764 - val_mae: 2199.5764\n",
      "Epoch 1522/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2243.3916 - mae: 2243.3916 - val_loss: 2200.1941 - val_mae: 2200.1941\n",
      "Epoch 1523/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2243.3220 - mae: 2243.3220 - val_loss: 2196.1489 - val_mae: 2196.1489\n",
      "Epoch 1524/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2241.4604 - mae: 2241.4604 - val_loss: 2194.7361 - val_mae: 2194.7361\n",
      "Epoch 1525/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2239.7078 - mae: 2239.7078 - val_loss: 2194.7041 - val_mae: 2194.7041\n",
      "Epoch 1526/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2238.7168 - mae: 2238.7168 - val_loss: 2192.2671 - val_mae: 2192.2671\n",
      "Epoch 1527/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2238.4692 - mae: 2238.4692 - val_loss: 2190.6914 - val_mae: 2190.6914\n",
      "Epoch 1528/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2237.3162 - mae: 2237.3162 - val_loss: 2190.2061 - val_mae: 2190.2061\n",
      "Epoch 1529/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2236.4199 - mae: 2236.4199 - val_loss: 2188.5652 - val_mae: 2188.5652\n",
      "Epoch 1530/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2235.2153 - mae: 2235.2153 - val_loss: 2189.2202 - val_mae: 2189.2202\n",
      "Epoch 1531/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2235.1899 - mae: 2235.1899 - val_loss: 2186.3918 - val_mae: 2186.3918\n",
      "Epoch 1532/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2235.4207 - mae: 2235.4207 - val_loss: 2185.7876 - val_mae: 2185.7876\n",
      "Epoch 1533/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2232.2769 - mae: 2232.2769 - val_loss: 2184.0852 - val_mae: 2184.0852\n",
      "Epoch 1534/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2231.0854 - mae: 2231.0854 - val_loss: 2187.3901 - val_mae: 2187.3901\n",
      "Epoch 1535/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2231.0015 - mae: 2231.0015 - val_loss: 2181.3254 - val_mae: 2181.3254\n",
      "Epoch 1536/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2229.5427 - mae: 2229.5427 - val_loss: 2180.0337 - val_mae: 2180.0337\n",
      "Epoch 1537/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2228.2583 - mae: 2228.2583 - val_loss: 2180.1541 - val_mae: 2180.1541\n",
      "Epoch 1538/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2227.2593 - mae: 2227.2593 - val_loss: 2179.3713 - val_mae: 2179.3713\n",
      "Epoch 1539/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2226.3645 - mae: 2226.3645 - val_loss: 2177.9185 - val_mae: 2177.9185\n",
      "Epoch 1540/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2226.8960 - mae: 2226.8960 - val_loss: 2176.0586 - val_mae: 2176.0586\n",
      "Epoch 1541/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2225.0100 - mae: 2225.0100 - val_loss: 2174.5762 - val_mae: 2174.5762\n",
      "Epoch 1542/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2223.1555 - mae: 2223.1555 - val_loss: 2174.0186 - val_mae: 2174.0186\n",
      "Epoch 1543/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2221.3809 - mae: 2221.3809 - val_loss: 2171.5142 - val_mae: 2171.5142\n",
      "Epoch 1544/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2222.7107 - mae: 2222.7107 - val_loss: 2171.5962 - val_mae: 2171.5962\n",
      "Epoch 1545/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2219.3186 - mae: 2219.3186 - val_loss: 2169.8989 - val_mae: 2169.8989\n",
      "Epoch 1546/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2218.5698 - mae: 2218.5698 - val_loss: 2168.2668 - val_mae: 2168.2668\n",
      "Epoch 1547/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2217.1885 - mae: 2217.1885 - val_loss: 2168.2583 - val_mae: 2168.2583\n",
      "Epoch 1548/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2217.9580 - mae: 2217.9580 - val_loss: 2168.7100 - val_mae: 2168.7100\n",
      "Epoch 1549/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2218.7983 - mae: 2218.7983 - val_loss: 2167.3162 - val_mae: 2167.3162\n",
      "Epoch 1550/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2214.5251 - mae: 2214.5251 - val_loss: 2164.3313 - val_mae: 2164.3313\n",
      "Epoch 1551/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2214.1167 - mae: 2214.1167 - val_loss: 2162.8137 - val_mae: 2162.8137\n",
      "Epoch 1552/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2213.4963 - mae: 2213.4963 - val_loss: 2161.5156 - val_mae: 2161.5156\n",
      "Epoch 1553/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2213.4910 - mae: 2213.4910 - val_loss: 2160.2944 - val_mae: 2160.2944\n",
      "Epoch 1554/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2211.0220 - mae: 2211.0220 - val_loss: 2158.9175 - val_mae: 2158.9175\n",
      "Epoch 1555/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2209.9766 - mae: 2209.9766 - val_loss: 2158.7285 - val_mae: 2158.7285\n",
      "Epoch 1556/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2209.1504 - mae: 2209.1504 - val_loss: 2157.1584 - val_mae: 2157.1584\n",
      "Epoch 1557/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2208.6628 - mae: 2208.6628 - val_loss: 2155.9048 - val_mae: 2155.9048\n",
      "Epoch 1558/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2206.2747 - mae: 2206.2747 - val_loss: 2154.4258 - val_mae: 2154.4258\n",
      "Epoch 1559/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2205.4380 - mae: 2205.4380 - val_loss: 2153.6230 - val_mae: 2153.6230\n",
      "Epoch 1560/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2205.7505 - mae: 2205.7505 - val_loss: 2153.8220 - val_mae: 2153.8220\n",
      "Epoch 1561/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2204.6721 - mae: 2204.6721 - val_loss: 2151.5859 - val_mae: 2151.5859\n",
      "Epoch 1562/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2203.4368 - mae: 2203.4368 - val_loss: 2150.5435 - val_mae: 2150.5435\n",
      "Epoch 1563/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2203.1013 - mae: 2203.1013 - val_loss: 2148.8423 - val_mae: 2148.8423\n",
      "Epoch 1564/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2200.9922 - mae: 2200.9922 - val_loss: 2147.9434 - val_mae: 2147.9434\n",
      "Epoch 1565/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2199.3047 - mae: 2199.3047 - val_loss: 2146.1091 - val_mae: 2146.1091\n",
      "Epoch 1566/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2198.8831 - mae: 2198.8831 - val_loss: 2146.3767 - val_mae: 2146.3767\n",
      "Epoch 1567/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2197.7063 - mae: 2197.7063 - val_loss: 2144.2061 - val_mae: 2144.2061\n",
      "Epoch 1568/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2196.5239 - mae: 2196.5239 - val_loss: 2144.2212 - val_mae: 2144.2212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1569/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2195.3025 - mae: 2195.3025 - val_loss: 2143.0896 - val_mae: 2143.0896\n",
      "Epoch 1570/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2194.4468 - mae: 2194.4468 - val_loss: 2140.9514 - val_mae: 2140.9514\n",
      "Epoch 1571/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2193.7822 - mae: 2193.7822 - val_loss: 2142.0139 - val_mae: 2142.0139\n",
      "Epoch 1572/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2193.4258 - mae: 2193.4258 - val_loss: 2141.7368 - val_mae: 2141.7368\n",
      "Epoch 1573/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2192.7815 - mae: 2192.7815 - val_loss: 2138.9995 - val_mae: 2138.9995\n",
      "Epoch 1574/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2190.5081 - mae: 2190.5081 - val_loss: 2136.4939 - val_mae: 2136.4939\n",
      "Epoch 1575/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2190.3005 - mae: 2190.3005 - val_loss: 2135.3655 - val_mae: 2135.3655\n",
      "Epoch 1576/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2187.7456 - mae: 2187.7456 - val_loss: 2134.5479 - val_mae: 2134.5479\n",
      "Epoch 1577/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2190.3611 - mae: 2190.3611 - val_loss: 2134.5913 - val_mae: 2134.5913\n",
      "Epoch 1578/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2186.9634 - mae: 2186.9634 - val_loss: 2131.8406 - val_mae: 2131.8406\n",
      "Epoch 1579/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2185.7046 - mae: 2185.7046 - val_loss: 2130.6777 - val_mae: 2130.6777\n",
      "Epoch 1580/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2183.9612 - mae: 2183.9612 - val_loss: 2129.8430 - val_mae: 2129.8430\n",
      "Epoch 1581/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2183.5352 - mae: 2183.5352 - val_loss: 2128.1519 - val_mae: 2128.1519\n",
      "Epoch 1582/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2182.7498 - mae: 2182.7498 - val_loss: 2127.4736 - val_mae: 2127.4736\n",
      "Epoch 1583/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2181.4504 - mae: 2181.4504 - val_loss: 2126.3955 - val_mae: 2126.3955\n",
      "Epoch 1584/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2179.7266 - mae: 2179.7266 - val_loss: 2124.5186 - val_mae: 2124.5186\n",
      "Epoch 1585/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2179.3513 - mae: 2179.3513 - val_loss: 2122.6394 - val_mae: 2122.6394\n",
      "Epoch 1586/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2177.7043 - mae: 2177.7043 - val_loss: 2122.2449 - val_mae: 2122.2449\n",
      "Epoch 1587/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2177.0979 - mae: 2177.0979 - val_loss: 2120.5837 - val_mae: 2120.5837\n",
      "Epoch 1588/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2177.2180 - mae: 2177.2180 - val_loss: 2120.8154 - val_mae: 2120.8154\n",
      "Epoch 1589/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2175.1470 - mae: 2175.1470 - val_loss: 2118.8010 - val_mae: 2118.8010\n",
      "Epoch 1590/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2174.7913 - mae: 2174.7913 - val_loss: 2116.4165 - val_mae: 2116.4165\n",
      "Epoch 1591/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2173.9065 - mae: 2173.9065 - val_loss: 2115.3828 - val_mae: 2115.3828\n",
      "Epoch 1592/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2172.2664 - mae: 2172.2664 - val_loss: 2116.0454 - val_mae: 2116.0454\n",
      "Epoch 1593/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2171.8943 - mae: 2171.8943 - val_loss: 2114.5403 - val_mae: 2114.5403\n",
      "Epoch 1594/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2170.7795 - mae: 2170.7795 - val_loss: 2114.8804 - val_mae: 2114.8804\n",
      "Epoch 1595/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2170.4150 - mae: 2170.4150 - val_loss: 2111.7019 - val_mae: 2111.7019\n",
      "Epoch 1596/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2169.0593 - mae: 2169.0593 - val_loss: 2110.8318 - val_mae: 2110.8318\n",
      "Epoch 1597/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2167.4297 - mae: 2167.4297 - val_loss: 2111.1541 - val_mae: 2111.1541\n",
      "Epoch 1598/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2168.3320 - mae: 2168.3320 - val_loss: 2108.3103 - val_mae: 2108.3103\n",
      "Epoch 1599/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2166.8169 - mae: 2166.8169 - val_loss: 2111.1362 - val_mae: 2111.1362\n",
      "Epoch 1600/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2166.1372 - mae: 2166.1372 - val_loss: 2106.4963 - val_mae: 2106.4963\n",
      "Epoch 1601/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2164.9680 - mae: 2164.9680 - val_loss: 2105.3911 - val_mae: 2105.3911\n",
      "Epoch 1602/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2163.0242 - mae: 2163.0242 - val_loss: 2104.7227 - val_mae: 2104.7227\n",
      "Epoch 1603/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2162.6567 - mae: 2162.6567 - val_loss: 2104.2324 - val_mae: 2104.2324\n",
      "Epoch 1604/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2161.4202 - mae: 2161.4202 - val_loss: 2102.9624 - val_mae: 2102.9624\n",
      "Epoch 1605/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2161.1865 - mae: 2161.1865 - val_loss: 2101.3140 - val_mae: 2101.3140\n",
      "Epoch 1606/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2159.7275 - mae: 2159.7275 - val_loss: 2100.2231 - val_mae: 2100.2231\n",
      "Epoch 1607/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2159.3604 - mae: 2159.3604 - val_loss: 2097.6587 - val_mae: 2097.6587\n",
      "Epoch 1608/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2158.4077 - mae: 2158.4077 - val_loss: 2097.6792 - val_mae: 2097.6792\n",
      "Epoch 1609/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2156.8262 - mae: 2156.8262 - val_loss: 2097.5696 - val_mae: 2097.5696\n",
      "Epoch 1610/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2155.8828 - mae: 2155.8828 - val_loss: 2095.4707 - val_mae: 2095.4707\n",
      "Epoch 1611/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2154.5430 - mae: 2154.5430 - val_loss: 2095.1047 - val_mae: 2095.1047\n",
      "Epoch 1612/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2154.2012 - mae: 2154.2012 - val_loss: 2092.8271 - val_mae: 2092.8271\n",
      "Epoch 1613/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2152.5063 - mae: 2152.5063 - val_loss: 2092.4995 - val_mae: 2092.4995\n",
      "Epoch 1614/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2152.0642 - mae: 2152.0642 - val_loss: 2090.9485 - val_mae: 2090.9485\n",
      "Epoch 1615/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2152.0933 - mae: 2152.0933 - val_loss: 2089.6428 - val_mae: 2089.6428\n",
      "Epoch 1616/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2150.9500 - mae: 2150.9500 - val_loss: 2089.8899 - val_mae: 2089.8899\n",
      "Epoch 1617/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2150.2581 - mae: 2150.2581 - val_loss: 2090.2471 - val_mae: 2090.2471\n",
      "Epoch 1618/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2150.1724 - mae: 2150.1724 - val_loss: 2087.6394 - val_mae: 2087.6394\n",
      "Epoch 1619/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2147.6526 - mae: 2147.6526 - val_loss: 2086.5740 - val_mae: 2086.5740\n",
      "Epoch 1620/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2146.4517 - mae: 2146.4517 - val_loss: 2086.5234 - val_mae: 2086.5234\n",
      "Epoch 1621/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2146.4741 - mae: 2146.4741 - val_loss: 2084.7363 - val_mae: 2084.7363\n",
      "Epoch 1622/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2145.0144 - mae: 2145.0144 - val_loss: 2082.3777 - val_mae: 2082.3777\n",
      "Epoch 1623/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2144.6570 - mae: 2144.6570 - val_loss: 2082.5168 - val_mae: 2082.5168\n",
      "Epoch 1624/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2144.8247 - mae: 2144.8247 - val_loss: 2082.0403 - val_mae: 2082.0403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1625/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2142.6494 - mae: 2142.6494 - val_loss: 2080.3159 - val_mae: 2080.3159\n",
      "Epoch 1626/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2141.1226 - mae: 2141.1226 - val_loss: 2078.9866 - val_mae: 2078.9866\n",
      "Epoch 1627/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2140.9529 - mae: 2140.9529 - val_loss: 2078.1160 - val_mae: 2078.1160\n",
      "Epoch 1628/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2139.9370 - mae: 2139.9370 - val_loss: 2076.8896 - val_mae: 2076.8896\n",
      "Epoch 1629/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2139.1013 - mae: 2139.1013 - val_loss: 2075.4634 - val_mae: 2075.4634\n",
      "Epoch 1630/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2137.5146 - mae: 2137.5146 - val_loss: 2074.7051 - val_mae: 2074.7051\n",
      "Epoch 1631/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2137.1038 - mae: 2137.1038 - val_loss: 2072.8889 - val_mae: 2072.8889\n",
      "Epoch 1632/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2136.2920 - mae: 2136.2920 - val_loss: 2072.3081 - val_mae: 2072.3081\n",
      "Epoch 1633/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2135.2622 - mae: 2135.2622 - val_loss: 2072.6584 - val_mae: 2072.6584\n",
      "Epoch 1634/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2135.8796 - mae: 2135.8796 - val_loss: 2069.8289 - val_mae: 2069.8289\n",
      "Epoch 1635/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2133.4534 - mae: 2133.4534 - val_loss: 2069.4934 - val_mae: 2069.4934\n",
      "Epoch 1636/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2132.9866 - mae: 2132.9866 - val_loss: 2068.2266 - val_mae: 2068.2266\n",
      "Epoch 1637/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2132.3997 - mae: 2132.3997 - val_loss: 2066.7278 - val_mae: 2066.7278\n",
      "Epoch 1638/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2130.5994 - mae: 2130.5994 - val_loss: 2066.2334 - val_mae: 2066.2334\n",
      "Epoch 1639/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2130.1392 - mae: 2130.1392 - val_loss: 2064.6814 - val_mae: 2064.6814\n",
      "Epoch 1640/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2128.1396 - mae: 2128.1396 - val_loss: 2063.5906 - val_mae: 2063.5906\n",
      "Epoch 1641/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2128.3494 - mae: 2128.3494 - val_loss: 2062.8818 - val_mae: 2062.8818\n",
      "Epoch 1642/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2126.7158 - mae: 2126.7158 - val_loss: 2062.4858 - val_mae: 2062.4858\n",
      "Epoch 1643/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2126.3713 - mae: 2126.3713 - val_loss: 2059.8223 - val_mae: 2059.8223\n",
      "Epoch 1644/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2125.0552 - mae: 2125.0552 - val_loss: 2059.6125 - val_mae: 2059.6125\n",
      "Epoch 1645/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2125.0752 - mae: 2125.0752 - val_loss: 2058.3381 - val_mae: 2058.3381\n",
      "Epoch 1646/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2126.0991 - mae: 2126.0991 - val_loss: 2059.0171 - val_mae: 2059.0171\n",
      "Epoch 1647/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2122.4363 - mae: 2122.4363 - val_loss: 2055.5391 - val_mae: 2055.5391\n",
      "Epoch 1648/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2122.0520 - mae: 2122.0520 - val_loss: 2054.7234 - val_mae: 2054.7234\n",
      "Epoch 1649/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2120.8582 - mae: 2120.8582 - val_loss: 2053.0779 - val_mae: 2053.0779\n",
      "Epoch 1650/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2120.9731 - mae: 2120.9731 - val_loss: 2053.4558 - val_mae: 2053.4558\n",
      "Epoch 1651/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2119.3333 - mae: 2119.3333 - val_loss: 2052.1179 - val_mae: 2052.1179\n",
      "Epoch 1652/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2118.5315 - mae: 2118.5315 - val_loss: 2049.5549 - val_mae: 2049.5549\n",
      "Epoch 1653/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2117.8335 - mae: 2117.8335 - val_loss: 2049.9360 - val_mae: 2049.9360\n",
      "Epoch 1654/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2116.7886 - mae: 2116.7886 - val_loss: 2048.1392 - val_mae: 2048.1392\n",
      "Epoch 1655/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2117.6389 - mae: 2117.6389 - val_loss: 2048.3423 - val_mae: 2048.3423\n",
      "Epoch 1656/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2115.6594 - mae: 2115.6594 - val_loss: 2044.5216 - val_mae: 2044.5216\n",
      "Epoch 1657/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2113.5190 - mae: 2113.5190 - val_loss: 2044.6101 - val_mae: 2044.6101\n",
      "Epoch 1658/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2114.7065 - mae: 2114.7065 - val_loss: 2042.4791 - val_mae: 2042.4791\n",
      "Epoch 1659/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2112.3950 - mae: 2112.3950 - val_loss: 2040.6256 - val_mae: 2040.6256\n",
      "Epoch 1660/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2111.6350 - mae: 2111.6350 - val_loss: 2039.7606 - val_mae: 2039.7606\n",
      "Epoch 1661/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2110.3826 - mae: 2110.3826 - val_loss: 2039.5789 - val_mae: 2039.5789\n",
      "Epoch 1662/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2109.4614 - mae: 2109.4614 - val_loss: 2038.1138 - val_mae: 2038.1138\n",
      "Epoch 1663/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2108.5999 - mae: 2108.5999 - val_loss: 2036.9943 - val_mae: 2036.9943\n",
      "Epoch 1664/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2107.7249 - mae: 2107.7249 - val_loss: 2038.5022 - val_mae: 2038.5022\n",
      "Epoch 1665/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2107.1958 - mae: 2107.1958 - val_loss: 2034.1268 - val_mae: 2034.1268\n",
      "Epoch 1666/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2107.0161 - mae: 2107.0161 - val_loss: 2035.7185 - val_mae: 2035.7185\n",
      "Epoch 1667/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2106.1868 - mae: 2106.1868 - val_loss: 2032.7800 - val_mae: 2032.7800\n",
      "Epoch 1668/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2104.8376 - mae: 2104.8376 - val_loss: 2031.6112 - val_mae: 2031.6112\n",
      "Epoch 1669/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2103.7961 - mae: 2103.7961 - val_loss: 2030.1129 - val_mae: 2030.1129\n",
      "Epoch 1670/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2102.7341 - mae: 2102.7341 - val_loss: 2028.3591 - val_mae: 2028.3591\n",
      "Epoch 1671/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2104.1033 - mae: 2104.1033 - val_loss: 2028.6252 - val_mae: 2028.6252\n",
      "Epoch 1672/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2102.1565 - mae: 2102.1565 - val_loss: 2026.1315 - val_mae: 2026.1315\n",
      "Epoch 1673/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2101.3850 - mae: 2101.3850 - val_loss: 2025.7394 - val_mae: 2025.7394\n",
      "Epoch 1674/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2099.0447 - mae: 2099.0447 - val_loss: 2023.7175 - val_mae: 2023.7175\n",
      "Epoch 1675/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2099.4385 - mae: 2099.4385 - val_loss: 2024.8203 - val_mae: 2024.8203\n",
      "Epoch 1676/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2097.0752 - mae: 2097.0752 - val_loss: 2021.2157 - val_mae: 2021.2157\n",
      "Epoch 1677/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2097.4468 - mae: 2097.4468 - val_loss: 2021.6310 - val_mae: 2021.6310\n",
      "Epoch 1678/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2096.3701 - mae: 2096.3701 - val_loss: 2018.4520 - val_mae: 2018.4520\n",
      "Epoch 1679/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2095.1917 - mae: 2095.1917 - val_loss: 2019.4520 - val_mae: 2019.4520\n",
      "Epoch 1680/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2094.9275 - mae: 2094.9275 - val_loss: 2016.9241 - val_mae: 2016.9241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1681/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2094.0586 - mae: 2094.0586 - val_loss: 2015.5868 - val_mae: 2015.5868\n",
      "Epoch 1682/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2093.0063 - mae: 2093.0063 - val_loss: 2014.3507 - val_mae: 2014.3507\n",
      "Epoch 1683/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2091.9207 - mae: 2091.9207 - val_loss: 2012.8906 - val_mae: 2012.8906\n",
      "Epoch 1684/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2090.8804 - mae: 2090.8804 - val_loss: 2011.7214 - val_mae: 2011.7214\n",
      "Epoch 1685/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2089.4734 - mae: 2089.4734 - val_loss: 2010.1592 - val_mae: 2010.1592\n",
      "Epoch 1686/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2089.1538 - mae: 2089.1538 - val_loss: 2011.0675 - val_mae: 2011.0675\n",
      "Epoch 1687/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2089.4492 - mae: 2089.4492 - val_loss: 2008.1023 - val_mae: 2008.1023\n",
      "Epoch 1688/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2087.3157 - mae: 2087.3157 - val_loss: 2007.5151 - val_mae: 2007.5151\n",
      "Epoch 1689/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2088.4634 - mae: 2088.4634 - val_loss: 2007.2217 - val_mae: 2007.2217\n",
      "Epoch 1690/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2086.4038 - mae: 2086.4038 - val_loss: 2003.8959 - val_mae: 2003.8959\n",
      "Epoch 1691/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2086.3162 - mae: 2086.3162 - val_loss: 2003.0125 - val_mae: 2003.0125\n",
      "Epoch 1692/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2085.2778 - mae: 2085.2778 - val_loss: 2004.5321 - val_mae: 2004.5321\n",
      "Epoch 1693/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2084.2207 - mae: 2084.2207 - val_loss: 2000.8347 - val_mae: 2000.8347\n",
      "Epoch 1694/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2082.8892 - mae: 2082.8892 - val_loss: 2000.4410 - val_mae: 2000.4410\n",
      "Epoch 1695/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2082.6772 - mae: 2082.6772 - val_loss: 1997.9191 - val_mae: 1997.9191\n",
      "Epoch 1696/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2082.4768 - mae: 2082.4768 - val_loss: 1998.4531 - val_mae: 1998.4531\n",
      "Epoch 1697/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2080.6536 - mae: 2080.6536 - val_loss: 1996.1373 - val_mae: 1996.1373\n",
      "Epoch 1698/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2080.4509 - mae: 2080.4509 - val_loss: 1994.6567 - val_mae: 1994.6567\n",
      "Epoch 1699/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2079.6846 - mae: 2079.6846 - val_loss: 1993.7362 - val_mae: 1993.7362\n",
      "Epoch 1700/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2077.8606 - mae: 2077.8606 - val_loss: 1991.8071 - val_mae: 1991.8071\n",
      "Epoch 1701/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2076.9824 - mae: 2076.9824 - val_loss: 1994.4795 - val_mae: 1994.4795\n",
      "Epoch 1702/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2077.2976 - mae: 2077.2976 - val_loss: 1993.5819 - val_mae: 1993.5819\n",
      "Epoch 1703/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2077.2109 - mae: 2077.2109 - val_loss: 1989.6680 - val_mae: 1989.6680\n",
      "Epoch 1704/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2075.5652 - mae: 2075.5652 - val_loss: 1988.4697 - val_mae: 1988.4697\n",
      "Epoch 1705/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2075.5103 - mae: 2075.5103 - val_loss: 1987.3143 - val_mae: 1987.3143\n",
      "Epoch 1706/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2074.5325 - mae: 2074.5325 - val_loss: 1985.6804 - val_mae: 1985.6804\n",
      "Epoch 1707/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2073.2915 - mae: 2073.2915 - val_loss: 1985.4446 - val_mae: 1985.4446\n",
      "Epoch 1708/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2072.8760 - mae: 2072.8760 - val_loss: 1984.1394 - val_mae: 1984.1394\n",
      "Epoch 1709/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2073.3962 - mae: 2073.3962 - val_loss: 1983.7356 - val_mae: 1983.7356\n",
      "Epoch 1710/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2074.3123 - mae: 2074.3123 - val_loss: 1982.0663 - val_mae: 1982.0663\n",
      "Epoch 1711/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2073.7986 - mae: 2073.7986 - val_loss: 1984.1829 - val_mae: 1984.1829\n",
      "Epoch 1712/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2072.9312 - mae: 2072.9312 - val_loss: 1979.9939 - val_mae: 1979.9939\n",
      "Epoch 1713/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2070.3884 - mae: 2070.3884 - val_loss: 1979.7383 - val_mae: 1979.7383\n",
      "Epoch 1714/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2069.8999 - mae: 2069.8999 - val_loss: 1980.5851 - val_mae: 1980.5851\n",
      "Epoch 1715/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2069.4802 - mae: 2069.4802 - val_loss: 1977.6002 - val_mae: 1977.6002\n",
      "Epoch 1716/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2068.2556 - mae: 2068.2556 - val_loss: 1976.7701 - val_mae: 1976.7701\n",
      "Epoch 1717/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2067.6096 - mae: 2067.6096 - val_loss: 1976.0883 - val_mae: 1976.0883\n",
      "Epoch 1718/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2066.8726 - mae: 2066.8726 - val_loss: 1975.4658 - val_mae: 1975.4658\n",
      "Epoch 1719/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2066.2971 - mae: 2066.2971 - val_loss: 1973.5774 - val_mae: 1973.5774\n",
      "Epoch 1720/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2065.5496 - mae: 2065.5496 - val_loss: 1973.4496 - val_mae: 1973.4496\n",
      "Epoch 1721/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2064.4988 - mae: 2064.4988 - val_loss: 1971.7617 - val_mae: 1971.7617\n",
      "Epoch 1722/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2063.7791 - mae: 2063.7791 - val_loss: 1970.4512 - val_mae: 1970.4512\n",
      "Epoch 1723/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2063.3472 - mae: 2063.3472 - val_loss: 1969.4962 - val_mae: 1969.4962\n",
      "Epoch 1724/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2063.4246 - mae: 2063.4246 - val_loss: 1968.6401 - val_mae: 1968.6401\n",
      "Epoch 1725/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2062.8684 - mae: 2062.8684 - val_loss: 1968.3226 - val_mae: 1968.3226\n",
      "Epoch 1726/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2062.4314 - mae: 2062.4314 - val_loss: 1966.8733 - val_mae: 1966.8733\n",
      "Epoch 1727/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2062.6045 - mae: 2062.6045 - val_loss: 1969.4436 - val_mae: 1969.4436\n",
      "Epoch 1728/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2061.8391 - mae: 2061.8391 - val_loss: 1964.5698 - val_mae: 1964.5698\n",
      "Epoch 1729/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2061.2368 - mae: 2061.2368 - val_loss: 1966.8478 - val_mae: 1966.8478\n",
      "Epoch 1730/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2060.5530 - mae: 2060.5530 - val_loss: 1962.2358 - val_mae: 1962.2358\n",
      "Epoch 1731/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2060.0303 - mae: 2060.0303 - val_loss: 1961.7524 - val_mae: 1961.7524\n",
      "Epoch 1732/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2060.1611 - mae: 2060.1611 - val_loss: 1961.3873 - val_mae: 1961.3873\n",
      "Epoch 1733/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2057.8206 - mae: 2057.8206 - val_loss: 1959.3890 - val_mae: 1959.3890\n",
      "Epoch 1734/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2057.2261 - mae: 2057.2261 - val_loss: 1958.3785 - val_mae: 1958.3785\n",
      "Epoch 1735/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2056.4250 - mae: 2056.4250 - val_loss: 1959.1698 - val_mae: 1959.1698\n",
      "Epoch 1736/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2055.8176 - mae: 2055.8176 - val_loss: 1956.7153 - val_mae: 1956.7153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1737/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2054.9780 - mae: 2054.9780 - val_loss: 1956.0724 - val_mae: 1956.0724\n",
      "Epoch 1738/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2054.5139 - mae: 2054.5139 - val_loss: 1956.0381 - val_mae: 1956.0381\n",
      "Epoch 1739/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2055.4187 - mae: 2055.4187 - val_loss: 1954.1233 - val_mae: 1954.1233\n",
      "Epoch 1740/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2054.0493 - mae: 2054.0493 - val_loss: 1953.3527 - val_mae: 1953.3527\n",
      "Epoch 1741/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2053.6116 - mae: 2053.6116 - val_loss: 1952.0967 - val_mae: 1952.0967\n",
      "Epoch 1742/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2052.0493 - mae: 2052.0493 - val_loss: 1950.3834 - val_mae: 1950.3834\n",
      "Epoch 1743/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2051.8491 - mae: 2051.8491 - val_loss: 1949.3630 - val_mae: 1949.3630\n",
      "Epoch 1744/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2051.0679 - mae: 2051.0679 - val_loss: 1949.1038 - val_mae: 1949.1038\n",
      "Epoch 1745/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2051.3372 - mae: 2051.3372 - val_loss: 1947.3320 - val_mae: 1947.3320\n",
      "Epoch 1746/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2049.6057 - mae: 2049.6057 - val_loss: 1946.6624 - val_mae: 1946.6624\n",
      "Epoch 1747/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2050.1741 - mae: 2050.1741 - val_loss: 1945.6161 - val_mae: 1945.6161\n",
      "Epoch 1748/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2049.2947 - mae: 2049.2947 - val_loss: 1945.0833 - val_mae: 1945.0833\n",
      "Epoch 1749/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2048.2913 - mae: 2048.2913 - val_loss: 1943.4966 - val_mae: 1943.4966\n",
      "Epoch 1750/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2048.4268 - mae: 2048.4268 - val_loss: 1942.3521 - val_mae: 1942.3521\n",
      "Epoch 1751/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2046.7856 - mae: 2046.7856 - val_loss: 1942.2455 - val_mae: 1942.2455\n",
      "Epoch 1752/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2046.4630 - mae: 2046.4630 - val_loss: 1941.5029 - val_mae: 1941.5029\n",
      "Epoch 1753/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2046.1027 - mae: 2046.1027 - val_loss: 1937.9905 - val_mae: 1937.9905\n",
      "Epoch 1754/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2044.9894 - mae: 2044.9894 - val_loss: 1937.1246 - val_mae: 1937.1246\n",
      "Epoch 1755/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2044.2980 - mae: 2044.2980 - val_loss: 1937.2108 - val_mae: 1937.2108\n",
      "Epoch 1756/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2044.4023 - mae: 2044.4023 - val_loss: 1936.4763 - val_mae: 1936.4763\n",
      "Epoch 1757/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2045.2469 - mae: 2045.2469 - val_loss: 1933.8853 - val_mae: 1933.8853\n",
      "Epoch 1758/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2043.7793 - mae: 2043.7793 - val_loss: 1933.3602 - val_mae: 1933.3602\n",
      "Epoch 1759/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2043.1278 - mae: 2043.1278 - val_loss: 1934.1416 - val_mae: 1934.1416\n",
      "Epoch 1760/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2042.8702 - mae: 2042.8702 - val_loss: 1931.8019 - val_mae: 1931.8019\n",
      "Epoch 1761/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2042.5106 - mae: 2042.5106 - val_loss: 1933.6793 - val_mae: 1933.6793\n",
      "Epoch 1762/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2041.3077 - mae: 2041.3077 - val_loss: 1929.4160 - val_mae: 1929.4160\n",
      "Epoch 1763/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2041.0836 - mae: 2041.0836 - val_loss: 1929.6949 - val_mae: 1929.6949\n",
      "Epoch 1764/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2040.8275 - mae: 2040.8275 - val_loss: 1928.2498 - val_mae: 1928.2498\n",
      "Epoch 1765/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2040.9630 - mae: 2040.9630 - val_loss: 1927.2885 - val_mae: 1927.2885\n",
      "Epoch 1766/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2039.6896 - mae: 2039.6896 - val_loss: 1925.3888 - val_mae: 1925.3888\n",
      "Epoch 1767/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2038.5898 - mae: 2038.5898 - val_loss: 1924.7828 - val_mae: 1924.7828\n",
      "Epoch 1768/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2039.0552 - mae: 2039.0552 - val_loss: 1926.3552 - val_mae: 1926.3552\n",
      "Epoch 1769/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2038.4188 - mae: 2038.4188 - val_loss: 1923.9414 - val_mae: 1923.9414\n",
      "Epoch 1770/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2036.6123 - mae: 2036.6123 - val_loss: 1921.9325 - val_mae: 1921.9325\n",
      "Epoch 1771/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2036.0200 - mae: 2036.0200 - val_loss: 1921.2472 - val_mae: 1921.2472\n",
      "Epoch 1772/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2036.2993 - mae: 2036.2993 - val_loss: 1921.3638 - val_mae: 1921.3638\n",
      "Epoch 1773/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2035.9352 - mae: 2035.9352 - val_loss: 1921.2179 - val_mae: 1921.2179\n",
      "Epoch 1774/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2034.8154 - mae: 2034.8154 - val_loss: 1919.1815 - val_mae: 1919.1815\n",
      "Epoch 1775/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2034.5520 - mae: 2034.5520 - val_loss: 1917.5168 - val_mae: 1917.5168\n",
      "Epoch 1776/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2034.4050 - mae: 2034.4050 - val_loss: 1916.3416 - val_mae: 1916.3416\n",
      "Epoch 1777/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2033.9249 - mae: 2033.9249 - val_loss: 1916.0555 - val_mae: 1916.0555\n",
      "Epoch 1778/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2032.7941 - mae: 2032.7941 - val_loss: 1915.4269 - val_mae: 1915.4269\n",
      "Epoch 1779/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2032.0955 - mae: 2032.0955 - val_loss: 1914.5919 - val_mae: 1914.5919\n",
      "Epoch 1780/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2031.8438 - mae: 2031.8438 - val_loss: 1913.4866 - val_mae: 1913.4866\n",
      "Epoch 1781/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2030.9497 - mae: 2030.9497 - val_loss: 1913.1129 - val_mae: 1913.1129\n",
      "Epoch 1782/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2031.8484 - mae: 2031.8484 - val_loss: 1911.8386 - val_mae: 1911.8386\n",
      "Epoch 1783/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2030.5229 - mae: 2030.5229 - val_loss: 1916.0046 - val_mae: 1916.0046\n",
      "Epoch 1784/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2030.5629 - mae: 2030.5629 - val_loss: 1910.6324 - val_mae: 1910.6324\n",
      "Epoch 1785/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2029.0463 - mae: 2029.0463 - val_loss: 1908.7906 - val_mae: 1908.7906\n",
      "Epoch 1786/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2028.8286 - mae: 2028.8286 - val_loss: 1908.7620 - val_mae: 1908.7620\n",
      "Epoch 1787/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2027.7142 - mae: 2027.7142 - val_loss: 1908.1968 - val_mae: 1908.1968\n",
      "Epoch 1788/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2027.4924 - mae: 2027.4924 - val_loss: 1905.6805 - val_mae: 1905.6805\n",
      "Epoch 1789/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2027.8937 - mae: 2027.8937 - val_loss: 1905.2521 - val_mae: 1905.2521\n",
      "Epoch 1790/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2026.7220 - mae: 2026.7220 - val_loss: 1905.6282 - val_mae: 1905.6282\n",
      "Epoch 1791/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2026.5746 - mae: 2026.5746 - val_loss: 1904.3298 - val_mae: 1904.3298\n",
      "Epoch 1792/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2025.6885 - mae: 2025.6885 - val_loss: 1902.6088 - val_mae: 1902.6088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1793/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2024.9087 - mae: 2024.9087 - val_loss: 1901.2228 - val_mae: 1901.2228\n",
      "Epoch 1794/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2024.5049 - mae: 2024.5049 - val_loss: 1900.7273 - val_mae: 1900.7273\n",
      "Epoch 1795/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2024.0345 - mae: 2024.0345 - val_loss: 1900.6608 - val_mae: 1900.6608\n",
      "Epoch 1796/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2023.6068 - mae: 2023.6068 - val_loss: 1899.0480 - val_mae: 1899.0480\n",
      "Epoch 1797/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2022.5184 - mae: 2022.5184 - val_loss: 1898.5396 - val_mae: 1898.5396\n",
      "Epoch 1798/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2021.8782 - mae: 2021.8782 - val_loss: 1897.7197 - val_mae: 1897.7197\n",
      "Epoch 1799/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2021.7615 - mae: 2021.7615 - val_loss: 1896.6586 - val_mae: 1896.6586\n",
      "Epoch 1800/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2020.9725 - mae: 2020.9725 - val_loss: 1896.3630 - val_mae: 1896.3630\n",
      "Epoch 1801/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2021.6631 - mae: 2021.6631 - val_loss: 1894.8970 - val_mae: 1894.8970\n",
      "Epoch 1802/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2021.5641 - mae: 2021.5641 - val_loss: 1894.5649 - val_mae: 1894.5649\n",
      "Epoch 1803/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2019.8799 - mae: 2019.8799 - val_loss: 1893.8259 - val_mae: 1893.8259\n",
      "Epoch 1804/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2020.7522 - mae: 2020.7522 - val_loss: 1893.4541 - val_mae: 1893.4541\n",
      "Epoch 1805/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2018.6144 - mae: 2018.6144 - val_loss: 1891.6847 - val_mae: 1891.6847\n",
      "Epoch 1806/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2020.1310 - mae: 2020.1310 - val_loss: 1895.4760 - val_mae: 1895.4760\n",
      "Epoch 1807/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2018.5372 - mae: 2018.5372 - val_loss: 1889.9360 - val_mae: 1889.9360\n",
      "Epoch 1808/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2016.9873 - mae: 2016.9873 - val_loss: 1889.3636 - val_mae: 1889.3636\n",
      "Epoch 1809/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2016.9617 - mae: 2016.9617 - val_loss: 1889.9194 - val_mae: 1889.9194\n",
      "Epoch 1810/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2016.9983 - mae: 2016.9983 - val_loss: 1888.1643 - val_mae: 1888.1643\n",
      "Epoch 1811/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2015.8319 - mae: 2015.8319 - val_loss: 1887.8411 - val_mae: 1887.8411\n",
      "Epoch 1812/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2014.9755 - mae: 2014.9755 - val_loss: 1886.6422 - val_mae: 1886.6422\n",
      "Epoch 1813/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2015.5483 - mae: 2015.5483 - val_loss: 1886.1489 - val_mae: 1886.1489\n",
      "Epoch 1814/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2014.7734 - mae: 2014.7734 - val_loss: 1889.1698 - val_mae: 1889.1698\n",
      "Epoch 1815/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2015.6384 - mae: 2015.6384 - val_loss: 1885.7612 - val_mae: 1885.7612\n",
      "Epoch 1816/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2014.1515 - mae: 2014.1515 - val_loss: 1884.4341 - val_mae: 1884.4341\n",
      "Epoch 1817/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2012.9446 - mae: 2012.9446 - val_loss: 1883.5896 - val_mae: 1883.5896\n",
      "Epoch 1818/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2013.1860 - mae: 2013.1860 - val_loss: 1883.3834 - val_mae: 1883.3834\n",
      "Epoch 1819/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2012.9858 - mae: 2012.9858 - val_loss: 1884.4456 - val_mae: 1884.4456\n",
      "Epoch 1820/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2012.8955 - mae: 2012.8955 - val_loss: 1883.7676 - val_mae: 1883.7676\n",
      "Epoch 1821/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2012.7399 - mae: 2012.7399 - val_loss: 1882.1610 - val_mae: 1882.1610\n",
      "Epoch 1822/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2012.2052 - mae: 2012.2052 - val_loss: 1881.3396 - val_mae: 1881.3396\n",
      "Epoch 1823/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2011.4592 - mae: 2011.4592 - val_loss: 1881.2759 - val_mae: 1881.2759\n",
      "Epoch 1824/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2011.6454 - mae: 2011.6454 - val_loss: 1882.7948 - val_mae: 1882.7948\n",
      "Epoch 1825/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2011.6311 - mae: 2011.6311 - val_loss: 1882.9806 - val_mae: 1882.9806\n",
      "Epoch 1826/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2011.0214 - mae: 2011.0214 - val_loss: 1877.5198 - val_mae: 1877.5198\n",
      "Epoch 1827/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2010.6537 - mae: 2010.6537 - val_loss: 1878.7605 - val_mae: 1878.7605\n",
      "Epoch 1828/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2010.8457 - mae: 2010.8457 - val_loss: 1877.7758 - val_mae: 1877.7758\n",
      "Epoch 1829/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2009.5787 - mae: 2009.5787 - val_loss: 1877.0420 - val_mae: 1877.0420\n",
      "Epoch 1830/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2010.1462 - mae: 2010.1462 - val_loss: 1878.0748 - val_mae: 1878.0748\n",
      "Epoch 1831/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2009.0844 - mae: 2009.0844 - val_loss: 1876.8090 - val_mae: 1876.8090\n",
      "Epoch 1832/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2008.9620 - mae: 2008.9620 - val_loss: 1878.3456 - val_mae: 1878.3456\n",
      "Epoch 1833/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2009.0673 - mae: 2009.0673 - val_loss: 1875.6444 - val_mae: 1875.6444\n",
      "Epoch 1834/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2009.0072 - mae: 2009.0072 - val_loss: 1875.3839 - val_mae: 1875.3839\n",
      "Epoch 1835/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2008.1730 - mae: 2008.1730 - val_loss: 1873.8217 - val_mae: 1873.8217\n",
      "Epoch 1836/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2008.8727 - mae: 2008.8727 - val_loss: 1874.3815 - val_mae: 1874.3815\n",
      "Epoch 1837/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2007.3903 - mae: 2007.3903 - val_loss: 1873.8336 - val_mae: 1873.8336\n",
      "Epoch 1838/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2006.5792 - mae: 2006.5792 - val_loss: 1872.4930 - val_mae: 1872.4930\n",
      "Epoch 1839/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2007.3958 - mae: 2007.3958 - val_loss: 1872.0222 - val_mae: 1872.0222\n",
      "Epoch 1840/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2006.1982 - mae: 2006.1982 - val_loss: 1872.5038 - val_mae: 1872.5038\n",
      "Epoch 1841/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2007.4994 - mae: 2007.4994 - val_loss: 1871.1056 - val_mae: 1871.1056\n",
      "Epoch 1842/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2007.4969 - mae: 2007.4969 - val_loss: 1873.3062 - val_mae: 1873.3062\n",
      "Epoch 1843/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2005.7985 - mae: 2005.7985 - val_loss: 1870.4266 - val_mae: 1870.4266\n",
      "Epoch 1844/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2005.2247 - mae: 2005.2247 - val_loss: 1869.3065 - val_mae: 1869.3065\n",
      "Epoch 1845/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2005.9340 - mae: 2005.9340 - val_loss: 1869.3817 - val_mae: 1869.3817\n",
      "Epoch 1846/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2004.7797 - mae: 2004.7797 - val_loss: 1869.0065 - val_mae: 1869.0065\n",
      "Epoch 1847/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2005.7102 - mae: 2005.7102 - val_loss: 1866.6561 - val_mae: 1866.6561\n",
      "Epoch 1848/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2003.9821 - mae: 2003.9821 - val_loss: 1868.5370 - val_mae: 1868.5370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1849/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2004.2047 - mae: 2004.2047 - val_loss: 1865.8345 - val_mae: 1865.8345\n",
      "Epoch 1850/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2003.3167 - mae: 2003.3167 - val_loss: 1865.1429 - val_mae: 1865.1429\n",
      "Epoch 1851/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2003.2250 - mae: 2003.2250 - val_loss: 1865.2711 - val_mae: 1865.2711\n",
      "Epoch 1852/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2006.2883 - mae: 2006.2883 - val_loss: 1867.3806 - val_mae: 1867.3806\n",
      "Epoch 1853/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2005.5717 - mae: 2005.5717 - val_loss: 1865.2970 - val_mae: 1865.2970\n",
      "Epoch 1854/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2002.2507 - mae: 2002.2507 - val_loss: 1864.9301 - val_mae: 1864.9301\n",
      "Epoch 1855/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2001.5306 - mae: 2001.5306 - val_loss: 1863.0227 - val_mae: 1863.0227\n",
      "Epoch 1856/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2001.9818 - mae: 2001.9818 - val_loss: 1864.1367 - val_mae: 1864.1367\n",
      "Epoch 1857/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2001.1877 - mae: 2001.1877 - val_loss: 1862.0739 - val_mae: 1862.0739\n",
      "Epoch 1858/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2001.6952 - mae: 2001.6952 - val_loss: 1861.1201 - val_mae: 1861.1201\n",
      "Epoch 1859/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2001.4539 - mae: 2001.4539 - val_loss: 1859.8026 - val_mae: 1859.8026\n",
      "Epoch 1860/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2000.7633 - mae: 2000.7633 - val_loss: 1859.3267 - val_mae: 1859.3267\n",
      "Epoch 1861/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2000.5270 - mae: 2000.5270 - val_loss: 1858.9318 - val_mae: 1858.9318\n",
      "Epoch 1862/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2000.8906 - mae: 2000.8906 - val_loss: 1857.9646 - val_mae: 1857.9646\n",
      "Epoch 1863/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2000.0889 - mae: 2000.0889 - val_loss: 1858.6060 - val_mae: 1858.6060\n",
      "Epoch 1864/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2002.0818 - mae: 2002.0818 - val_loss: 1856.5957 - val_mae: 1856.5957\n",
      "Epoch 1865/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2000.7662 - mae: 2000.7662 - val_loss: 1857.6885 - val_mae: 1857.6885\n",
      "Epoch 1866/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2001.7302 - mae: 2001.7302 - val_loss: 1857.5067 - val_mae: 1857.5067\n",
      "Epoch 1867/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2000.3560 - mae: 2000.3560 - val_loss: 1857.3375 - val_mae: 1857.3375\n",
      "Epoch 1868/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1998.6249 - mae: 1998.6249 - val_loss: 1857.3354 - val_mae: 1857.3354\n",
      "Epoch 1869/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1999.2114 - mae: 1999.2114 - val_loss: 1856.9840 - val_mae: 1856.9840\n",
      "Epoch 1870/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1998.6504 - mae: 1998.6504 - val_loss: 1856.4630 - val_mae: 1856.4630\n",
      "Epoch 1871/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1998.9308 - mae: 1998.9308 - val_loss: 1854.4166 - val_mae: 1854.4166\n",
      "Epoch 1872/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1998.8015 - mae: 1998.8015 - val_loss: 1854.4760 - val_mae: 1854.4760\n",
      "Epoch 1873/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1998.9902 - mae: 1998.9902 - val_loss: 1856.1272 - val_mae: 1856.1272\n",
      "Epoch 1874/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1998.0361 - mae: 1998.0361 - val_loss: 1854.0795 - val_mae: 1854.0795\n",
      "Epoch 1875/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2001.5726 - mae: 2001.5726 - val_loss: 1857.0974 - val_mae: 1857.0974\n",
      "Epoch 1876/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1997.9377 - mae: 1997.9377 - val_loss: 1853.1825 - val_mae: 1853.1825\n",
      "Epoch 1877/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1998.1182 - mae: 1998.1182 - val_loss: 1851.2437 - val_mae: 1851.2437\n",
      "Epoch 1878/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1996.9331 - mae: 1996.9331 - val_loss: 1851.4548 - val_mae: 1851.4548\n",
      "Epoch 1879/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1997.3296 - mae: 1997.3296 - val_loss: 1850.2905 - val_mae: 1850.2905\n",
      "Epoch 1880/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1996.9764 - mae: 1996.9764 - val_loss: 1850.6006 - val_mae: 1850.6006\n",
      "Epoch 1881/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1998.5374 - mae: 1998.5374 - val_loss: 1854.4351 - val_mae: 1854.4351\n",
      "Epoch 1882/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1997.5508 - mae: 1997.5508 - val_loss: 1850.9817 - val_mae: 1850.9817\n",
      "Epoch 1883/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1995.9952 - mae: 1995.9952 - val_loss: 1850.3055 - val_mae: 1850.3055\n",
      "Epoch 1884/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1994.9701 - mae: 1994.9701 - val_loss: 1848.3612 - val_mae: 1848.3612\n",
      "Epoch 1885/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1996.6499 - mae: 1996.6499 - val_loss: 1850.5750 - val_mae: 1850.5750\n",
      "Epoch 1886/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1997.6818 - mae: 1997.6818 - val_loss: 1850.2034 - val_mae: 1850.2034\n",
      "Epoch 1887/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1996.3075 - mae: 1996.3075 - val_loss: 1847.5334 - val_mae: 1847.5334\n",
      "Epoch 1888/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1996.3785 - mae: 1996.3785 - val_loss: 1847.5922 - val_mae: 1847.5922\n",
      "Epoch 1889/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1996.7904 - mae: 1996.7904 - val_loss: 1847.7664 - val_mae: 1847.7664\n",
      "Epoch 1890/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1996.2325 - mae: 1996.2325 - val_loss: 1846.1207 - val_mae: 1846.1207\n",
      "Epoch 1891/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1995.0916 - mae: 1995.0916 - val_loss: 1847.5405 - val_mae: 1847.5405\n",
      "Epoch 1892/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1994.5339 - mae: 1994.5339 - val_loss: 1848.4470 - val_mae: 1848.4470\n",
      "Epoch 1893/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1997.5428 - mae: 1997.5428 - val_loss: 1850.4883 - val_mae: 1850.4883\n",
      "Epoch 1894/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1997.2772 - mae: 1997.2772 - val_loss: 1848.9467 - val_mae: 1848.9467\n",
      "Epoch 1895/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1995.1058 - mae: 1995.1058 - val_loss: 1845.1494 - val_mae: 1845.1494\n",
      "Epoch 1896/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1995.2814 - mae: 1995.2814 - val_loss: 1847.1707 - val_mae: 1847.1707\n",
      "Epoch 1897/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1996.2131 - mae: 1996.2131 - val_loss: 1844.9194 - val_mae: 1844.9194\n",
      "Epoch 1898/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1994.0494 - mae: 1994.0494 - val_loss: 1846.0692 - val_mae: 1846.0692\n",
      "Epoch 1899/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1993.6305 - mae: 1993.6305 - val_loss: 1844.4388 - val_mae: 1844.4388\n",
      "Epoch 1900/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1992.4008 - mae: 1992.4008 - val_loss: 1843.7925 - val_mae: 1843.7925\n",
      "Epoch 1901/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1992.9703 - mae: 1992.9703 - val_loss: 1844.6353 - val_mae: 1844.6353\n",
      "Epoch 1902/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1992.2307 - mae: 1992.2307 - val_loss: 1843.9966 - val_mae: 1843.9966\n",
      "Epoch 1903/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1993.5607 - mae: 1993.5607 - val_loss: 1843.7161 - val_mae: 1843.7161\n",
      "Epoch 1904/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1993.1521 - mae: 1993.1521 - val_loss: 1842.9302 - val_mae: 1842.9302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1905/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1993.5640 - mae: 1993.5640 - val_loss: 1844.6578 - val_mae: 1844.6578\n",
      "Epoch 1906/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1994.9904 - mae: 1994.9904 - val_loss: 1842.7338 - val_mae: 1842.7338\n",
      "Epoch 1907/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1993.0468 - mae: 1993.0468 - val_loss: 1843.0049 - val_mae: 1843.0049\n",
      "Epoch 1908/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1991.8207 - mae: 1991.8207 - val_loss: 1842.9353 - val_mae: 1842.9353\n",
      "Epoch 1909/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1992.9529 - mae: 1992.9529 - val_loss: 1842.1462 - val_mae: 1842.1462\n",
      "Epoch 1910/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1992.0942 - mae: 1992.0942 - val_loss: 1842.5840 - val_mae: 1842.5840\n",
      "Epoch 1911/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1992.5624 - mae: 1992.5624 - val_loss: 1842.1571 - val_mae: 1842.1571\n",
      "Epoch 1912/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1993.7708 - mae: 1993.7708 - val_loss: 1842.5498 - val_mae: 1842.5498\n",
      "Epoch 1913/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1993.5657 - mae: 1993.5657 - val_loss: 1841.3412 - val_mae: 1841.3412\n",
      "Epoch 1914/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1991.2393 - mae: 1991.2393 - val_loss: 1841.3186 - val_mae: 1841.3186\n",
      "Epoch 1915/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1991.1545 - mae: 1991.1545 - val_loss: 1840.0833 - val_mae: 1840.0833\n",
      "Epoch 1916/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1990.4276 - mae: 1990.4276 - val_loss: 1843.0675 - val_mae: 1843.0675\n",
      "Epoch 1917/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1990.8651 - mae: 1990.8651 - val_loss: 1841.7595 - val_mae: 1841.7595\n",
      "Epoch 1918/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1991.6771 - mae: 1991.6771 - val_loss: 1839.9562 - val_mae: 1839.9562\n",
      "Epoch 1919/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1990.8267 - mae: 1990.8267 - val_loss: 1839.2037 - val_mae: 1839.2037\n",
      "Epoch 1920/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1990.2980 - mae: 1990.2980 - val_loss: 1839.9183 - val_mae: 1839.9183\n",
      "Epoch 1921/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1991.6106 - mae: 1991.6106 - val_loss: 1842.1189 - val_mae: 1842.1189\n",
      "Epoch 1922/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1990.6100 - mae: 1990.6100 - val_loss: 1839.4071 - val_mae: 1839.4071\n",
      "Epoch 1923/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1990.3889 - mae: 1990.3889 - val_loss: 1839.2737 - val_mae: 1839.2737\n",
      "Epoch 1924/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1991.8700 - mae: 1991.8700 - val_loss: 1839.6642 - val_mae: 1839.6642\n",
      "Epoch 1925/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1990.7640 - mae: 1990.7640 - val_loss: 1838.2808 - val_mae: 1838.2808\n",
      "Epoch 1926/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1989.9414 - mae: 1989.9414 - val_loss: 1838.6158 - val_mae: 1838.6158\n",
      "Epoch 1927/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1989.8188 - mae: 1989.8188 - val_loss: 1839.1832 - val_mae: 1839.1832\n",
      "Epoch 1928/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1990.7719 - mae: 1990.7719 - val_loss: 1838.3051 - val_mae: 1838.3051\n",
      "Epoch 1929/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1989.3409 - mae: 1989.3409 - val_loss: 1838.4011 - val_mae: 1838.4011\n",
      "Epoch 1930/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1989.8059 - mae: 1989.8060 - val_loss: 1838.0239 - val_mae: 1838.0239\n",
      "Epoch 1931/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1989.3208 - mae: 1989.3208 - val_loss: 1837.5156 - val_mae: 1837.5156\n",
      "Epoch 1932/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1989.0355 - mae: 1989.0355 - val_loss: 1837.4872 - val_mae: 1837.4872\n",
      "Epoch 1933/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1990.3873 - mae: 1990.3873 - val_loss: 1837.5056 - val_mae: 1837.5056\n",
      "Epoch 1934/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1989.2943 - mae: 1989.2943 - val_loss: 1836.7593 - val_mae: 1836.7593\n",
      "Epoch 1935/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1988.4819 - mae: 1988.4819 - val_loss: 1836.9097 - val_mae: 1836.9097\n",
      "Epoch 1936/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1988.3130 - mae: 1988.3130 - val_loss: 1836.6459 - val_mae: 1836.6459\n",
      "Epoch 1937/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1988.1881 - mae: 1988.1881 - val_loss: 1837.1892 - val_mae: 1837.1892\n",
      "Epoch 1938/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1988.4198 - mae: 1988.4198 - val_loss: 1837.3672 - val_mae: 1837.3672\n",
      "Epoch 1939/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1987.2028 - mae: 1987.2028 - val_loss: 1835.6161 - val_mae: 1835.6161\n",
      "Epoch 1940/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1987.1790 - mae: 1987.1790 - val_loss: 1835.8839 - val_mae: 1835.8839\n",
      "Epoch 1941/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1987.6608 - mae: 1987.6608 - val_loss: 1835.6377 - val_mae: 1835.6377\n",
      "Epoch 1942/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1987.3037 - mae: 1987.3037 - val_loss: 1836.8654 - val_mae: 1836.8654\n",
      "Epoch 1943/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1988.0408 - mae: 1988.0408 - val_loss: 1835.7366 - val_mae: 1835.7366\n",
      "Epoch 1944/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1987.0652 - mae: 1987.0652 - val_loss: 1835.5393 - val_mae: 1835.5393\n",
      "Epoch 1945/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1986.6915 - mae: 1986.6915 - val_loss: 1836.8083 - val_mae: 1836.8083\n",
      "Epoch 1946/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1988.5361 - mae: 1988.5361 - val_loss: 1838.1936 - val_mae: 1838.1936\n",
      "Epoch 1947/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1987.5594 - mae: 1987.5594 - val_loss: 1834.8838 - val_mae: 1834.8838\n",
      "Epoch 1948/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1986.7831 - mae: 1986.7831 - val_loss: 1834.2664 - val_mae: 1834.2664\n",
      "Epoch 1949/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1985.9559 - mae: 1985.9559 - val_loss: 1834.3778 - val_mae: 1834.3778\n",
      "Epoch 1950/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1986.3754 - mae: 1986.3754 - val_loss: 1840.3800 - val_mae: 1840.3800\n",
      "Epoch 1951/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1987.6829 - mae: 1987.6829 - val_loss: 1834.4100 - val_mae: 1834.4100\n",
      "Epoch 1952/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1986.6058 - mae: 1986.6058 - val_loss: 1834.2627 - val_mae: 1834.2627\n",
      "Epoch 1953/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1985.6324 - mae: 1985.6324 - val_loss: 1834.4637 - val_mae: 1834.4637\n",
      "Epoch 1954/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1987.3713 - mae: 1987.3713 - val_loss: 1834.0820 - val_mae: 1834.0820\n",
      "Epoch 1955/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1985.4164 - mae: 1985.4164 - val_loss: 1834.0795 - val_mae: 1834.0795\n",
      "Epoch 1956/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1984.9183 - mae: 1984.9183 - val_loss: 1833.3518 - val_mae: 1833.3518\n",
      "Epoch 1957/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1985.5046 - mae: 1985.5046 - val_loss: 1832.9689 - val_mae: 1832.9689\n",
      "Epoch 1958/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1985.0737 - mae: 1985.0737 - val_loss: 1832.7793 - val_mae: 1832.7793\n",
      "Epoch 1959/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1986.3107 - mae: 1986.3107 - val_loss: 1832.3524 - val_mae: 1832.3524\n",
      "Epoch 1960/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1984.9943 - mae: 1984.9943 - val_loss: 1832.0660 - val_mae: 1832.0660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1961/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1984.2783 - mae: 1984.2783 - val_loss: 1832.4884 - val_mae: 1832.4884\n",
      "Epoch 1962/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1984.5813 - mae: 1984.5813 - val_loss: 1833.5198 - val_mae: 1833.5198\n",
      "Epoch 1963/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1985.0985 - mae: 1985.0985 - val_loss: 1832.6279 - val_mae: 1832.6279\n",
      "Epoch 1964/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1984.5758 - mae: 1984.5758 - val_loss: 1833.1920 - val_mae: 1833.1920\n",
      "Epoch 1965/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1984.1754 - mae: 1984.1754 - val_loss: 1834.2440 - val_mae: 1834.2440\n",
      "Epoch 1966/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1984.8887 - mae: 1984.8887 - val_loss: 1831.9247 - val_mae: 1831.9247\n",
      "Epoch 1967/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1983.7797 - mae: 1983.7797 - val_loss: 1831.5360 - val_mae: 1831.5360\n",
      "Epoch 1968/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1984.0511 - mae: 1984.0511 - val_loss: 1836.6903 - val_mae: 1836.6903\n",
      "Epoch 1969/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1983.8591 - mae: 1983.8591 - val_loss: 1831.2672 - val_mae: 1831.2672\n",
      "Epoch 1970/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1984.2335 - mae: 1984.2335 - val_loss: 1830.8590 - val_mae: 1830.8590\n",
      "Epoch 1971/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1983.6434 - mae: 1983.6434 - val_loss: 1832.2122 - val_mae: 1832.2122\n",
      "Epoch 1972/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1983.1832 - mae: 1983.1832 - val_loss: 1835.1713 - val_mae: 1835.1713\n",
      "Epoch 1973/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1983.6920 - mae: 1983.6920 - val_loss: 1832.1608 - val_mae: 1832.1608\n",
      "Epoch 1974/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1983.5621 - mae: 1983.5621 - val_loss: 1830.2593 - val_mae: 1830.2593\n",
      "Epoch 1975/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1983.6080 - mae: 1983.6080 - val_loss: 1831.3190 - val_mae: 1831.3190\n",
      "Epoch 1976/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1983.8325 - mae: 1983.8325 - val_loss: 1831.7836 - val_mae: 1831.7836\n",
      "Epoch 1977/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1983.1333 - mae: 1983.1333 - val_loss: 1832.0247 - val_mae: 1832.0247\n",
      "Epoch 1978/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1983.3146 - mae: 1983.3146 - val_loss: 1830.8127 - val_mae: 1830.8127\n",
      "Epoch 1979/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1982.5964 - mae: 1982.5964 - val_loss: 1832.3132 - val_mae: 1832.3132\n",
      "Epoch 1980/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1982.2852 - mae: 1982.2852 - val_loss: 1831.0237 - val_mae: 1831.0237\n",
      "Epoch 1981/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1984.8291 - mae: 1984.8291 - val_loss: 1831.7522 - val_mae: 1831.7522\n",
      "Epoch 1982/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1984.0299 - mae: 1984.0299 - val_loss: 1830.4128 - val_mae: 1830.4128\n",
      "Epoch 1983/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1983.4729 - mae: 1983.4729 - val_loss: 1833.2578 - val_mae: 1833.2578\n",
      "Epoch 1984/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1981.7930 - mae: 1981.7930 - val_loss: 1830.4625 - val_mae: 1830.4625\n",
      "Epoch 1985/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1982.0161 - mae: 1982.0161 - val_loss: 1830.1311 - val_mae: 1830.1311\n",
      "Epoch 1986/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1981.7736 - mae: 1981.7736 - val_loss: 1830.1614 - val_mae: 1830.1614\n",
      "Epoch 1987/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1980.9358 - mae: 1980.9358 - val_loss: 1831.5660 - val_mae: 1831.5660\n",
      "Epoch 1988/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1981.8342 - mae: 1981.8342 - val_loss: 1830.0919 - val_mae: 1830.0919\n",
      "Epoch 1989/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1981.6007 - mae: 1981.6007 - val_loss: 1830.6584 - val_mae: 1830.6584\n",
      "Epoch 1990/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1981.1877 - mae: 1981.1877 - val_loss: 1830.7317 - val_mae: 1830.7317\n",
      "Epoch 1991/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1981.0580 - mae: 1981.0580 - val_loss: 1830.7804 - val_mae: 1830.7804\n",
      "Epoch 1992/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1981.0914 - mae: 1981.0914 - val_loss: 1831.4393 - val_mae: 1831.4393\n",
      "Epoch 1993/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1980.8140 - mae: 1980.8140 - val_loss: 1832.6674 - val_mae: 1832.6674\n",
      "Epoch 1994/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1981.8215 - mae: 1981.8215 - val_loss: 1837.6812 - val_mae: 1837.6812\n",
      "Epoch 1995/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1981.6456 - mae: 1981.6456 - val_loss: 1833.0326 - val_mae: 1833.0326\n",
      "Epoch 1996/2000\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1980.3250 - mae: 1980.3250 - val_loss: 1830.8177 - val_mae: 1830.8177\n",
      "Epoch 1997/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1980.0651 - mae: 1980.0651 - val_loss: 1830.8831 - val_mae: 1830.8831\n",
      "Epoch 1998/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1980.1638 - mae: 1980.1638 - val_loss: 1835.9664 - val_mae: 1835.9664\n",
      "Epoch 1999/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1980.8772 - mae: 1980.8772 - val_loss: 1832.6533 - val_mae: 1832.6533\n",
      "Epoch 2000/2000\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1980.0026 - mae: 1980.0026 - val_loss: 1830.7842 - val_mae: 1830.7842\n"
     ]
    }
   ],
   "source": [
    "# Set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Build the model (3 layers, 11, 100, 1 units)\n",
    "model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(11, activation=\"relu\", input_shape = x_train_transformed.shape[1:]),\n",
    "        tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(1)])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss= tf.keras.losses.mae, optimizer= tf.keras.optimizers.Adam(), metrics= ['mae'])\n",
    "\n",
    "# Fit the model for 100 epochs \n",
    "history = model.fit(x_train_transformed, y_train, validation_data=(x_test_transformed, y_test), epochs=2000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3208384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 11)                132       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               1200      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,433\n",
      "Trainable params: 1,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9b4f0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 601us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 9112.778 ],\n",
       "       [ 5297.144 ],\n",
       "       [32019.658 ],\n",
       "       [ 9318.088 ],\n",
       "       [27904.082 ],\n",
       "       [ 4565.3213],\n",
       "       [ 2075.2844],\n",
       "       [14155.562 ],\n",
       "       [ 3738.3591],\n",
       "       [10279.015 ],\n",
       "       [19837.904 ],\n",
       "       [ 7204.9785],\n",
       "       [ 3917.1838],\n",
       "       [50851.84  ],\n",
       "       [53288.992 ],\n",
       "       [45994.41  ],\n",
       "       [ 9814.629 ],\n",
       "       [43102.57  ],\n",
       "       [ 8257.184 ],\n",
       "       [26325.914 ],\n",
       "       [ 5111.7246],\n",
       "       [ 7408.6763],\n",
       "       [ 1262.463 ],\n",
       "       [ 2819.7878],\n",
       "       [11076.87  ],\n",
       "       [10881.1045],\n",
       "       [12664.368 ],\n",
       "       [ 5187.1465],\n",
       "       [ 9752.757 ],\n",
       "       [ 1136.4686],\n",
       "       [ 8234.11  ],\n",
       "       [11873.46  ],\n",
       "       [ 2019.9954],\n",
       "       [ 5648.622 ],\n",
       "       [ 2912.4421],\n",
       "       [ 7485.577 ],\n",
       "       [ 2605.8984],\n",
       "       [ 7302.247 ],\n",
       "       [24020.76  ],\n",
       "       [37350.516 ],\n",
       "       [ 4664.4165],\n",
       "       [ 2664.2559],\n",
       "       [11696.437 ],\n",
       "       [12156.415 ],\n",
       "       [ 4910.7603],\n",
       "       [12366.237 ],\n",
       "       [ 3621.5107],\n",
       "       [ 4442.8315],\n",
       "       [39866.79  ],\n",
       "       [ 4437.8945],\n",
       "       [13836.825 ],\n",
       "       [ 1723.4381],\n",
       "       [ 6874.85  ],\n",
       "       [ 1709.3228],\n",
       "       [10600.261 ],\n",
       "       [10237.341 ],\n",
       "       [ 3597.8552],\n",
       "       [33673.5   ],\n",
       "       [11873.45  ],\n",
       "       [10510.293 ],\n",
       "       [13832.255 ],\n",
       "       [ 4711.915 ],\n",
       "       [14376.739 ],\n",
       "       [ 7937.406 ],\n",
       "       [10188.325 ],\n",
       "       [ 4148.763 ],\n",
       "       [19469.533 ],\n",
       "       [10855.888 ],\n",
       "       [ 3649.6792],\n",
       "       [ 1690.3472],\n",
       "       [ 6127.365 ],\n",
       "       [ 9440.399 ],\n",
       "       [ 8319.139 ],\n",
       "       [ 6339.1533],\n",
       "       [ 7157.913 ],\n",
       "       [ 4919.841 ],\n",
       "       [ 4881.3125],\n",
       "       [11462.812 ],\n",
       "       [ 4357.9336],\n",
       "       [ 9100.749 ],\n",
       "       [ 1406.8951],\n",
       "       [32375.809 ],\n",
       "       [ 5126.163 ],\n",
       "       [37813.848 ],\n",
       "       [56784.594 ],\n",
       "       [37358.445 ],\n",
       "       [ 4772.4277],\n",
       "       [10607.791 ],\n",
       "       [ 8263.331 ],\n",
       "       [11238.019 ],\n",
       "       [15237.399 ],\n",
       "       [26503.314 ],\n",
       "       [27553.088 ],\n",
       "       [ 5019.201 ],\n",
       "       [37408.887 ],\n",
       "       [ 6816.416 ],\n",
       "       [26227.217 ],\n",
       "       [ 1879.9115],\n",
       "       [16648.906 ],\n",
       "       [ 6427.2466],\n",
       "       [ 4482.001 ],\n",
       "       [ 1741.5206],\n",
       "       [ 5657.661 ],\n",
       "       [12414.122 ],\n",
       "       [13118.869 ],\n",
       "       [ 1731.7677],\n",
       "       [ 7731.532 ],\n",
       "       [21696.012 ],\n",
       "       [ 1702.653 ],\n",
       "       [22785.342 ],\n",
       "       [ 1266.3315],\n",
       "       [ 2953.7375],\n",
       "       [13050.468 ],\n",
       "       [33332.25  ],\n",
       "       [ 9813.401 ],\n",
       "       [ 1989.2339],\n",
       "       [12740.845 ],\n",
       "       [27858.992 ],\n",
       "       [ 6337.9346],\n",
       "       [ 2816.7068],\n",
       "       [ 5615.535 ],\n",
       "       [ 7212.994 ],\n",
       "       [11362.478 ],\n",
       "       [ 2002.6594],\n",
       "       [ 4737.8696],\n",
       "       [ 7691.426 ],\n",
       "       [ 7315.673 ],\n",
       "       [ 9332.876 ],\n",
       "       [12263.502 ],\n",
       "       [ 1854.3667],\n",
       "       [ 3957.9968],\n",
       "       [ 5974.904 ],\n",
       "       [ 5866.442 ],\n",
       "       [ 7963.1626],\n",
       "       [ 5642.702 ],\n",
       "       [12394.414 ],\n",
       "       [ 4337.0137],\n",
       "       [33048.812 ],\n",
       "       [45915.49  ],\n",
       "       [32637.557 ],\n",
       "       [ 5291.7046],\n",
       "       [ 9872.804 ],\n",
       "       [ 2328.8394],\n",
       "       [11143.737 ],\n",
       "       [ 2139.3213],\n",
       "       [27238.17  ],\n",
       "       [ 5273.8726],\n",
       "       [ 3708.3752],\n",
       "       [11405.849 ],\n",
       "       [ 4777.682 ],\n",
       "       [44085.523 ],\n",
       "       [ 2743.5635],\n",
       "       [ 1145.2203],\n",
       "       [35053.832 ],\n",
       "       [ 6388.804 ],\n",
       "       [ 4575.076 ],\n",
       "       [13536.209 ],\n",
       "       [ 8577.8955],\n",
       "       [30559.965 ],\n",
       "       [35850.17  ],\n",
       "       [13648.902 ],\n",
       "       [ 2678.8162],\n",
       "       [14923.386 ],\n",
       "       [ 2422.016 ],\n",
       "       [ 3619.4995],\n",
       "       [ 7106.91  ],\n",
       "       [51536.344 ],\n",
       "       [36985.242 ],\n",
       "       [34646.867 ],\n",
       "       [ 2674.781 ],\n",
       "       [ 9206.8545],\n",
       "       [ 6230.982 ],\n",
       "       [ 5791.765 ],\n",
       "       [ 4203.32  ],\n",
       "       [ 2176.2932],\n",
       "       [24642.826 ],\n",
       "       [23640.262 ],\n",
       "       [13758.206 ],\n",
       "       [15532.351 ],\n",
       "       [11848.085 ],\n",
       "       [34806.242 ],\n",
       "       [ 3188.953 ],\n",
       "       [ 8464.659 ],\n",
       "       [ 5024.5854],\n",
       "       [ 5180.548 ],\n",
       "       [ 2921.0017],\n",
       "       [ 4010.3215],\n",
       "       [ 3484.0823],\n",
       "       [ 8596.88  ],\n",
       "       [10592.734 ],\n",
       "       [ 3631.43  ],\n",
       "       [ 2137.8562],\n",
       "       [ 2774.7085],\n",
       "       [39508.695 ],\n",
       "       [12968.316 ],\n",
       "       [ 9277.476 ],\n",
       "       [ 2675.467 ],\n",
       "       [12070.165 ],\n",
       "       [ 2130.1282],\n",
       "       [ 8948.503 ],\n",
       "       [ 3217.4033],\n",
       "       [29047.14  ],\n",
       "       [ 4211.2603],\n",
       "       [ 2815.6262],\n",
       "       [20534.777 ],\n",
       "       [18964.32  ],\n",
       "       [ 8847.611 ],\n",
       "       [ 4346.0664],\n",
       "       [ 7847.8633],\n",
       "       [ 3243.7947],\n",
       "       [12925.794 ],\n",
       "       [12045.629 ],\n",
       "       [ 9459.169 ],\n",
       "       [21936.225 ],\n",
       "       [ 6562.443 ],\n",
       "       [ 4093.7507],\n",
       "       [ 4785.197 ],\n",
       "       [14042.192 ],\n",
       "       [12624.708 ],\n",
       "       [ 5275.3613],\n",
       "       [ 2437.6306],\n",
       "       [ 6948.3726],\n",
       "       [ 6480.031 ],\n",
       "       [43505.465 ],\n",
       "       [ 2071.5432],\n",
       "       [29872.031 ],\n",
       "       [ 1728.3927],\n",
       "       [ 1644.08  ],\n",
       "       [ 9375.849 ],\n",
       "       [10978.408 ],\n",
       "       [ 1548.3407],\n",
       "       [ 9636.194 ],\n",
       "       [ 4501.9526],\n",
       "       [29787.846 ],\n",
       "       [ 9955.675 ],\n",
       "       [ 8163.075 ],\n",
       "       [ 3910.8225],\n",
       "       [ 5926.1025],\n",
       "       [47398.457 ],\n",
       "       [ 1680.3567],\n",
       "       [13194.142 ],\n",
       "       [37687.812 ],\n",
       "       [ 3087.3633],\n",
       "       [ 3428.268 ],\n",
       "       [ 1636.3212],\n",
       "       [ 2721.791 ],\n",
       "       [ 4346.546 ],\n",
       "       [ 4061.9539],\n",
       "       [12550.273 ],\n",
       "       [ 1625.2958],\n",
       "       [ 1875.7992],\n",
       "       [ 7171.3447],\n",
       "       [ 3152.39  ],\n",
       "       [12028.003 ],\n",
       "       [ 2695.302 ],\n",
       "       [ 3271.2617],\n",
       "       [12162.763 ],\n",
       "       [ 3324.2917],\n",
       "       [ 8741.56  ],\n",
       "       [ 6067.295 ],\n",
       "       [ 8030.3623],\n",
       "       [13412.046 ],\n",
       "       [25546.955 ],\n",
       "       [44815.59  ],\n",
       "       [12252.319 ],\n",
       "       [ 6107.326 ],\n",
       "       [62215.75  ],\n",
       "       [ 9860.489 ],\n",
       "       [ 9207.975 ],\n",
       "       [ 8584.821 ],\n",
       "       [ 9585.619 ],\n",
       "       [ 8588.831 ],\n",
       "       [12465.819 ],\n",
       "       [13368.591 ],\n",
       "       [ 2168.701 ],\n",
       "       [12830.323 ],\n",
       "       [ 6618.968 ],\n",
       "       [37324.62  ],\n",
       "       [ 5725.751 ],\n",
       "       [ 9065.087 ],\n",
       "       [ 9012.05  ],\n",
       "       [14825.858 ],\n",
       "       [10105.834 ],\n",
       "       [ 8854.509 ],\n",
       "       [36236.35  ],\n",
       "       [ 2753.193 ],\n",
       "       [ 9864.407 ],\n",
       "       [51909.688 ],\n",
       "       [ 4191.251 ],\n",
       "       [ 2570.8845],\n",
       "       [ 2145.3264],\n",
       "       [10982.842 ],\n",
       "       [13752.997 ],\n",
       "       [10768.464 ],\n",
       "       [ 9786.681 ],\n",
       "       [10372.318 ],\n",
       "       [ 9329.081 ],\n",
       "       [ 3404.3538],\n",
       "       [ 2166.2097],\n",
       "       [40770.152 ],\n",
       "       [32164.164 ],\n",
       "       [ 6367.592 ],\n",
       "       [ 3286.7268],\n",
       "       [18129.12  ],\n",
       "       [12964.164 ],\n",
       "       [36537.406 ],\n",
       "       [ 2468.2605],\n",
       "       [43159.496 ],\n",
       "       [ 5163.921 ],\n",
       "       [ 1634.226 ],\n",
       "       [ 6431.1294],\n",
       "       [ 1724.3812],\n",
       "       [16608.156 ],\n",
       "       [13443.841 ],\n",
       "       [ 2108.7932],\n",
       "       [12096.929 ],\n",
       "       [10203.54  ],\n",
       "       [12279.6455],\n",
       "       [42368.832 ],\n",
       "       [12687.005 ],\n",
       "       [23646.691 ],\n",
       "       [ 7893.3184],\n",
       "       [15172.647 ],\n",
       "       [ 3583.1362],\n",
       "       [ 8256.79  ],\n",
       "       [ 8042.3154],\n",
       "       [13966.804 ],\n",
       "       [10778.53  ],\n",
       "       [ 3313.72  ],\n",
       "       [10624.073 ],\n",
       "       [13221.189 ],\n",
       "       [11974.2   ],\n",
       "       [ 4543.3784],\n",
       "       [25646.266 ],\n",
       "       [ 5643.0835]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted = model.predict(x_test_transformed)\n",
    "y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f6805d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((335,), (335, 1))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the tensor shapes\n",
    "y_test.shape, y_predicted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2bfeacd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(335,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape after squeeze()\n",
    "y_predicted.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9b6333d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1830.7843>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calcuate the MAE\n",
    "mae = tf.metrics.mean_absolute_error(y_true=y_test, \n",
    "                                     y_pred=y_predicted.squeeze()) # use squeeze() to make same shape\n",
    "mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee0fcb0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=22099688.0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the MSE\n",
    "mse = tf.metrics.mean_squared_error(y_true=y_test,\n",
    "                                    y_pred=y_predicted.squeeze())\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "deeabad3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=1830.7843>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=22099688.0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4485d2ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8cd56a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(pd.DataFrame(history.history).loc[:, ['loss', 'val_loss']]).plot(figsize=(8, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d92af9",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "\n",
    "There is very less improvement nearly around  epochs. We can stop the epochs using early stopping to get quick overview of how sharp the model will have reductio in loss / val_loss etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77890fa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_mae</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>1980.324951</td>\n",
       "      <td>1980.324951</td>\n",
       "      <td>1830.817749</td>\n",
       "      <td>1830.817749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1980.065063</td>\n",
       "      <td>1980.065063</td>\n",
       "      <td>1830.883057</td>\n",
       "      <td>1830.883057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1980.163818</td>\n",
       "      <td>1980.163818</td>\n",
       "      <td>1835.966431</td>\n",
       "      <td>1835.966431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1980.877197</td>\n",
       "      <td>1980.877197</td>\n",
       "      <td>1832.653320</td>\n",
       "      <td>1832.653320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1980.002563</td>\n",
       "      <td>1980.002563</td>\n",
       "      <td>1830.784180</td>\n",
       "      <td>1830.784180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             loss          mae     val_loss      val_mae\n",
       "1995  1980.324951  1980.324951  1830.817749  1830.817749\n",
       "1996  1980.065063  1980.065063  1830.883057  1830.883057\n",
       "1997  1980.163818  1980.163818  1835.966431  1835.966431\n",
       "1998  1980.877197  1980.877197  1832.653320  1832.653320\n",
       "1999  1980.002563  1980.002563  1830.784180  1830.784180"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hist = pd.DataFrame(history.history)\n",
    "df_hist.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603a5bdd",
   "metadata": {},
   "source": [
    "Note: last values at 2k epochs, lets compare it with the values that we get by earlly stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14f7ab67",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='auto', patience = 10, min_delta=.01,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45429eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving best model weights observed during the epochs within early stopping\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint('data/best_model.h5', monitor='val_loss', save_best_only=True, mode='min', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c731679d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      " 1/32 [..............................] - ETA: 10s - loss: 14668.8008 - mae: 14668.8008\n",
      "Epoch 1: val_loss improved from inf to 13276.34961, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 13267.3145 - mae: 13267.3145 - val_loss: 13276.3496 - val_mae: 13276.3496\n",
      "Epoch 2/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 11206.2168 - mae: 11206.2168\n",
      "Epoch 2: val_loss improved from 13276.34961 to 13272.89062, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 13265.0879 - mae: 13265.0879 - val_loss: 13272.8906 - val_mae: 13272.8906\n",
      "Epoch 3/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 10276.3848 - mae: 10276.3848\n",
      "Epoch 3: val_loss improved from 13272.89062 to 13264.56836, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 13259.6855 - mae: 13259.6855 - val_loss: 13264.5684 - val_mae: 13264.5684\n",
      "Epoch 4/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 9043.7959 - mae: 9043.7959\n",
      "Epoch 4: val_loss improved from 13264.56836 to 13246.71387, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 13247.4248 - mae: 13247.4248 - val_loss: 13246.7139 - val_mae: 13246.7139\n",
      "Epoch 5/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 13077.3721 - mae: 13077.3721\n",
      "Epoch 5: val_loss improved from 13246.71387 to 13213.50586, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 13223.0703 - mae: 13223.0703 - val_loss: 13213.5059 - val_mae: 13213.5059\n",
      "Epoch 6/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 12948.3936 - mae: 12948.3936\n",
      "Epoch 6: val_loss improved from 13213.50586 to 13158.47656, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 13180.5273 - mae: 13180.5273 - val_loss: 13158.4766 - val_mae: 13158.4766\n",
      "Epoch 7/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 10920.7793 - mae: 10920.7793\n",
      "Epoch 7: val_loss improved from 13158.47656 to 13075.43262, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 13113.2656 - mae: 13113.2656 - val_loss: 13075.4326 - val_mae: 13075.4326\n",
      "Epoch 8/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 13469.0176 - mae: 13469.0176\n",
      "Epoch 8: val_loss improved from 13075.43262 to 12958.11035, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 13014.8945 - mae: 13014.8945 - val_loss: 12958.1104 - val_mae: 12958.1104\n",
      "Epoch 9/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 15945.9795 - mae: 15945.9795\n",
      "Epoch 9: val_loss improved from 12958.11035 to 12801.63867, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 12880.1621 - mae: 12880.1621 - val_loss: 12801.6387 - val_mae: 12801.6387\n",
      "Epoch 10/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 13296.3281 - mae: 13296.3281\n",
      "Epoch 10: val_loss improved from 12801.63867 to 12602.09863, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 12704.4736 - mae: 12704.4736 - val_loss: 12602.0986 - val_mae: 12602.0986\n",
      "Epoch 11/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 10290.6406 - mae: 10290.6406\n",
      "Epoch 11: val_loss improved from 12602.09863 to 12354.15625, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 12483.2568 - mae: 12483.2568 - val_loss: 12354.1562 - val_mae: 12354.1562\n",
      "Epoch 12/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 11486.1602 - mae: 11486.1602\n",
      "Epoch 12: val_loss improved from 12354.15625 to 12054.80176, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 12212.3467 - mae: 12212.3467 - val_loss: 12054.8018 - val_mae: 12054.8018\n",
      "Epoch 13/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 13825.5547 - mae: 13825.5547\n",
      "Epoch 13: val_loss improved from 12054.80176 to 11708.10547, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 11891.5068 - mae: 11891.5068 - val_loss: 11708.1055 - val_mae: 11708.1055\n",
      "Epoch 14/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 11416.2031 - mae: 11416.2031\n",
      "Epoch 14: val_loss improved from 11708.10547 to 11320.74512, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 11525.4639 - mae: 11525.4639 - val_loss: 11320.7451 - val_mae: 11320.7451\n",
      "Epoch 15/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 10783.2822 - mae: 10783.2822\n",
      "Epoch 15: val_loss improved from 11320.74512 to 10923.74121, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 11134.2441 - mae: 11134.2441 - val_loss: 10923.7412 - val_mae: 10923.7412\n",
      "Epoch 16/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 10387.9414 - mae: 10387.9414\n",
      "Epoch 16: val_loss improved from 10923.74121 to 10539.37500, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 10743.5625 - mae: 10743.5625 - val_loss: 10539.3750 - val_mae: 10539.3750\n",
      "Epoch 17/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 12067.0029 - mae: 12067.0029\n",
      "Epoch 17: val_loss improved from 10539.37500 to 10158.67090, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 10357.1641 - mae: 10357.1641 - val_loss: 10158.6709 - val_mae: 10158.6709\n",
      "Epoch 18/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 13761.7412 - mae: 13761.7412\n",
      "Epoch 18: val_loss improved from 10158.67090 to 9788.52246, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 9977.7568 - mae: 9977.7568 - val_loss: 9788.5225 - val_mae: 9788.5225\n",
      "Epoch 19/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 11469.1436 - mae: 11469.1436\n",
      "Epoch 19: val_loss improved from 9788.52246 to 9450.85938, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 9609.1768 - mae: 9609.1768 - val_loss: 9450.8594 - val_mae: 9450.8594\n",
      "Epoch 20/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 10739.0811 - mae: 10739.0811\n",
      "Epoch 20: val_loss improved from 9450.85938 to 9163.70117, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 9278.4951 - mae: 9278.4951 - val_loss: 9163.7012 - val_mae: 9163.7012\n",
      "Epoch 21/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 7448.7710 - mae: 7448.7710\n",
      "Epoch 21: val_loss improved from 9163.70117 to 8922.77246, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 8978.8340 - mae: 8978.8340 - val_loss: 8922.7725 - val_mae: 8922.7725\n",
      "Epoch 22/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 10473.4160 - mae: 10473.4160\n",
      "Epoch 22: val_loss improved from 8922.77246 to 8726.44727, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 8724.6982 - mae: 8724.6982 - val_loss: 8726.4473 - val_mae: 8726.4473\n",
      "Epoch 23/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 11036.7617 - mae: 11036.7617\n",
      "Epoch 23: val_loss improved from 8726.44727 to 8568.62305, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 8512.4268 - mae: 8512.4268 - val_loss: 8568.6230 - val_mae: 8568.6230\n",
      "Epoch 24/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 10289.8359 - mae: 10289.8359\n",
      "Epoch 24: val_loss improved from 8568.62305 to 8447.78613, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 8344.3682 - mae: 8344.3682 - val_loss: 8447.7861 - val_mae: 8447.7861\n",
      "Epoch 25/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 8053.3208 - mae: 8053.3208\n",
      "Epoch 25: val_loss improved from 8447.78613 to 8341.73828, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 8209.8750 - mae: 8209.8750 - val_loss: 8341.7383 - val_mae: 8341.7383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 7217.8604 - mae: 7217.8604\n",
      "Epoch 26: val_loss improved from 8341.73828 to 8267.33105, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 8105.5298 - mae: 8105.5298 - val_loss: 8267.3311 - val_mae: 8267.3311\n",
      "Epoch 27/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 7602.7456 - mae: 7602.7456\n",
      "Epoch 27: val_loss improved from 8267.33105 to 8202.48145, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 8033.3169 - mae: 8033.3169 - val_loss: 8202.4814 - val_mae: 8202.4814\n",
      "Epoch 28/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 8422.7451 - mae: 8422.7451\n",
      "Epoch 28: val_loss improved from 8202.48145 to 8154.06787, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7979.1118 - mae: 7979.1123 - val_loss: 8154.0679 - val_mae: 8154.0679\n",
      "Epoch 29/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 8452.3096 - mae: 8452.3096\n",
      "Epoch 29: val_loss improved from 8154.06787 to 8112.26562, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7932.8467 - mae: 7932.8467 - val_loss: 8112.2656 - val_mae: 8112.2656\n",
      "Epoch 30/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 7908.5869 - mae: 7908.5869\n",
      "Epoch 30: val_loss improved from 8112.26562 to 8074.38672, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7892.1733 - mae: 7892.1733 - val_loss: 8074.3867 - val_mae: 8074.3867\n",
      "Epoch 31/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 9543.4043 - mae: 9543.4043\n",
      "Epoch 31: val_loss improved from 8074.38672 to 8035.55078, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7857.7900 - mae: 7857.7900 - val_loss: 8035.5508 - val_mae: 8035.5508\n",
      "Epoch 32/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 7409.8340 - mae: 7409.8340\n",
      "Epoch 32: val_loss improved from 8035.55078 to 8004.78662, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7824.7012 - mae: 7824.7012 - val_loss: 8004.7866 - val_mae: 8004.7866\n",
      "Epoch 33/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 6867.6484 - mae: 6867.6484\n",
      "Epoch 33: val_loss improved from 8004.78662 to 7974.08350, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7792.9087 - mae: 7792.9087 - val_loss: 7974.0835 - val_mae: 7974.0835\n",
      "Epoch 34/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 8002.5615 - mae: 8002.5615\n",
      "Epoch 34: val_loss improved from 7974.08350 to 7943.48633, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7761.6968 - mae: 7761.6968 - val_loss: 7943.4863 - val_mae: 7943.4863\n",
      "Epoch 35/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 9447.5996 - mae: 9447.5996\n",
      "Epoch 35: val_loss improved from 7943.48633 to 7913.85010, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7729.8936 - mae: 7729.8936 - val_loss: 7913.8501 - val_mae: 7913.8501\n",
      "Epoch 36/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 6824.7578 - mae: 6824.7578\n",
      "Epoch 36: val_loss improved from 7913.85010 to 7884.31787, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7698.7344 - mae: 7698.7344 - val_loss: 7884.3179 - val_mae: 7884.3179\n",
      "Epoch 37/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 9677.7852 - mae: 9677.7852\n",
      "Epoch 37: val_loss improved from 7884.31787 to 7854.33789, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7666.2725 - mae: 7666.2725 - val_loss: 7854.3379 - val_mae: 7854.3379\n",
      "Epoch 38/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 9124.4844 - mae: 9124.4844\n",
      "Epoch 38: val_loss improved from 7854.33789 to 7822.76123, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7635.5908 - mae: 7635.5908 - val_loss: 7822.7612 - val_mae: 7822.7612\n",
      "Epoch 39/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 9602.2197 - mae: 9602.2197\n",
      "Epoch 39: val_loss improved from 7822.76123 to 7792.29785, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7601.0425 - mae: 7601.0425 - val_loss: 7792.2979 - val_mae: 7792.2979\n",
      "Epoch 40/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 7317.8613 - mae: 7317.8613\n",
      "Epoch 40: val_loss improved from 7792.29785 to 7759.49414, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7570.6841 - mae: 7570.6841 - val_loss: 7759.4941 - val_mae: 7759.4941\n",
      "Epoch 41/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 7490.9712 - mae: 7490.9712\n",
      "Epoch 41: val_loss improved from 7759.49414 to 7727.59424, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7537.4160 - mae: 7537.4160 - val_loss: 7727.5942 - val_mae: 7727.5942\n",
      "Epoch 42/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 11689.2803 - mae: 11689.2803\n",
      "Epoch 42: val_loss improved from 7727.59424 to 7694.73291, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7505.5410 - mae: 7505.5410 - val_loss: 7694.7329 - val_mae: 7694.7329\n",
      "Epoch 43/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 8001.2046 - mae: 8001.2046\n",
      "Epoch 43: val_loss improved from 7694.73291 to 7661.06787, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7472.7026 - mae: 7472.7026 - val_loss: 7661.0679 - val_mae: 7661.0679\n",
      "Epoch 44/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 7556.1997 - mae: 7556.1997\n",
      "Epoch 44: val_loss improved from 7661.06787 to 7627.45068, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7439.1953 - mae: 7439.1953 - val_loss: 7627.4507 - val_mae: 7627.4507\n",
      "Epoch 45/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 7767.8164 - mae: 7767.8164\n",
      "Epoch 45: val_loss improved from 7627.45068 to 7593.01953, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7405.6172 - mae: 7405.6172 - val_loss: 7593.0195 - val_mae: 7593.0195\n",
      "Epoch 46/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 7867.3882 - mae: 7867.3882\n",
      "Epoch 46: val_loss improved from 7593.01953 to 7558.00684, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7370.5024 - mae: 7370.5024 - val_loss: 7558.0068 - val_mae: 7558.0068\n",
      "Epoch 47/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 4981.9277 - mae: 4981.9277\n",
      "Epoch 47: val_loss improved from 7558.00684 to 7522.26709, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7335.6089 - mae: 7335.6089 - val_loss: 7522.2671 - val_mae: 7522.2671\n",
      "Epoch 48/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 8490.9170 - mae: 8490.9170\n",
      "Epoch 48: val_loss improved from 7522.26709 to 7486.35693, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7299.0376 - mae: 7299.0376 - val_loss: 7486.3569 - val_mae: 7486.3569\n",
      "Epoch 49/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 5293.2393 - mae: 5293.2393\n",
      "Epoch 49: val_loss improved from 7486.35693 to 7448.97900, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7262.4585 - mae: 7262.4585 - val_loss: 7448.9790 - val_mae: 7448.9790\n",
      "Epoch 50/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 6293.9590 - mae: 6293.9590\n",
      "Epoch 50: val_loss improved from 7448.97900 to 7411.89160, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7223.9106 - mae: 7223.9106 - val_loss: 7411.8916 - val_mae: 7411.8916\n",
      "Epoch 51/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 10318.3506 - mae: 10318.3506\n",
      "Epoch 51: val_loss improved from 7411.89160 to 7371.87891, saving model to data\\best_model.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 2ms/step - loss: 7185.4365 - mae: 7185.4365 - val_loss: 7371.8789 - val_mae: 7371.8789\n",
      "Epoch 52/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 4761.7236 - mae: 4761.7236\n",
      "Epoch 52: val_loss improved from 7371.87891 to 7332.00977, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7147.2495 - mae: 7147.2495 - val_loss: 7332.0098 - val_mae: 7332.0098\n",
      "Epoch 53/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 9750.3291 - mae: 9750.3291\n",
      "Epoch 53: val_loss improved from 7332.00977 to 7289.46777, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7104.5820 - mae: 7104.5820 - val_loss: 7289.4678 - val_mae: 7289.4678\n",
      "Epoch 54/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 7722.7363 - mae: 7722.7363\n",
      "Epoch 54: val_loss improved from 7289.46777 to 7247.73975, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7061.9697 - mae: 7061.9697 - val_loss: 7247.7397 - val_mae: 7247.7397\n",
      "Epoch 55/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 6923.1753 - mae: 6923.1753\n",
      "Epoch 55: val_loss improved from 7247.73975 to 7204.62598, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7019.1289 - mae: 7019.1289 - val_loss: 7204.6260 - val_mae: 7204.6260\n",
      "Epoch 56/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 8722.1611 - mae: 8722.1611\n",
      "Epoch 56: val_loss improved from 7204.62598 to 7160.23877, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 6974.7661 - mae: 6974.7661 - val_loss: 7160.2388 - val_mae: 7160.2388\n",
      "Epoch 57/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 6186.4160 - mae: 6186.4160\n",
      "Epoch 57: val_loss improved from 7160.23877 to 7113.16992, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 6929.2686 - mae: 6929.2686 - val_loss: 7113.1699 - val_mae: 7113.1699\n",
      "Epoch 58/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 8298.5195 - mae: 8298.5195\n",
      "Epoch 58: val_loss improved from 7113.16992 to 7066.58447, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 6881.8320 - mae: 6881.8320 - val_loss: 7066.5845 - val_mae: 7066.5845\n",
      "Epoch 59/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 6756.7217 - mae: 6756.7217\n",
      "Epoch 59: val_loss improved from 7066.58447 to 7016.39111, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 6834.7222 - mae: 6834.7222 - val_loss: 7016.3911 - val_mae: 7016.3911\n",
      "Epoch 60/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 8070.4248 - mae: 8070.4248\n",
      "Epoch 60: val_loss improved from 7016.39111 to 6965.07324, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 6782.8936 - mae: 6782.8936 - val_loss: 6965.0732 - val_mae: 6965.0732\n",
      "Epoch 61/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 6495.7144 - mae: 6495.7144\n",
      "Epoch 61: val_loss improved from 6965.07324 to 6913.08643, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 6731.2339 - mae: 6731.2339 - val_loss: 6913.0864 - val_mae: 6913.0864\n",
      "Epoch 62/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 5359.6885 - mae: 5359.6885\n",
      "Epoch 62: val_loss improved from 6913.08643 to 6858.84912, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 6677.2285 - mae: 6677.2285 - val_loss: 6858.8491 - val_mae: 6858.8491\n",
      "Epoch 63/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 5795.2915 - mae: 5795.2915\n",
      "Epoch 63: val_loss improved from 6858.84912 to 6801.75537, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 6621.1724 - mae: 6621.1724 - val_loss: 6801.7554 - val_mae: 6801.7554\n",
      "Epoch 64/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 5230.9214 - mae: 5230.9214\n",
      "Epoch 64: val_loss improved from 6801.75537 to 6742.42188, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 6563.5649 - mae: 6563.5649 - val_loss: 6742.4219 - val_mae: 6742.4219\n",
      "Epoch 65/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 5931.9375 - mae: 5931.9375\n",
      "Epoch 65: val_loss improved from 6742.42188 to 6680.91846, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 6501.9209 - mae: 6501.9209 - val_loss: 6680.9185 - val_mae: 6680.9185\n",
      "Epoch 66/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 6205.5234 - mae: 6205.5234\n",
      "Epoch 66: val_loss improved from 6680.91846 to 6619.21191, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 6441.1055 - mae: 6441.1055 - val_loss: 6619.2119 - val_mae: 6619.2119\n",
      "Epoch 67/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 5537.8789 - mae: 5537.8789\n",
      "Epoch 67: val_loss improved from 6619.21191 to 6553.32910, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 6375.8882 - mae: 6375.8882 - val_loss: 6553.3291 - val_mae: 6553.3291\n",
      "Epoch 68/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 4653.6099 - mae: 4653.6099\n",
      "Epoch 68: val_loss improved from 6553.32910 to 6485.13867, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 6310.2925 - mae: 6310.2925 - val_loss: 6485.1387 - val_mae: 6485.1387\n",
      "Epoch 69/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 5287.0742 - mae: 5287.0742\n",
      "Epoch 69: val_loss improved from 6485.13867 to 6414.53564, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 6242.6802 - mae: 6242.6802 - val_loss: 6414.5356 - val_mae: 6414.5356\n",
      "Epoch 70/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 5844.9214 - mae: 5844.9214\n",
      "Epoch 70: val_loss improved from 6414.53564 to 6339.64111, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 6171.1323 - mae: 6171.1323 - val_loss: 6339.6411 - val_mae: 6339.6411\n",
      "Epoch 71/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 6508.0669 - mae: 6508.0669\n",
      "Epoch 71: val_loss improved from 6339.64111 to 6262.90918, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 6096.1836 - mae: 6096.1836 - val_loss: 6262.9092 - val_mae: 6262.9092\n",
      "Epoch 72/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 4917.9824 - mae: 4917.9824\n",
      "Epoch 72: val_loss improved from 6262.90918 to 6183.72217, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 6018.0337 - mae: 6018.0337 - val_loss: 6183.7222 - val_mae: 6183.7222\n",
      "Epoch 73/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3494.1289 - mae: 3494.1289\n",
      "Epoch 73: val_loss improved from 6183.72217 to 6100.43262, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 5938.0693 - mae: 5938.0693 - val_loss: 6100.4326 - val_mae: 6100.4326\n",
      "Epoch 74/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 4799.1182 - mae: 4799.1182\n",
      "Epoch 74: val_loss improved from 6100.43262 to 6014.83447, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 5852.8647 - mae: 5852.8647 - val_loss: 6014.8345 - val_mae: 6014.8345\n",
      "Epoch 75/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 4259.8086 - mae: 4259.8086\n",
      "Epoch 75: val_loss improved from 6014.83447 to 5926.99902, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 5766.6123 - mae: 5766.6123 - val_loss: 5926.9990 - val_mae: 5926.9990\n",
      "Epoch 76/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 6515.5166 - mae: 6515.5166\n",
      "Epoch 76: val_loss improved from 5926.99902 to 5836.67432, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 5676.0010 - mae: 5676.0010 - val_loss: 5836.6743 - val_mae: 5836.6743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 7367.8921 - mae: 7367.8921\n",
      "Epoch 77: val_loss improved from 5836.67432 to 5740.97852, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 5583.4590 - mae: 5583.4590 - val_loss: 5740.9785 - val_mae: 5740.9785\n",
      "Epoch 78/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 4959.2158 - mae: 4959.2158\n",
      "Epoch 78: val_loss improved from 5740.97852 to 5640.65234, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 5489.9326 - mae: 5489.9326 - val_loss: 5640.6523 - val_mae: 5640.6523\n",
      "Epoch 79/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 5988.8066 - mae: 5988.8066\n",
      "Epoch 79: val_loss improved from 5640.65234 to 5540.98340, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 5392.8208 - mae: 5392.8208 - val_loss: 5540.9834 - val_mae: 5540.9834\n",
      "Epoch 80/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 6073.0967 - mae: 6073.0967\n",
      "Epoch 80: val_loss improved from 5540.98340 to 5433.56934, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 5298.1436 - mae: 5298.1436 - val_loss: 5433.5693 - val_mae: 5433.5693\n",
      "Epoch 81/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 5385.7188 - mae: 5385.7188\n",
      "Epoch 81: val_loss improved from 5433.56934 to 5325.60205, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 5195.7759 - mae: 5195.7759 - val_loss: 5325.6021 - val_mae: 5325.6021\n",
      "Epoch 82/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 4714.0244 - mae: 4714.0244\n",
      "Epoch 82: val_loss improved from 5325.60205 to 5221.13232, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 5093.7314 - mae: 5093.7314 - val_loss: 5221.1323 - val_mae: 5221.1323\n",
      "Epoch 83/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 5308.0806 - mae: 5308.0806\n",
      "Epoch 83: val_loss improved from 5221.13232 to 5109.60205, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 4991.0483 - mae: 4991.0483 - val_loss: 5109.6021 - val_mae: 5109.6021\n",
      "Epoch 84/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 5168.1025 - mae: 5168.1025\n",
      "Epoch 84: val_loss improved from 5109.60205 to 4990.95508, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 4885.0942 - mae: 4885.0942 - val_loss: 4990.9551 - val_mae: 4990.9551\n",
      "Epoch 85/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 4476.1104 - mae: 4476.1104\n",
      "Epoch 85: val_loss improved from 4990.95508 to 4876.40283, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 4780.4795 - mae: 4780.4795 - val_loss: 4876.4028 - val_mae: 4876.4028\n",
      "Epoch 86/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 6594.8110 - mae: 6594.8110\n",
      "Epoch 86: val_loss improved from 4876.40283 to 4766.82178, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 4675.7344 - mae: 4675.7344 - val_loss: 4766.8218 - val_mae: 4766.8218\n",
      "Epoch 87/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3942.5508 - mae: 3942.5508\n",
      "Epoch 87: val_loss improved from 4766.82178 to 4660.85791, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 4576.3838 - mae: 4576.3838 - val_loss: 4660.8579 - val_mae: 4660.8579\n",
      "Epoch 88/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 5373.5654 - mae: 5373.5654\n",
      "Epoch 88: val_loss improved from 4660.85791 to 4554.96484, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 4473.4692 - mae: 4473.4692 - val_loss: 4554.9648 - val_mae: 4554.9648\n",
      "Epoch 89/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 5461.4160 - mae: 5461.4160\n",
      "Epoch 89: val_loss improved from 4554.96484 to 4458.68262, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 4378.0767 - mae: 4378.0767 - val_loss: 4458.6826 - val_mae: 4458.6826\n",
      "Epoch 90/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 4648.1602 - mae: 4648.1602\n",
      "Epoch 90: val_loss improved from 4458.68262 to 4367.62109, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 4291.1318 - mae: 4291.1318 - val_loss: 4367.6211 - val_mae: 4367.6211\n",
      "Epoch 91/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3930.7439 - mae: 3930.7439\n",
      "Epoch 91: val_loss improved from 4367.62109 to 4281.70947, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 4207.4268 - mae: 4207.4268 - val_loss: 4281.7095 - val_mae: 4281.7095\n",
      "Epoch 92/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 4303.6123 - mae: 4303.6123\n",
      "Epoch 92: val_loss improved from 4281.70947 to 4202.97949, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 4128.6543 - mae: 4128.6543 - val_loss: 4202.9795 - val_mae: 4202.9795\n",
      "Epoch 93/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 5534.8047 - mae: 5534.8047\n",
      "Epoch 93: val_loss improved from 4202.97949 to 4127.48145, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 4058.0383 - mae: 4058.0383 - val_loss: 4127.4814 - val_mae: 4127.4814\n",
      "Epoch 94/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3560.6313 - mae: 3560.6313\n",
      "Epoch 94: val_loss improved from 4127.48145 to 4062.13989, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3990.9968 - mae: 3990.9968 - val_loss: 4062.1399 - val_mae: 4062.1399\n",
      "Epoch 95/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3199.0332 - mae: 3199.0332\n",
      "Epoch 95: val_loss improved from 4062.13989 to 3998.93921, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3933.3447 - mae: 3933.3447 - val_loss: 3998.9392 - val_mae: 3998.9392\n",
      "Epoch 96/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 4963.1064 - mae: 4963.1064\n",
      "Epoch 96: val_loss improved from 3998.93921 to 3941.24731, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3879.0627 - mae: 3879.0627 - val_loss: 3941.2473 - val_mae: 3941.2473\n",
      "Epoch 97/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 2290.2905 - mae: 2290.2905\n",
      "Epoch 97: val_loss improved from 3941.24731 to 3890.24951, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3829.8464 - mae: 3829.8464 - val_loss: 3890.2495 - val_mae: 3890.2495\n",
      "Epoch 98/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 2849.9692 - mae: 2849.9692\n",
      "Epoch 98: val_loss improved from 3890.24951 to 3840.06860, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3785.1685 - mae: 3785.1685 - val_loss: 3840.0686 - val_mae: 3840.0686\n",
      "Epoch 99/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3799.0264 - mae: 3799.0264\n",
      "Epoch 99: val_loss improved from 3840.06860 to 3800.54932, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3746.5481 - mae: 3746.5481 - val_loss: 3800.5493 - val_mae: 3800.5493\n",
      "Epoch 100/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3247.5430 - mae: 3247.5430\n",
      "Epoch 100: val_loss improved from 3800.54932 to 3768.47656, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 3714.0850 - mae: 3714.0850 - val_loss: 3768.4766 - val_mae: 3768.4766\n",
      "Epoch 101/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3887.3193 - mae: 3887.3193\n",
      "Epoch 101: val_loss improved from 3768.47656 to 3739.81787, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3689.1680 - mae: 3689.1680 - val_loss: 3739.8179 - val_mae: 3739.8179\n",
      "Epoch 102/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 2863.1760 - mae: 2863.1760\n",
      "Epoch 102: val_loss improved from 3739.81787 to 3718.60815, saving model to data\\best_model.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 2ms/step - loss: 3666.2769 - mae: 3666.2769 - val_loss: 3718.6082 - val_mae: 3718.6082\n",
      "Epoch 103/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 2596.1250 - mae: 2596.1250\n",
      "Epoch 103: val_loss improved from 3718.60815 to 3699.08252, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 3648.7361 - mae: 3648.7361 - val_loss: 3699.0825 - val_mae: 3699.0825\n",
      "Epoch 104/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3887.7063 - mae: 3887.7063\n",
      "Epoch 104: val_loss improved from 3699.08252 to 3683.52808, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3632.7681 - mae: 3632.7681 - val_loss: 3683.5281 - val_mae: 3683.5281\n",
      "Epoch 105/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 2777.9402 - mae: 2777.9402\n",
      "Epoch 105: val_loss improved from 3683.52808 to 3669.20825, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3619.4771 - mae: 3619.4771 - val_loss: 3669.2083 - val_mae: 3669.2083\n",
      "Epoch 106/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 2948.5483 - mae: 2948.5483\n",
      "Epoch 106: val_loss improved from 3669.20825 to 3655.99878, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3609.5444 - mae: 3609.5444 - val_loss: 3655.9988 - val_mae: 3655.9988\n",
      "Epoch 107/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3987.4104 - mae: 3987.4104\n",
      "Epoch 107: val_loss improved from 3655.99878 to 3644.84961, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3598.5642 - mae: 3598.5642 - val_loss: 3644.8496 - val_mae: 3644.8496\n",
      "Epoch 108/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 2211.2046 - mae: 2211.2046\n",
      "Epoch 108: val_loss improved from 3644.84961 to 3632.43164, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3590.3923 - mae: 3590.3923 - val_loss: 3632.4316 - val_mae: 3632.4316\n",
      "Epoch 109/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3892.8098 - mae: 3892.8098\n",
      "Epoch 109: val_loss improved from 3632.43164 to 3621.38574, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3582.6345 - mae: 3582.6345 - val_loss: 3621.3857 - val_mae: 3621.3857\n",
      "Epoch 110/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 2586.5498 - mae: 2586.5498\n",
      "Epoch 110: val_loss improved from 3621.38574 to 3613.16943, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3575.4189 - mae: 3575.4189 - val_loss: 3613.1694 - val_mae: 3613.1694\n",
      "Epoch 111/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 2606.7065 - mae: 2606.7065\n",
      "Epoch 111: val_loss improved from 3613.16943 to 3601.68018, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3569.5928 - mae: 3569.5928 - val_loss: 3601.6802 - val_mae: 3601.6802\n",
      "Epoch 112/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 4691.7139 - mae: 4691.7139\n",
      "Epoch 112: val_loss improved from 3601.68018 to 3593.27979, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3563.2747 - mae: 3563.2747 - val_loss: 3593.2798 - val_mae: 3593.2798\n",
      "Epoch 113/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3312.9680 - mae: 3312.9680\n",
      "Epoch 113: val_loss improved from 3593.27979 to 3585.01147, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3558.9800 - mae: 3558.9800 - val_loss: 3585.0115 - val_mae: 3585.0115\n",
      "Epoch 114/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 2973.8315 - mae: 2973.8315\n",
      "Epoch 114: val_loss improved from 3585.01147 to 3574.85449, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3550.0229 - mae: 3550.0229 - val_loss: 3574.8545 - val_mae: 3574.8545\n",
      "Epoch 115/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 4695.3262 - mae: 4695.3262\n",
      "Epoch 115: val_loss improved from 3574.85449 to 3568.90454, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3543.0264 - mae: 3543.0264 - val_loss: 3568.9045 - val_mae: 3568.9045\n",
      "Epoch 116/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 2605.9116 - mae: 2605.9116\n",
      "Epoch 116: val_loss improved from 3568.90454 to 3560.48022, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3536.0835 - mae: 3536.0835 - val_loss: 3560.4802 - val_mae: 3560.4802\n",
      "Epoch 117/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 4688.4043 - mae: 4688.4043\n",
      "Epoch 117: val_loss improved from 3560.48022 to 3553.09302, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3530.9878 - mae: 3530.9878 - val_loss: 3553.0930 - val_mae: 3553.0930\n",
      "Epoch 118/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 4930.5620 - mae: 4930.5620\n",
      "Epoch 118: val_loss improved from 3553.09302 to 3546.06641, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3525.6111 - mae: 3525.6111 - val_loss: 3546.0664 - val_mae: 3546.0664\n",
      "Epoch 119/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 2644.7354 - mae: 2644.7354\n",
      "Epoch 119: val_loss improved from 3546.06641 to 3537.69995, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3520.2407 - mae: 3520.2407 - val_loss: 3537.7000 - val_mae: 3537.7000\n",
      "Epoch 120/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3732.1001 - mae: 3732.1001\n",
      "Epoch 120: val_loss improved from 3537.69995 to 3530.99805, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3516.1311 - mae: 3516.1311 - val_loss: 3530.9980 - val_mae: 3530.9980\n",
      "Epoch 121/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 4339.1040 - mae: 4339.1040\n",
      "Epoch 121: val_loss improved from 3530.99805 to 3521.94141, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3510.7036 - mae: 3510.7036 - val_loss: 3521.9414 - val_mae: 3521.9414\n",
      "Epoch 122/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3858.8879 - mae: 3858.8879\n",
      "Epoch 122: val_loss improved from 3521.94141 to 3514.61816, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3504.8425 - mae: 3504.8425 - val_loss: 3514.6182 - val_mae: 3514.6182\n",
      "Epoch 123/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 2117.4055 - mae: 2117.4055\n",
      "Epoch 123: val_loss improved from 3514.61816 to 3508.52393, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 3500.6768 - mae: 3500.6768 - val_loss: 3508.5239 - val_mae: 3508.5239\n",
      "Epoch 124/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 2898.4229 - mae: 2898.4229\n",
      "Epoch 124: val_loss improved from 3508.52393 to 3499.77246, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3496.0344 - mae: 3496.0344 - val_loss: 3499.7725 - val_mae: 3499.7725\n",
      "Epoch 125/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3788.0837 - mae: 3788.0837\n",
      "Epoch 125: val_loss improved from 3499.77246 to 3492.33057, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3491.5972 - mae: 3491.5972 - val_loss: 3492.3306 - val_mae: 3492.3306\n",
      "Epoch 126/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 4977.6289 - mae: 4977.6289\n",
      "Epoch 126: val_loss improved from 3492.33057 to 3485.12573, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3487.4216 - mae: 3487.4216 - val_loss: 3485.1257 - val_mae: 3485.1257\n",
      "Epoch 127/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 1986.7283 - mae: 1986.7283\n",
      "Epoch 127: val_loss improved from 3485.12573 to 3476.18628, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 3483.2881 - mae: 3483.2881 - val_loss: 3476.1863 - val_mae: 3476.1863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 128/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3836.9480 - mae: 3836.9480\n",
      "Epoch 128: val_loss improved from 3476.18628 to 3466.83667, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3478.7258 - mae: 3478.7258 - val_loss: 3466.8367 - val_mae: 3466.8367\n",
      "Epoch 129/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 4629.2124 - mae: 4629.2124\n",
      "Epoch 129: val_loss improved from 3466.83667 to 3461.88916, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3471.4546 - mae: 3471.4546 - val_loss: 3461.8892 - val_mae: 3461.8892\n",
      "Epoch 130/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3327.0254 - mae: 3327.0254\n",
      "Epoch 130: val_loss improved from 3461.88916 to 3453.46606, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3466.7146 - mae: 3466.7146 - val_loss: 3453.4661 - val_mae: 3453.4661\n",
      "Epoch 131/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3071.5620 - mae: 3071.5620\n",
      "Epoch 131: val_loss improved from 3453.46606 to 3444.36108, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3463.0935 - mae: 3463.0935 - val_loss: 3444.3611 - val_mae: 3444.3611\n",
      "Epoch 132/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 2891.9878 - mae: 2891.9878\n",
      "Epoch 132: val_loss improved from 3444.36108 to 3436.29932, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3457.2031 - mae: 3457.2031 - val_loss: 3436.2993 - val_mae: 3436.2993\n",
      "Epoch 133/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 4040.3650 - mae: 4040.3650\n",
      "Epoch 133: val_loss improved from 3436.29932 to 3431.34131, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3453.6780 - mae: 3453.6780 - val_loss: 3431.3413 - val_mae: 3431.3413\n",
      "Epoch 134/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3993.5791 - mae: 3993.5791\n",
      "Epoch 134: val_loss improved from 3431.34131 to 3421.03906, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3448.6555 - mae: 3448.6555 - val_loss: 3421.0391 - val_mae: 3421.0391\n",
      "Epoch 135/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 1959.5457 - mae: 1959.5457\n",
      "Epoch 135: val_loss improved from 3421.03906 to 3411.00220, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3443.9214 - mae: 3443.9214 - val_loss: 3411.0022 - val_mae: 3411.0022\n",
      "Epoch 136/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 4179.2314 - mae: 4179.2314\n",
      "Epoch 136: val_loss improved from 3411.00220 to 3401.50635, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3437.8076 - mae: 3437.8076 - val_loss: 3401.5063 - val_mae: 3401.5063\n",
      "Epoch 137/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3072.0520 - mae: 3072.0520\n",
      "Epoch 137: val_loss improved from 3401.50635 to 3391.87915, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3431.0942 - mae: 3431.0942 - val_loss: 3391.8792 - val_mae: 3391.8792\n",
      "Epoch 138/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3153.0142 - mae: 3153.0142\n",
      "Epoch 138: val_loss improved from 3391.87915 to 3384.62500, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 3427.2095 - mae: 3427.2095 - val_loss: 3384.6250 - val_mae: 3384.6250\n",
      "Epoch 139/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3356.3813 - mae: 3356.3813\n",
      "Epoch 139: val_loss improved from 3384.62500 to 3375.36938, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3420.7737 - mae: 3420.7737 - val_loss: 3375.3694 - val_mae: 3375.3694\n",
      "Epoch 140/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 2869.8125 - mae: 2869.8125\n",
      "Epoch 140: val_loss improved from 3375.36938 to 3364.35522, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3413.9297 - mae: 3413.9297 - val_loss: 3364.3552 - val_mae: 3364.3552\n",
      "Epoch 141/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 2344.4644 - mae: 2344.4644\n",
      "Epoch 141: val_loss improved from 3364.35522 to 3356.97803, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3408.9819 - mae: 3408.9819 - val_loss: 3356.9780 - val_mae: 3356.9780\n",
      "Epoch 142/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 5024.0918 - mae: 5024.0918\n",
      "Epoch 142: val_loss improved from 3356.97803 to 3347.06348, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3403.4180 - mae: 3403.4180 - val_loss: 3347.0635 - val_mae: 3347.0635\n",
      "Epoch 143/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 2969.2783 - mae: 2969.2783\n",
      "Epoch 143: val_loss improved from 3347.06348 to 3339.34106, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 3396.9148 - mae: 3396.9148 - val_loss: 3339.3411 - val_mae: 3339.3411\n",
      "Epoch 144/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3390.1736 - mae: 3390.1736\n",
      "Epoch 144: val_loss improved from 3339.34106 to 3333.31982, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3392.0229 - mae: 3392.0229 - val_loss: 3333.3198 - val_mae: 3333.3198\n",
      "Epoch 145/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3860.4395 - mae: 3860.4395\n",
      "Epoch 145: val_loss improved from 3333.31982 to 3318.97900, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3386.0571 - mae: 3386.0571 - val_loss: 3318.9790 - val_mae: 3318.9790\n",
      "Epoch 146/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 5278.3818 - mae: 5278.3818\n",
      "Epoch 146: val_loss improved from 3318.97900 to 3311.98462, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3381.4968 - mae: 3381.4968 - val_loss: 3311.9846 - val_mae: 3311.9846\n",
      "Epoch 147/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 4750.6895 - mae: 4750.6895\n",
      "Epoch 147: val_loss improved from 3311.98462 to 3302.68384, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3374.2278 - mae: 3374.2278 - val_loss: 3302.6838 - val_mae: 3302.6838\n",
      "Epoch 148/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 5521.0977 - mae: 5521.0977\n",
      "Epoch 148: val_loss improved from 3302.68384 to 3293.64258, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3368.7773 - mae: 3368.7773 - val_loss: 3293.6426 - val_mae: 3293.6426\n",
      "Epoch 149/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 5180.9189 - mae: 5180.9189\n",
      "Epoch 149: val_loss improved from 3293.64258 to 3282.44507, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3364.2668 - mae: 3364.2668 - val_loss: 3282.4451 - val_mae: 3282.4451\n",
      "Epoch 150/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 2983.7627 - mae: 2983.7627\n",
      "Epoch 150: val_loss improved from 3282.44507 to 3275.19189, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3358.5518 - mae: 3358.5518 - val_loss: 3275.1919 - val_mae: 3275.1919\n",
      "Epoch 151/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 4107.4639 - mae: 4107.4639\n",
      "Epoch 151: val_loss improved from 3275.19189 to 3264.16333, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3354.4536 - mae: 3354.4536 - val_loss: 3264.1633 - val_mae: 3264.1633\n",
      "Epoch 152/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 2720.6760 - mae: 2720.6760\n",
      "Epoch 152: val_loss improved from 3264.16333 to 3258.35107, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3347.0752 - mae: 3347.0752 - val_loss: 3258.3511 - val_mae: 3258.3511\n",
      "Epoch 153/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/32 [..............................] - ETA: 0s - loss: 3283.6118 - mae: 3283.6118\n",
      "Epoch 153: val_loss improved from 3258.35107 to 3251.67236, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3342.8193 - mae: 3342.8193 - val_loss: 3251.6724 - val_mae: 3251.6724\n",
      "Epoch 154/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 1628.6512 - mae: 1628.6512\n",
      "Epoch 154: val_loss improved from 3251.67236 to 3243.57275, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3337.9983 - mae: 3337.9983 - val_loss: 3243.5728 - val_mae: 3243.5728\n",
      "Epoch 155/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3838.8887 - mae: 3838.8887\n",
      "Epoch 155: val_loss improved from 3243.57275 to 3236.40674, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3332.0981 - mae: 3332.0981 - val_loss: 3236.4067 - val_mae: 3236.4067\n",
      "Epoch 156/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3279.9277 - mae: 3279.9277\n",
      "Epoch 156: val_loss improved from 3236.40674 to 3228.46973, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 3327.5862 - mae: 3327.5862 - val_loss: 3228.4697 - val_mae: 3228.4697\n",
      "Epoch 157/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 1890.4746 - mae: 1890.4746\n",
      "Epoch 157: val_loss improved from 3228.46973 to 3222.08838, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 3323.1172 - mae: 3323.1172 - val_loss: 3222.0884 - val_mae: 3222.0884\n",
      "Epoch 158/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3238.8931 - mae: 3238.8931\n",
      "Epoch 158: val_loss improved from 3222.08838 to 3213.10181, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 3318.7629 - mae: 3318.7629 - val_loss: 3213.1018 - val_mae: 3213.1018\n",
      "Epoch 159/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 2680.7119 - mae: 2680.7119\n",
      "Epoch 159: val_loss improved from 3213.10181 to 3208.68481, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 3313.8406 - mae: 3313.8406 - val_loss: 3208.6848 - val_mae: 3208.6848\n",
      "Epoch 160/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 4023.4397 - mae: 4023.4397\n",
      "Epoch 160: val_loss improved from 3208.68481 to 3196.68433, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3309.7693 - mae: 3309.7693 - val_loss: 3196.6843 - val_mae: 3196.6843\n",
      "Epoch 161/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3181.0610 - mae: 3181.0610\n",
      "Epoch 161: val_loss improved from 3196.68433 to 3190.38135, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3305.3267 - mae: 3305.3267 - val_loss: 3190.3813 - val_mae: 3190.3813\n",
      "Epoch 162/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 2040.4714 - mae: 2040.4714\n",
      "Epoch 162: val_loss improved from 3190.38135 to 3186.85669, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3300.8943 - mae: 3300.8943 - val_loss: 3186.8567 - val_mae: 3186.8567\n",
      "Epoch 163/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3330.0662 - mae: 3330.0662\n",
      "Epoch 163: val_loss improved from 3186.85669 to 3176.41870, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3299.0879 - mae: 3299.0879 - val_loss: 3176.4187 - val_mae: 3176.4187\n",
      "Epoch 164/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 4154.3501 - mae: 4154.3501\n",
      "Epoch 164: val_loss improved from 3176.41870 to 3169.92798, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3293.2104 - mae: 3293.2104 - val_loss: 3169.9280 - val_mae: 3169.9280\n",
      "Epoch 165/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3324.7686 - mae: 3324.7686\n",
      "Epoch 165: val_loss improved from 3169.92798 to 3165.02393, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 3288.9666 - mae: 3288.9666 - val_loss: 3165.0239 - val_mae: 3165.0239\n",
      "Epoch 166/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 5149.1382 - mae: 5149.1382\n",
      "Epoch 166: val_loss improved from 3165.02393 to 3156.82080, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3285.3687 - mae: 3285.3687 - val_loss: 3156.8208 - val_mae: 3156.8208\n",
      "Epoch 167/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3566.9448 - mae: 3566.9448\n",
      "Epoch 167: val_loss improved from 3156.82080 to 3150.25977, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3281.8564 - mae: 3281.8564 - val_loss: 3150.2598 - val_mae: 3150.2598\n",
      "Epoch 168/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 4023.5930 - mae: 4023.5930\n",
      "Epoch 168: val_loss improved from 3150.25977 to 3141.27466, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3279.6453 - mae: 3279.6453 - val_loss: 3141.2747 - val_mae: 3141.2747\n",
      "Epoch 169/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 4774.6763 - mae: 4774.6763\n",
      "Epoch 169: val_loss improved from 3141.27466 to 3137.71094, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3274.0957 - mae: 3274.0957 - val_loss: 3137.7109 - val_mae: 3137.7109\n",
      "Epoch 170/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3302.1836 - mae: 3302.1836\n",
      "Epoch 170: val_loss improved from 3137.71094 to 3130.55811, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 3271.8088 - mae: 3271.8088 - val_loss: 3130.5581 - val_mae: 3130.5581\n",
      "Epoch 171/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 2689.4236 - mae: 2689.4236\n",
      "Epoch 171: val_loss improved from 3130.55811 to 3123.44336, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3267.0779 - mae: 3267.0779 - val_loss: 3123.4434 - val_mae: 3123.4434\n",
      "Epoch 172/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3565.3550 - mae: 3565.3550\n",
      "Epoch 172: val_loss improved from 3123.44336 to 3117.92358, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3263.6753 - mae: 3263.6753 - val_loss: 3117.9236 - val_mae: 3117.9236\n",
      "Epoch 173/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 2593.6880 - mae: 2593.6880\n",
      "Epoch 173: val_loss improved from 3117.92358 to 3111.72339, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3260.8027 - mae: 3260.8027 - val_loss: 3111.7234 - val_mae: 3111.7234\n",
      "Epoch 174/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3895.6565 - mae: 3895.6565\n",
      "Epoch 174: val_loss improved from 3111.72339 to 3104.74194, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 3257.9636 - mae: 3257.9636 - val_loss: 3104.7419 - val_mae: 3104.7419\n",
      "Epoch 175/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 5309.2349 - mae: 5309.2349\n",
      "Epoch 175: val_loss improved from 3104.74194 to 3099.26880, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3254.9075 - mae: 3254.9075 - val_loss: 3099.2688 - val_mae: 3099.2688\n",
      "Epoch 176/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3238.6860 - mae: 3238.6860\n",
      "Epoch 176: val_loss improved from 3099.26880 to 3094.22461, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3251.6909 - mae: 3251.6909 - val_loss: 3094.2246 - val_mae: 3094.2246\n",
      "Epoch 177/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 2400.4104 - mae: 2400.4104\n",
      "Epoch 177: val_loss improved from 3094.22461 to 3088.69702, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3249.1089 - mae: 3249.1089 - val_loss: 3088.6970 - val_mae: 3088.6970\n",
      "Epoch 178/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/32 [..............................] - ETA: 0s - loss: 2818.9724 - mae: 2818.9724\n",
      "Epoch 178: val_loss improved from 3088.69702 to 3081.95728, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3247.7407 - mae: 3247.7407 - val_loss: 3081.9573 - val_mae: 3081.9573\n",
      "Epoch 179/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3456.0991 - mae: 3456.0991\n",
      "Epoch 179: val_loss improved from 3081.95728 to 3076.81982, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3243.5723 - mae: 3243.5723 - val_loss: 3076.8198 - val_mae: 3076.8198\n",
      "Epoch 180/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3343.5728 - mae: 3343.5728\n",
      "Epoch 180: val_loss improved from 3076.81982 to 3075.78101, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3241.5732 - mae: 3241.5732 - val_loss: 3075.7810 - val_mae: 3075.7810\n",
      "Epoch 181/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3265.6663 - mae: 3265.6663\n",
      "Epoch 181: val_loss improved from 3075.78101 to 3072.77954, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3241.2751 - mae: 3241.2751 - val_loss: 3072.7795 - val_mae: 3072.7795\n",
      "Epoch 182/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3316.7412 - mae: 3316.7412\n",
      "Epoch 182: val_loss improved from 3072.77954 to 3069.00269, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3240.8311 - mae: 3240.8311 - val_loss: 3069.0027 - val_mae: 3069.0027\n",
      "Epoch 183/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3271.0000 - mae: 3271.0000\n",
      "Epoch 183: val_loss improved from 3069.00269 to 3067.16626, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3237.2891 - mae: 3237.2891 - val_loss: 3067.1663 - val_mae: 3067.1663\n",
      "Epoch 184/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 4231.7422 - mae: 4231.7422\n",
      "Epoch 184: val_loss improved from 3067.16626 to 3056.72803, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3234.2415 - mae: 3234.2415 - val_loss: 3056.7280 - val_mae: 3056.7280\n",
      "Epoch 185/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 4872.9912 - mae: 4872.9912\n",
      "Epoch 185: val_loss improved from 3056.72803 to 3053.27734, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3231.3877 - mae: 3231.3877 - val_loss: 3053.2773 - val_mae: 3053.2773\n",
      "Epoch 186/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3200.8237 - mae: 3200.8237\n",
      "Epoch 186: val_loss improved from 3053.27734 to 3051.14404, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3230.3518 - mae: 3230.3518 - val_loss: 3051.1440 - val_mae: 3051.1440\n",
      "Epoch 187/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3580.7981 - mae: 3580.7981\n",
      "Epoch 187: val_loss improved from 3051.14404 to 3044.46118, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3227.0947 - mae: 3227.0947 - val_loss: 3044.4612 - val_mae: 3044.4612\n",
      "Epoch 188/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 2914.9717 - mae: 2914.9717\n",
      "Epoch 188: val_loss improved from 3044.46118 to 3042.73608, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3224.6907 - mae: 3224.6907 - val_loss: 3042.7361 - val_mae: 3042.7361\n",
      "Epoch 189/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3165.6367 - mae: 3165.6367\n",
      "Epoch 189: val_loss improved from 3042.73608 to 3038.03027, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3224.1267 - mae: 3224.1267 - val_loss: 3038.0303 - val_mae: 3038.0303\n",
      "Epoch 190/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 4647.8037 - mae: 4647.8037\n",
      "Epoch 190: val_loss improved from 3038.03027 to 3036.30811, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3222.9519 - mae: 3222.9519 - val_loss: 3036.3081 - val_mae: 3036.3081\n",
      "Epoch 191/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 5276.6543 - mae: 5276.6543\n",
      "Epoch 191: val_loss improved from 3036.30811 to 3033.76758, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 3224.6182 - mae: 3224.6182 - val_loss: 3033.7676 - val_mae: 3033.7676\n",
      "Epoch 192/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 1993.9554 - mae: 1993.9554\n",
      "Epoch 192: val_loss improved from 3033.76758 to 3027.01172, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3220.2158 - mae: 3220.2158 - val_loss: 3027.0117 - val_mae: 3027.0117\n",
      "Epoch 193/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3379.2520 - mae: 3379.2520\n",
      "Epoch 193: val_loss improved from 3027.01172 to 3025.09009, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3217.1021 - mae: 3217.1021 - val_loss: 3025.0901 - val_mae: 3025.0901\n",
      "Epoch 194/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3126.5535 - mae: 3126.5535\n",
      "Epoch 194: val_loss improved from 3025.09009 to 3022.34521, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3214.7278 - mae: 3214.7278 - val_loss: 3022.3452 - val_mae: 3022.3452\n",
      "Epoch 195/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 2992.5801 - mae: 2992.5801\n",
      "Epoch 195: val_loss improved from 3022.34521 to 3018.94751, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3213.8872 - mae: 3213.8872 - val_loss: 3018.9475 - val_mae: 3018.9475\n",
      "Epoch 196/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3316.0259 - mae: 3316.0259\n",
      "Epoch 196: val_loss did not improve from 3018.94751\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3211.8386 - mae: 3211.8386 - val_loss: 3019.1287 - val_mae: 3019.1287\n",
      "Epoch 197/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 5127.4917 - mae: 5127.4917\n",
      "Epoch 197: val_loss improved from 3018.94751 to 3012.52173, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3210.5139 - mae: 3210.5139 - val_loss: 3012.5217 - val_mae: 3012.5217\n",
      "Epoch 198/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 1938.8082 - mae: 1938.8082\n",
      "Epoch 198: val_loss improved from 3012.52173 to 3010.85010, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3208.6987 - mae: 3208.6987 - val_loss: 3010.8501 - val_mae: 3010.8501\n",
      "Epoch 199/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 4558.3052 - mae: 4558.3052\n",
      "Epoch 199: val_loss improved from 3010.85010 to 3006.29639, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3206.8132 - mae: 3206.8132 - val_loss: 3006.2964 - val_mae: 3006.2964\n",
      "Epoch 200/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3497.0579 - mae: 3497.0579\n",
      "Epoch 200: val_loss improved from 3006.29639 to 3003.40308, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3206.5784 - mae: 3206.5784 - val_loss: 3003.4031 - val_mae: 3003.4031\n",
      "Epoch 201/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 2186.7510 - mae: 2186.7510\n",
      "Epoch 201: val_loss improved from 3003.40308 to 2999.63232, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3204.0735 - mae: 3204.0735 - val_loss: 2999.6323 - val_mae: 2999.6323\n",
      "Epoch 202/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 4618.5303 - mae: 4618.5303\n",
      "Epoch 202: val_loss improved from 2999.63232 to 2996.19580, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 3203.9060 - mae: 3203.9060 - val_loss: 2996.1958 - val_mae: 2996.1958\n",
      "Epoch 203/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3117.5894 - mae: 3117.5894\n",
      "Epoch 203: val_loss improved from 2996.19580 to 2994.83887, saving model to data\\best_model.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 2ms/step - loss: 3201.3813 - mae: 3201.3813 - val_loss: 2994.8389 - val_mae: 2994.8389\n",
      "Epoch 204/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 2567.1460 - mae: 2567.1460\n",
      "Epoch 204: val_loss improved from 2994.83887 to 2993.76538, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3200.2751 - mae: 3200.2751 - val_loss: 2993.7654 - val_mae: 2993.7654\n",
      "Epoch 205/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 2586.6816 - mae: 2586.6816\n",
      "Epoch 205: val_loss improved from 2993.76538 to 2990.51440, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3199.6882 - mae: 3199.6882 - val_loss: 2990.5144 - val_mae: 2990.5144\n",
      "Epoch 206/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 4600.1870 - mae: 4600.1870\n",
      "Epoch 206: val_loss improved from 2990.51440 to 2986.74146, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3198.6443 - mae: 3198.6443 - val_loss: 2986.7415 - val_mae: 2986.7415\n",
      "Epoch 207/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 5781.0859 - mae: 5781.0859\n",
      "Epoch 207: val_loss improved from 2986.74146 to 2985.47974, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3197.1541 - mae: 3197.1541 - val_loss: 2985.4797 - val_mae: 2985.4797\n",
      "Epoch 208/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 1765.9327 - mae: 1765.9327\n",
      "Epoch 208: val_loss improved from 2985.47974 to 2984.39990, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3197.2842 - mae: 3197.2842 - val_loss: 2984.3999 - val_mae: 2984.3999\n",
      "Epoch 209/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 1915.0034 - mae: 1915.0034\n",
      "Epoch 209: val_loss improved from 2984.39990 to 2981.53223, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3198.0391 - mae: 3198.0391 - val_loss: 2981.5322 - val_mae: 2981.5322\n",
      "Epoch 210/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 1757.7742 - mae: 1757.7742\n",
      "Epoch 210: val_loss improved from 2981.53223 to 2980.90308, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3193.7913 - mae: 3193.7913 - val_loss: 2980.9031 - val_mae: 2980.9031\n",
      "Epoch 211/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3069.1091 - mae: 3069.1091\n",
      "Epoch 211: val_loss improved from 2980.90308 to 2975.31299, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3193.7654 - mae: 3193.7654 - val_loss: 2975.3130 - val_mae: 2975.3130\n",
      "Epoch 212/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3200.2554 - mae: 3200.2554\n",
      "Epoch 212: val_loss improved from 2975.31299 to 2971.65161, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3190.6628 - mae: 3190.6628 - val_loss: 2971.6516 - val_mae: 2971.6516\n",
      "Epoch 213/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 1536.4919 - mae: 1536.4919\n",
      "Epoch 213: val_loss improved from 2971.65161 to 2971.51807, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3189.5088 - mae: 3189.5088 - val_loss: 2971.5181 - val_mae: 2971.5181\n",
      "Epoch 214/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 4130.8877 - mae: 4130.8877\n",
      "Epoch 214: val_loss improved from 2971.51807 to 2968.33691, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3188.8438 - mae: 3188.8438 - val_loss: 2968.3369 - val_mae: 2968.3369\n",
      "Epoch 215/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3136.1807 - mae: 3136.1807\n",
      "Epoch 215: val_loss improved from 2968.33691 to 2965.51489, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3187.9414 - mae: 3187.9414 - val_loss: 2965.5149 - val_mae: 2965.5149\n",
      "Epoch 216/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3424.9856 - mae: 3424.9856\n",
      "Epoch 216: val_loss improved from 2965.51489 to 2961.05664, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3185.5515 - mae: 3185.5515 - val_loss: 2961.0566 - val_mae: 2961.0566\n",
      "Epoch 217/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3021.1411 - mae: 3021.1411\n",
      "Epoch 217: val_loss did not improve from 2961.05664\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3185.7175 - mae: 3185.7175 - val_loss: 2963.2058 - val_mae: 2963.2058\n",
      "Epoch 218/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 1714.6512 - mae: 1714.6512\n",
      "Epoch 218: val_loss improved from 2961.05664 to 2955.90649, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3186.3401 - mae: 3186.3401 - val_loss: 2955.9065 - val_mae: 2955.9065\n",
      "Epoch 219/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 2216.6013 - mae: 2216.6013\n",
      "Epoch 219: val_loss improved from 2955.90649 to 2955.56689, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3183.0029 - mae: 3183.0029 - val_loss: 2955.5669 - val_mae: 2955.5669\n",
      "Epoch 220/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 2914.8132 - mae: 2914.8132\n",
      "Epoch 220: val_loss improved from 2955.56689 to 2953.57471, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3183.3372 - mae: 3183.3372 - val_loss: 2953.5747 - val_mae: 2953.5747\n",
      "Epoch 221/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3764.4802 - mae: 3764.4802\n",
      "Epoch 221: val_loss did not improve from 2953.57471\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3182.2554 - mae: 3182.2554 - val_loss: 2954.5701 - val_mae: 2954.5701\n",
      "Epoch 222/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3432.0474 - mae: 3432.0474\n",
      "Epoch 222: val_loss did not improve from 2953.57471\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3182.7786 - mae: 3182.7786 - val_loss: 2953.7036 - val_mae: 2953.7036\n",
      "Epoch 223/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 2117.5671 - mae: 2117.5671\n",
      "Epoch 223: val_loss improved from 2953.57471 to 2952.71899, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3181.6365 - mae: 3181.6365 - val_loss: 2952.7190 - val_mae: 2952.7190\n",
      "Epoch 224/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 4399.9941 - mae: 4399.9941\n",
      "Epoch 224: val_loss improved from 2952.71899 to 2951.06885, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3181.3318 - mae: 3181.3318 - val_loss: 2951.0688 - val_mae: 2951.0688\n",
      "Epoch 225/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 2967.7976 - mae: 2967.7976\n",
      "Epoch 225: val_loss did not improve from 2951.06885\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3181.7129 - mae: 3181.7129 - val_loss: 2951.8508 - val_mae: 2951.8508\n",
      "Epoch 226/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 2324.1201 - mae: 2324.1201\n",
      "Epoch 226: val_loss did not improve from 2951.06885\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3182.2659 - mae: 3182.2659 - val_loss: 2951.3940 - val_mae: 2951.3940\n",
      "Epoch 227/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3910.2651 - mae: 3910.2651\n",
      "Epoch 227: val_loss did not improve from 2951.06885\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3180.9341 - mae: 3180.9341 - val_loss: 2951.4663 - val_mae: 2951.4663\n",
      "Epoch 228/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3850.0027 - mae: 3850.0027\n",
      "Epoch 228: val_loss did not improve from 2951.06885\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3179.9758 - mae: 3179.9758 - val_loss: 2951.5542 - val_mae: 2951.5542\n",
      "Epoch 229/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 2525.3711 - mae: 2525.3711\n",
      "Epoch 229: val_loss improved from 2951.06885 to 2950.25586, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3181.9487 - mae: 3181.9487 - val_loss: 2950.2559 - val_mae: 2950.2559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 230/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 4670.3208 - mae: 4670.3208\n",
      "Epoch 230: val_loss did not improve from 2950.25586\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3180.2861 - mae: 3180.2861 - val_loss: 2950.7449 - val_mae: 2950.7449\n",
      "Epoch 231/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3456.9058 - mae: 3456.9058\n",
      "Epoch 231: val_loss improved from 2950.25586 to 2949.93628, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3179.5374 - mae: 3179.5374 - val_loss: 2949.9363 - val_mae: 2949.9363\n",
      "Epoch 232/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 2464.7827 - mae: 2464.7827\n",
      "Epoch 232: val_loss improved from 2949.93628 to 2949.86914, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3179.5593 - mae: 3179.5593 - val_loss: 2949.8691 - val_mae: 2949.8691\n",
      "Epoch 233/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 2072.9307 - mae: 2072.9307\n",
      "Epoch 233: val_loss did not improve from 2949.86914\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3178.9670 - mae: 3178.9670 - val_loss: 2950.3933 - val_mae: 2950.3933\n",
      "Epoch 234/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 4523.9238 - mae: 4523.9238\n",
      "Epoch 234: val_loss did not improve from 2949.86914\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3179.4561 - mae: 3179.4561 - val_loss: 2950.0620 - val_mae: 2950.0620\n",
      "Epoch 235/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 4742.4644 - mae: 4742.4644\n",
      "Epoch 235: val_loss improved from 2949.86914 to 2949.42017, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3179.1895 - mae: 3179.1895 - val_loss: 2949.4202 - val_mae: 2949.4202\n",
      "Epoch 236/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 4449.9116 - mae: 4449.9116\n",
      "Epoch 236: val_loss did not improve from 2949.42017\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3178.4321 - mae: 3178.4321 - val_loss: 2949.7681 - val_mae: 2949.7681\n",
      "Epoch 237/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 4474.8462 - mae: 4474.8462\n",
      "Epoch 237: val_loss did not improve from 2949.42017\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3178.2634 - mae: 3178.2634 - val_loss: 2950.8782 - val_mae: 2950.8782\n",
      "Epoch 238/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 1622.5918 - mae: 1622.5918\n",
      "Epoch 238: val_loss did not improve from 2949.42017\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3179.0210 - mae: 3179.0210 - val_loss: 2950.2456 - val_mae: 2950.2456\n",
      "Epoch 239/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3695.1011 - mae: 3695.1011\n",
      "Epoch 239: val_loss improved from 2949.42017 to 2948.77954, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3176.8809 - mae: 3176.8809 - val_loss: 2948.7795 - val_mae: 2948.7795\n",
      "Epoch 240/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3066.6711 - mae: 3066.6711\n",
      "Epoch 240: val_loss did not improve from 2948.77954\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3177.7166 - mae: 3177.7166 - val_loss: 2949.1658 - val_mae: 2949.1658\n",
      "Epoch 241/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3896.1045 - mae: 3896.1045\n",
      "Epoch 241: val_loss improved from 2948.77954 to 2947.05005, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3177.4509 - mae: 3177.4509 - val_loss: 2947.0500 - val_mae: 2947.0500\n",
      "Epoch 242/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 4705.5215 - mae: 4705.5215\n",
      "Epoch 242: val_loss improved from 2947.05005 to 2946.51611, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3177.5332 - mae: 3177.5332 - val_loss: 2946.5161 - val_mae: 2946.5161\n",
      "Epoch 243/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 4250.0547 - mae: 4250.0547\n",
      "Epoch 243: val_loss did not improve from 2946.51611\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3176.1138 - mae: 3176.1138 - val_loss: 2947.7104 - val_mae: 2947.7104\n",
      "Epoch 244/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 4207.2041 - mae: 4207.2041\n",
      "Epoch 244: val_loss did not improve from 2946.51611\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3175.8516 - mae: 3175.8516 - val_loss: 2946.9832 - val_mae: 2946.9832\n",
      "Epoch 245/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3032.4707 - mae: 3032.4707\n",
      "Epoch 245: val_loss improved from 2946.51611 to 2946.44434, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3176.0371 - mae: 3176.0371 - val_loss: 2946.4443 - val_mae: 2946.4443\n",
      "Epoch 246/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 1079.3196 - mae: 1079.3196\n",
      "Epoch 246: val_loss did not improve from 2946.44434\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3175.8198 - mae: 3175.8198 - val_loss: 2948.1641 - val_mae: 2948.1641\n",
      "Epoch 247/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3737.4529 - mae: 3737.4529\n",
      "Epoch 247: val_loss improved from 2946.44434 to 2945.03955, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3176.0183 - mae: 3176.0183 - val_loss: 2945.0396 - val_mae: 2945.0396\n",
      "Epoch 248/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3353.8789 - mae: 3353.8789\n",
      "Epoch 248: val_loss did not improve from 2945.03955\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3175.8174 - mae: 3175.8174 - val_loss: 2950.3152 - val_mae: 2950.3152\n",
      "Epoch 249/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 2126.0039 - mae: 2126.0039\n",
      "Epoch 249: val_loss did not improve from 2945.03955\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3176.4363 - mae: 3176.4363 - val_loss: 2946.2710 - val_mae: 2946.2710\n",
      "Epoch 250/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3490.4805 - mae: 3490.4805\n",
      "Epoch 250: val_loss did not improve from 2945.03955\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3174.6555 - mae: 3174.6555 - val_loss: 2945.5596 - val_mae: 2945.5596\n",
      "Epoch 251/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 4090.1328 - mae: 4090.1328\n",
      "Epoch 251: val_loss improved from 2945.03955 to 2944.91089, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3175.2144 - mae: 3175.2144 - val_loss: 2944.9109 - val_mae: 2944.9109\n",
      "Epoch 252/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3121.6621 - mae: 3121.6621\n",
      "Epoch 252: val_loss did not improve from 2944.91089\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3175.4854 - mae: 3175.4854 - val_loss: 2945.9197 - val_mae: 2945.9197\n",
      "Epoch 253/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 4033.5815 - mae: 4033.5815\n",
      "Epoch 253: val_loss did not improve from 2944.91089\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3174.8877 - mae: 3174.8877 - val_loss: 2946.4675 - val_mae: 2946.4675\n",
      "Epoch 254/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3395.1880 - mae: 3395.1880\n",
      "Epoch 254: val_loss improved from 2944.91089 to 2944.81396, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3175.1235 - mae: 3175.1235 - val_loss: 2944.8140 - val_mae: 2944.8140\n",
      "Epoch 255/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 6102.2041 - mae: 6102.2041\n",
      "Epoch 255: val_loss improved from 2944.81396 to 2944.70117, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3174.3079 - mae: 3174.3079 - val_loss: 2944.7012 - val_mae: 2944.7012\n",
      "Epoch 256/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 4131.8677 - mae: 4131.8677\n",
      "Epoch 256: val_loss did not improve from 2944.70117\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3173.7480 - mae: 3173.7480 - val_loss: 2944.9817 - val_mae: 2944.9817\n",
      "Epoch 257/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 5325.4541 - mae: 5325.4541\n",
      "Epoch 257: val_loss did not improve from 2944.70117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 2ms/step - loss: 3174.5422 - mae: 3174.5422 - val_loss: 2948.8120 - val_mae: 2948.8120\n",
      "Epoch 258/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 2499.7371 - mae: 2499.7371\n",
      "Epoch 258: val_loss improved from 2944.70117 to 2943.28638, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3174.4329 - mae: 3174.4329 - val_loss: 2943.2864 - val_mae: 2943.2864\n",
      "Epoch 259/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3544.1797 - mae: 3544.1797\n",
      "Epoch 259: val_loss improved from 2943.28638 to 2943.27075, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3173.3596 - mae: 3173.3596 - val_loss: 2943.2708 - val_mae: 2943.2708\n",
      "Epoch 260/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3680.3447 - mae: 3680.3447\n",
      "Epoch 260: val_loss did not improve from 2943.27075\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3174.5583 - mae: 3174.5583 - val_loss: 2944.9419 - val_mae: 2944.9419\n",
      "Epoch 261/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 4079.0972 - mae: 4079.0972\n",
      "Epoch 261: val_loss did not improve from 2943.27075\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3174.5801 - mae: 3174.5801 - val_loss: 2943.5842 - val_mae: 2943.5842\n",
      "Epoch 262/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 2450.1541 - mae: 2450.1541\n",
      "Epoch 262: val_loss improved from 2943.27075 to 2942.95142, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3172.9556 - mae: 3172.9556 - val_loss: 2942.9514 - val_mae: 2942.9514\n",
      "Epoch 263/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3885.5574 - mae: 3885.5574\n",
      "Epoch 263: val_loss did not improve from 2942.95142\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3173.0627 - mae: 3173.0627 - val_loss: 2943.6875 - val_mae: 2943.6875\n",
      "Epoch 264/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3064.6704 - mae: 3064.6704\n",
      "Epoch 264: val_loss improved from 2942.95142 to 2942.55127, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3174.1389 - mae: 3174.1389 - val_loss: 2942.5513 - val_mae: 2942.5513\n",
      "Epoch 265/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 3682.8188 - mae: 3682.8188\n",
      "Epoch 265: val_loss improved from 2942.55127 to 2942.39868, saving model to data\\best_model.h5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3172.5208 - mae: 3172.5208 - val_loss: 2942.3987 - val_mae: 2942.3987\n",
      "Epoch 266/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 4390.9634 - mae: 4390.9634\n",
      "Epoch 266: val_loss did not improve from 2942.39868\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3173.1079 - mae: 3173.1079 - val_loss: 2946.1711 - val_mae: 2946.1711\n",
      "Epoch 267/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 2431.7576 - mae: 2431.7576\n",
      "Epoch 267: val_loss did not improve from 2942.39868\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3173.2141 - mae: 3173.2141 - val_loss: 2944.4170 - val_mae: 2944.4170\n",
      "Epoch 268/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 4184.6250 - mae: 4184.6250\n",
      "Epoch 268: val_loss did not improve from 2942.39868\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3173.2119 - mae: 3173.2119 - val_loss: 2945.1833 - val_mae: 2945.1833\n",
      "Epoch 269/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 5098.8506 - mae: 5098.8506\n",
      "Epoch 269: val_loss did not improve from 2942.39868\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3172.3384 - mae: 3172.3384 - val_loss: 2942.5107 - val_mae: 2942.5107\n",
      "Epoch 270/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 4275.9902 - mae: 4275.9902\n",
      "Epoch 270: val_loss did not improve from 2942.39868\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3172.1799 - mae: 3172.1799 - val_loss: 2943.7290 - val_mae: 2943.7290\n",
      "Epoch 271/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 5069.9199 - mae: 5069.9199\n",
      "Epoch 271: val_loss did not improve from 2942.39868\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3171.4927 - mae: 3171.4927 - val_loss: 2944.6064 - val_mae: 2944.6064\n",
      "Epoch 272/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 2167.0166 - mae: 2167.0166\n",
      "Epoch 272: val_loss did not improve from 2942.39868\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3171.6279 - mae: 3171.6279 - val_loss: 2943.1992 - val_mae: 2943.1992\n",
      "Epoch 273/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 4381.5581 - mae: 4381.5581\n",
      "Epoch 273: val_loss did not improve from 2942.39868\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3171.9668 - mae: 3171.9668 - val_loss: 2943.1953 - val_mae: 2943.1953\n",
      "Epoch 274/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 1020.0428 - mae: 1020.0428\n",
      "Epoch 274: val_loss did not improve from 2942.39868\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3171.8088 - mae: 3171.8088 - val_loss: 2944.1465 - val_mae: 2944.1465\n",
      "Epoch 275/2000\n",
      " 1/32 [..............................] - ETA: 0s - loss: 2274.6602 - mae: 2274.6602\n",
      "Epoch 275: val_loss did not improve from 2942.39868\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3172.2085 - mae: 3172.2085 - val_loss: 2946.2852 - val_mae: 2946.2852\n"
     ]
    }
   ],
   "source": [
    "# Set random seed\n",
    "tf.random.set_seed(43)\n",
    "\n",
    "# Build the model (3 layers, 11, 100, 1 units)\n",
    "model_es = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(11, activation=\"relu\", input_shape = x_train_transformed.shape[1:]),\n",
    "        tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(1)])\n",
    "\n",
    "# Compile the model\n",
    "model_es.compile(loss= tf.keras.losses.mae, optimizer= tf.keras.optimizers.Adam(), metrics= ['mae'])\n",
    "\n",
    "# Fit the model for 100 epochs \n",
    "history_es = model_es.fit(x_train_transformed, y_train, validation_data=(x_test_transformed, y_test), epochs=2000,\n",
    "                       callbacks=[early_stopping, model_checkpoint], verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "967f9488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_mae</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>3171.492676</td>\n",
       "      <td>3171.492676</td>\n",
       "      <td>2944.606445</td>\n",
       "      <td>2944.606445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>3171.627930</td>\n",
       "      <td>3171.627930</td>\n",
       "      <td>2943.199219</td>\n",
       "      <td>2943.199219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>3171.966797</td>\n",
       "      <td>3171.966797</td>\n",
       "      <td>2943.195312</td>\n",
       "      <td>2943.195312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>3171.808838</td>\n",
       "      <td>3171.808838</td>\n",
       "      <td>2944.146484</td>\n",
       "      <td>2944.146484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>3172.208496</td>\n",
       "      <td>3172.208496</td>\n",
       "      <td>2946.285156</td>\n",
       "      <td>2946.285156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            loss          mae     val_loss      val_mae\n",
       "270  3171.492676  3171.492676  2944.606445  2944.606445\n",
       "271  3171.627930  3171.627930  2943.199219  2943.199219\n",
       "272  3171.966797  3171.966797  2943.195312  2943.195312\n",
       "273  3171.808838  3171.808838  2944.146484  2944.146484\n",
       "274  3172.208496  3172.208496  2946.285156  2946.285156"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hist_es = pd.DataFrame(history_es.history)\n",
    "df_hist_es.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eb0d503a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(pd.DataFrame(history_es.history).loc[:, ['loss', 'val_loss']]).plot(figsize=(8, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0260a7b2",
   "metadata": {},
   "source": [
    "#### The training stopped when the losses saturated and the model was saved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5050d4f6",
   "metadata": {},
   "source": [
    "## Predicting a single Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "78d3a913",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>smoker</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>45</td>\n",
       "      <td>female</td>\n",
       "      <td>25.175</td>\n",
       "      <td>2</td>\n",
       "      <td>no</td>\n",
       "      <td>northeast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>36</td>\n",
       "      <td>female</td>\n",
       "      <td>30.020</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>64</td>\n",
       "      <td>female</td>\n",
       "      <td>26.885</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>northwest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1293</th>\n",
       "      <td>46</td>\n",
       "      <td>male</td>\n",
       "      <td>25.745</td>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>19</td>\n",
       "      <td>male</td>\n",
       "      <td>31.920</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>northwest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>60</td>\n",
       "      <td>female</td>\n",
       "      <td>27.550</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northeast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>58</td>\n",
       "      <td>male</td>\n",
       "      <td>34.865</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northeast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1128</th>\n",
       "      <td>34</td>\n",
       "      <td>male</td>\n",
       "      <td>32.800</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>southwest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>19</td>\n",
       "      <td>male</td>\n",
       "      <td>30.250</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>southeast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>41</td>\n",
       "      <td>male</td>\n",
       "      <td>33.550</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>335 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age     sex     bmi  children smoker     region\n",
       "764    45  female  25.175         2     no  northeast\n",
       "887    36  female  30.020         0     no  northwest\n",
       "890    64  female  26.885         0    yes  northwest\n",
       "1293   46    male  25.745         3     no  northwest\n",
       "259    19    male  31.920         0    yes  northwest\n",
       "...   ...     ...     ...       ...    ...        ...\n",
       "342    60  female  27.550         0     no  northeast\n",
       "308    58    male  34.865         0     no  northeast\n",
       "1128   34    male  32.800         1     no  southwest\n",
       "503    19    male  30.250         0    yes  southeast\n",
       "1197   41    male  33.550         0     no  southeast\n",
       "\n",
       "[335 rows x 6 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e8771c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                64\n",
       "sex            female\n",
       "bmi            26.885\n",
       "children            0\n",
       "smoker            yes\n",
       "region      northwest\n",
       "Name: 890, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "77cc9743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>890</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sex</th>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bmi</th>\n",
       "      <td>26.885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>children</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smoker</th>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region</th>\n",
       "      <td>northwest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                890\n",
       "age              64\n",
       "sex          female\n",
       "bmi          26.885\n",
       "children          0\n",
       "smoker          yes\n",
       "region    northwest"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(x_test.iloc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a9fa35cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'sex', 'bmi', 'children', 'smoker', 'region'], dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8a14b47b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>smoker</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64</td>\n",
       "      <td>female</td>\n",
       "      <td>26.885</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>northwest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  age     sex     bmi children smoker     region\n",
       "0  64  female  26.885        0    yes  northwest"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = pd.DataFrame(x_test.iloc[2].values.reshape(1,6), columns=x_test.columns)\n",
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7fab1982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.29391983, 0.        , 1.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 1.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct.transform(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "56cbe5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[32019.658]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(ct.transform(aa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "df274817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 48ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[46184.27]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_es.predict(ct.transform(aa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "362d0441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>smoker</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>45</td>\n",
       "      <td>female</td>\n",
       "      <td>25.175</td>\n",
       "      <td>2</td>\n",
       "      <td>no</td>\n",
       "      <td>northeast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>36</td>\n",
       "      <td>female</td>\n",
       "      <td>30.020</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>64</td>\n",
       "      <td>female</td>\n",
       "      <td>26.885</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>northwest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1293</th>\n",
       "      <td>46</td>\n",
       "      <td>male</td>\n",
       "      <td>25.745</td>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      age     sex     bmi  children smoker     region\n",
       "764    45  female  25.175         2     no  northeast\n",
       "887    36  female  30.020         0     no  northwest\n",
       "890    64  female  26.885         0    yes  northwest\n",
       "1293   46    male  25.745         3     no  northwest"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "981f174d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "764      9095.06825\n",
       "887      5272.17580\n",
       "890     29330.98315\n",
       "1293     9301.89355\n",
       "Name: charges, dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:4]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
